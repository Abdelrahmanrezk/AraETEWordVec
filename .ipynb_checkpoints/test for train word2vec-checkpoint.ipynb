{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37de25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gensim\n",
    "import csv\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "from nltk.tokenize import RegexpTokenizer, TreebankWordTokenizer\n",
    "from train_word2vec_from_scratch import *\n",
    "from ara_vec_preprocess_configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee8dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus(\"arabic_filtered_tweets_of_year_2008.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65eb3f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "False\n",
      "Epoch #0 start\n",
      "2022-03-03 12:45:42.152188\n",
      "Epoch #0 end\n",
      "2022-03-03 12:45:44.641794\n",
      "==================================================\n",
      "Epoch #1 start\n",
      "2022-03-03 12:45:44.648386\n",
      "Epoch #1 end\n",
      "2022-03-03 12:45:47.233627\n",
      "==================================================\n",
      "Epoch #2 start\n",
      "2022-03-03 12:45:47.240232\n",
      "Epoch #2 end\n",
      "2022-03-03 12:45:49.895967\n",
      "==================================================\n",
      "Epoch #3 start\n",
      "2022-03-03 12:45:49.902634\n",
      "Epoch #3 end\n",
      "2022-03-03 12:45:51.681894\n",
      "==================================================\n",
      "Epoch #4 start\n",
      "2022-03-03 12:45:51.687936\n",
      "Epoch #4 end\n",
      "2022-03-03 12:45:54.212141\n",
      "==================================================\n",
      "Epoch #5 start\n",
      "2022-03-03 12:45:54.218652\n",
      "Epoch #5 end\n",
      "2022-03-03 12:45:57.204904\n",
      "==================================================\n",
      "Epoch #6 start\n",
      "2022-03-03 12:45:57.211820\n",
      "Epoch #6 end\n",
      "2022-03-03 12:45:59.808582\n",
      "==================================================\n",
      "0:00:20.357659\n"
     ]
    }
   ],
   "source": [
    "model = train_word2vec_cbow(\"\", False, \"fullgram\", sentences, vector_size=300, min_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d72891f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12583\n",
      "7\n",
      "268\n"
     ]
    }
   ],
   "source": [
    "print(model.corpus_count)\n",
    "print(model.epochs)\n",
    "print(len(model.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b988b096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "True\n",
      "Epoch #0 start\n",
      "2022-03-03 12:46:02.481222\n",
      "Epoch #0 end\n",
      "2022-03-03 12:46:04.214508\n",
      "==================================================\n",
      "Epoch #1 start\n",
      "2022-03-03 12:46:04.221358\n",
      "Epoch #1 end\n",
      "2022-03-03 12:46:06.779494\n",
      "==================================================\n",
      "Epoch #2 start\n",
      "2022-03-03 12:46:06.787198\n",
      "Epoch #2 end\n",
      "2022-03-03 12:46:09.601704\n",
      "==================================================\n",
      "Epoch #3 start\n",
      "2022-03-03 12:46:09.609377\n",
      "Epoch #3 end\n",
      "2022-03-03 12:46:11.439335\n",
      "==================================================\n",
      "Epoch #4 start\n",
      "2022-03-03 12:46:11.445408\n",
      "Epoch #4 end\n",
      "2022-03-03 12:46:14.091502\n",
      "==================================================\n",
      "Epoch #5 start\n",
      "2022-03-03 12:46:14.098013\n",
      "Epoch #5 end\n",
      "2022-03-03 12:46:16.848982\n",
      "==================================================\n",
      "Epoch #6 start\n",
      "2022-03-03 12:46:16.855640\n",
      "Epoch #6 end\n",
      "2022-03-03 12:46:19.432794\n",
      "==================================================\n",
      "0:00:19.581441\n",
      "==================================================\n",
      "True\n",
      "Epoch #0 start\n",
      "2022-03-03 12:46:52.433540\n",
      "Epoch #0 end\n",
      "2022-03-03 12:47:24.352214\n",
      "==================================================\n",
      "Epoch #1 start\n",
      "2022-03-03 12:47:24.477661\n",
      "Epoch #1 end\n",
      "2022-03-03 12:47:58.534380\n",
      "==================================================\n",
      "Epoch #2 start\n",
      "2022-03-03 12:47:58.655380\n",
      "Epoch #2 end\n",
      "2022-03-03 12:48:29.474800\n",
      "==================================================\n",
      "Epoch #3 start\n",
      "2022-03-03 12:48:29.598019\n",
      "Epoch #3 end\n",
      "2022-03-03 12:49:00.373934\n",
      "==================================================\n",
      "Epoch #4 start\n",
      "2022-03-03 12:49:00.497207\n",
      "Epoch #4 end\n",
      "2022-03-03 12:49:33.108102\n",
      "==================================================\n",
      "Epoch #5 start\n",
      "2022-03-03 12:49:33.238369\n",
      "Epoch #5 end\n",
      "2022-03-03 12:50:03.175407\n",
      "==================================================\n",
      "Epoch #6 start\n",
      "2022-03-03 12:50:03.293522\n",
      "Epoch #6 end\n",
      "2022-03-03 12:50:34.037366\n",
      "==================================================\n",
      "0:04:14.843011\n",
      "==================================================\n",
      "True\n",
      "Epoch #0 start\n",
      "2022-03-03 12:56:46.079774\n",
      "Epoch #0 end\n",
      "2022-03-03 13:03:04.422675\n",
      "==================================================\n",
      "Epoch #1 start\n",
      "2022-03-03 13:03:06.119295\n",
      "Epoch #1 end\n",
      "2022-03-03 13:09:26.572107\n",
      "==================================================\n",
      "Epoch #2 start\n",
      "2022-03-03 13:09:28.291804\n",
      "Epoch #2 end\n",
      "2022-03-03 13:16:01.715256\n",
      "==================================================\n",
      "Epoch #3 start\n",
      "2022-03-03 13:16:03.562668\n",
      "Epoch #3 end\n",
      "2022-03-03 13:23:05.806939\n",
      "==================================================\n",
      "Epoch #4 start\n",
      "2022-03-03 13:23:07.638122\n",
      "Epoch #4 end\n",
      "2022-03-03 13:30:37.215286\n",
      "==================================================\n",
      "Epoch #5 start\n",
      "2022-03-03 13:30:39.116940\n",
      "Epoch #5 end\n",
      "2022-03-03 13:38:01.262653\n",
      "==================================================\n",
      "Epoch #6 start\n",
      "2022-03-03 13:38:03.096586\n",
      "Epoch #6 end\n",
      "2022-03-03 13:45:43.224764\n",
      "==================================================\n",
      "0:55:12.947443\n"
     ]
    }
   ],
   "source": [
    "for fname in os.listdir(ARABIC_FILTERED_DATA_DIR):\n",
    "    print(\"=\"*50)\n",
    "    print(\"=\"*50)\n",
    "    print(\"Train on file: \", fname)\n",
    "    print(\"=\"*50)\n",
    "    print(\"=\"*50)\n",
    "    sentences = MyCorpus(fname)\n",
    "    model = train_word2vec_cbow(model, True, \"fullgram\", sentences, vector_size=300, min_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc1d16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2181338\n",
      "7\n",
      "73382\n"
     ]
    }
   ],
   "source": [
    "print(model.corpus_count)\n",
    "print(model.epochs)\n",
    "print(len(model.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    def __init__(self, tweet):\n",
    "        self.tweets = tweet\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tweet in self.tweets:\n",
    "            try:\n",
    "                full_grams = get_all_ngrams([tweet], 3)\n",
    "                yield full_grams[0]\n",
    "            except:\n",
    "                pass\\\n",
    "            \n",
    "            \n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            # just one-d list\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fead95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se = [\n",
    "#     [\"The\", \" word2vec\", \" model\", \"application\", \"Mikolov\", \"have\", \"attracted\", \"great\", \" amount\",   \"of\" ],\n",
    "#     ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
    "#      'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']\n",
    "# ]\n",
    "\n",
    "# more_sentences = [\n",
    "#     ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
    "#      'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8eaa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984723ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "se = Generator(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d50d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=se, vector_size=300, window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3296ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.corpus_count)\n",
    "print(model.epochs)\n",
    "print(len(model.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb9e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_sentences = Generator(ARABIC_FILTERED_DATA_DIR + \"arabic_filtered_tweets_of_year_2009.csv\")\n",
    "\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5e2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.corpus_count)\n",
    "print(model.epochs)\n",
    "print(len(model.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828c8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_sentences = Generator(ARABIC_FILTERED_DATA_DIR + \"arabic_filtered_tweets_of_year_2012.csv\")\n",
    "\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413bc223",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.corpus_count)\n",
    "print(model.epochs)\n",
    "print(len(model.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b379d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = Generator(ARABIC_FILTERED_DATA_DIR + \"arabic_filtered_tweets_of_year_2012.csv\")\n",
    "model = train_word2vec_cbow_2(\"fullgram\", sentences, 300, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43676ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.corpus_count)\n",
    "print(model.epochs)\n",
    "print(len(model.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e21f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(model.wv.key_to_index):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4551a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(ARABIC_FILTERED_DATA_DIR):\n",
    "    f_path = os.path.join(ARABIC_FILTERED_DATA_DIR, file)\n",
    "    sentences = Generator(f_path)\n",
    "    model = train_word2vec_cbow_2(\"fullgram\", sentences, 300, 25)\n",
    "    print(len(model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4668a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            # just one-d list\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32fcb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(model.wv.key_to_index):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "import gensim.models.word2vec\n",
    "import gensim.downloader as api\n",
    "import smart_open\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def head(path, size):\n",
    "    with smart_open.open(path) as fin:\n",
    "        return io.StringIO(fin.read(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_data():\n",
    "    lee_path = datapath('lee_background.cor')\n",
    "    ls = gensim.models.word2vec.LineSentence(lee_path)\n",
    "    ls.name = '25kB'\n",
    "    yield ls\n",
    "\n",
    "    text8_path = api.load('text8').fn\n",
    "    labels = ('1MB', '10MB', '50MB', '100MB')\n",
    "    sizes = (1024 ** 2, 10 * 1024 ** 2, 50 * 1024 ** 2, 100 * 1024 ** 2)\n",
    "    for l, s in zip(labels, sizes):\n",
    "        ls = gensim.models.word2vec.LineSentence(head(text8_path, s))\n",
    "        ls.name = l\n",
    "        yield ls\n",
    "        \n",
    "input_data = list(generate_input_data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f225e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a436730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily reduce logging verbosity\n",
    "import logging\n",
    "logging.root.level = logging.ERROR\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_time_values = []\n",
    "seed_val = 42\n",
    "sg_values = [0, 1]\n",
    "hs_values = [0, 1]\n",
    "\n",
    "fast = True\n",
    "\n",
    "if fast:\n",
    "    input_data_subset = input_data[:3]\n",
    "else:\n",
    "    input_data_subset = input_data\n",
    "\n",
    "\n",
    "    \n",
    "for data in input_data_subset:\n",
    "    for sg_val in sg_values:\n",
    "        for hs_val in hs_values:\n",
    "            for loss_flag in [True, False]:\n",
    "                time_taken_list = []\n",
    "                for i in range(3):\n",
    "                    start_time = time.time()\n",
    "                    w2v_model = gensim.models.Word2Vec(\n",
    "                        data,\n",
    "                        compute_loss=loss_flag,\n",
    "                        sg=sg_val,\n",
    "                        hs=hs_val,\n",
    "                        seed=seed_val,\n",
    "                    )\n",
    "                    time_taken_list.append(time.time() - start_time)\n",
    "\n",
    "                time_taken_list = np.array(time_taken_list)\n",
    "                time_mean = np.mean(time_taken_list)\n",
    "                time_std = np.std(time_taken_list)\n",
    "\n",
    "                model_result = {\n",
    "                    'train_data': data.name,\n",
    "                    'compute_loss': loss_flag,\n",
    "                    'sg': sg_val,\n",
    "                    'hs': hs_val,\n",
    "                    'train_time_mean': time_mean,\n",
    "                    'train_time_std': time_std,\n",
    "                }\n",
    "                print(\"Word2vec model #%i: %s\" % (len(train_time_values), model_result))\n",
    "                train_time_values.append(model_result)\n",
    "\n",
    "train_times_table = pd.DataFrame(train_time_values)\n",
    "train_times_table = train_times_table.sort_values(\n",
    "    by=['train_data', 'sg', 'hs', 'compute_loss'],\n",
    "    ascending=[False, False, True, False],\n",
    ")\n",
    "print(train_times_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a6002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
