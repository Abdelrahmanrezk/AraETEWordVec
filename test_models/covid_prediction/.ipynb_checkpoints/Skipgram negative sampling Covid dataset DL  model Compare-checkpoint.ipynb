{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d19e480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../preprocess_assets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2595a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\n",
    "import tensorflow as tf\n",
    "from features_extraction import *\n",
    "from data_shuffling_split import *\n",
    "from ara_vec_preprocess_configs import *\n",
    "from ml_modeling import *\n",
    "from keras_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8deed7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>مع انتشار الامراض ومنها الكورونا اليكم النصيحه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1 دراسه بحثيه صادره معهد الدراسات الاستراتيجي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>بدات في مدينه وهان في الصين انتقلت لبعض المقاط...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>coronarvirus CoronaOutbreak فيروس_كورونا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>الاداره_العامه_للطيران_المدني اجراءات وقاءيه ح...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  مع انتشار الامراض ومنها الكورونا اليكم النصيحه...\n",
       "1      0   1 دراسه بحثيه صادره معهد الدراسات الاستراتيجي...\n",
       "2      1  بدات في مدينه وهان في الصين انتقلت لبعض المقاط...\n",
       "3      1           coronarvirus CoronaOutbreak فيروس_كورونا\n",
       "4      1  الاداره_العامه_للطيران_المدني اجراءات وقاءيه ح..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set = read_file(\"train/strat_train_set.csv\")\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9dd098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of instances in the training data after StratifiedShuffleSplit are:  8643\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   177\n",
      "The number of trainin instances:  8643\n",
      "The number of validation instances:  177\n",
      "The number of trainin labels :  8643\n",
      "The number of validation labels :  177\n"
     ]
    }
   ],
   "source": [
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0507dbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " [' صباح_الخير يقول بعض علماء الغرب امثال كاديل ومريسون ان المنقذ للغرب المادي المتدهور الايمان بالله وقالوا ان البعد عن الايمان بالله السبب الريءسي وراء تفشي ظاهره الانتحار بينهم رغم الانفتاح وتوفر كافه الحريه والمغريات ومع نسمع يطالب ليلا ونهارا في الحريات المزعومه ', 'المفروض غسل اليدين يكون تحتها عشر خطوط ', 'الشرقيه تتباكي علي تفشي الاميه في صفوف شراءح مجتمعيه مختلفه في العراق']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['صباح_الخير', 'يقول', 'بعض', 'علماء', 'الغرب', 'امثال', 'كاديل', 'ومريسون', 'ان', 'المنقذ', 'للغرب', 'المادي', 'المتدهور', 'الايمان', 'بالله', 'وقالوا', 'ان', 'البعد', 'عن', 'الايمان', 'بالله', 'السبب', 'الريءسي', 'وراء', 'تفشي', 'ظاهره', 'الانتحار', 'بينهم', 'رغم', 'الانفتاح', 'وتوفر', 'كافه', 'الحريه', 'والمغريات', 'ومع', 'نسمع', 'يطالب', 'ليلا', 'ونهارا', 'في', 'الحريات', 'المزعومه'], ['المفروض', 'غسل', 'اليدين', 'يكون', 'تحتها', 'عشر', 'خطوط'], ['الشرقيه', 'تتباكي', 'علي', 'تفشي', 'الاميه', 'في', 'صفوف', 'شراءح', 'مجتمعيه', 'مختلفه', 'في', 'العراق']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['يوه عاد الارك تعقيم بصري', 'جيرانا جو الصين امبارح', 'انتشار وباء مرض فيروسي بمدينه نيويورك بالولايات المتحده الامريكيه بيوم الجمعه السوداء يقدم فيه تخفضيات هاءله انتشر الفيروس عبر النقود لينتقل خلال انحاء البلاد باقل اسبوع ولم تستطع الامم المتحده علي احتواءه مما ادي انهيارها بخلال خمسه ايام ']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['يوه', 'عاد', 'الارك', 'تعقيم', 'بصري'], ['جيرانا', 'جو', 'الصين', 'امبارح'], ['انتشار', 'وباء', 'مرض', 'فيروسي', 'بمدينه', 'نيويورك', 'بالولايات', 'المتحده', 'الامريكيه', 'بيوم', 'الجمعه', 'السوداء', 'يقدم', 'فيه', 'تخفضيات', 'هاءله', 'انتشر', 'الفيروس', 'عبر', 'النقود', 'لينتقل', 'خلال', 'انحاء', 'البلاد', 'باقل', 'اسبوع', 'ولم', 'تستطع', 'الامم', 'المتحده', 'علي', 'احتواءه', 'مما', 'ادي', 'انهيارها', 'بخلال', 'خمسه', 'ايام']]\n",
      "full gram tokenization : \n",
      " [['صباح_الخير', 'يقول', 'بعض', 'علماء', 'الغرب', 'امثال', 'كاديل', 'ومريسون', 'ان', 'المنقذ', 'للغرب', 'المادي', 'المتدهور', 'الايمان', 'بالله', 'وقالوا', 'ان', 'البعد', 'عن', 'الايمان', 'بالله', 'السبب', 'الريءسي', 'وراء', 'تفشي', 'ظاهره', 'الانتحار', 'بينهم', 'رغم', 'الانفتاح', 'وتوفر', 'كافه', 'الحريه', 'والمغريات', 'ومع', 'نسمع', 'يطالب', 'ليلا', 'ونهارا', 'في', 'الحريات', 'المزعومه', 'صباح_الخير_يقول', 'يقول_بعض', 'بعض_علماء', 'علماء_الغرب', 'الغرب_امثال', 'امثال_كاديل', 'كاديل_ومريسون', 'ومريسون_ان', 'ان_المنقذ', 'المنقذ_للغرب', 'للغرب_المادي', 'المادي_المتدهور', 'المتدهور_الايمان', 'الايمان_بالله', 'بالله_وقالوا', 'وقالوا_ان', 'ان_البعد', 'البعد_عن', 'عن_الايمان', 'الايمان_بالله', 'بالله_السبب', 'السبب_الريءسي', 'الريءسي_وراء', 'وراء_تفشي', 'تفشي_ظاهره', 'ظاهره_الانتحار', 'الانتحار_بينهم', 'بينهم_رغم', 'رغم_الانفتاح', 'الانفتاح_وتوفر', 'وتوفر_كافه', 'كافه_الحريه', 'الحريه_والمغريات', 'والمغريات_ومع', 'ومع_نسمع', 'نسمع_يطالب', 'يطالب_ليلا', 'ليلا_ونهارا', 'ونهارا_في', 'في_الحريات', 'الحريات_المزعومه', 'صباح_الخير_يقول_بعض', 'يقول_بعض_علماء', 'بعض_علماء_الغرب', 'علماء_الغرب_امثال', 'الغرب_امثال_كاديل', 'امثال_كاديل_ومريسون', 'كاديل_ومريسون_ان', 'ومريسون_ان_المنقذ', 'ان_المنقذ_للغرب', 'المنقذ_للغرب_المادي', 'للغرب_المادي_المتدهور', 'المادي_المتدهور_الايمان', 'المتدهور_الايمان_بالله', 'الايمان_بالله_وقالوا', 'بالله_وقالوا_ان', 'وقالوا_ان_البعد', 'ان_البعد_عن', 'البعد_عن_الايمان', 'عن_الايمان_بالله', 'الايمان_بالله_السبب', 'بالله_السبب_الريءسي', 'السبب_الريءسي_وراء', 'الريءسي_وراء_تفشي', 'وراء_تفشي_ظاهره', 'تفشي_ظاهره_الانتحار', 'ظاهره_الانتحار_بينهم', 'الانتحار_بينهم_رغم', 'بينهم_رغم_الانفتاح', 'رغم_الانفتاح_وتوفر', 'الانفتاح_وتوفر_كافه', 'وتوفر_كافه_الحريه', 'كافه_الحريه_والمغريات', 'الحريه_والمغريات_ومع', 'والمغريات_ومع_نسمع', 'ومع_نسمع_يطالب', 'نسمع_يطالب_ليلا', 'يطالب_ليلا_ونهارا', 'ليلا_ونهارا_في', 'ونهارا_في_الحريات', 'في_الحريات_المزعومه'], ['المفروض', 'غسل', 'اليدين', 'يكون', 'تحتها', 'عشر', 'خطوط', 'المفروض_غسل', 'غسل_اليدين', 'اليدين_يكون', 'يكون_تحتها', 'تحتها_عشر', 'عشر_خطوط', 'المفروض_غسل_اليدين', 'غسل_اليدين_يكون', 'اليدين_يكون_تحتها', 'يكون_تحتها_عشر', 'تحتها_عشر_خطوط'], ['الشرقيه', 'تتباكي', 'علي', 'تفشي', 'الاميه', 'في', 'صفوف', 'شراءح', 'مجتمعيه', 'مختلفه', 'في', 'العراق', 'الشرقيه_تتباكي', 'تتباكي_علي', 'علي_تفشي', 'تفشي_الاميه', 'الاميه_في', 'في_صفوف', 'صفوف_شراءح', 'شراءح_مجتمعيه', 'مجتمعيه_مختلفه', 'مختلفه_في', 'في_العراق', 'الشرقيه_تتباكي_علي', 'تتباكي_علي_تفشي', 'علي_تفشي_الاميه', 'تفشي_الاميه_في', 'الاميه_في_صفوف', 'في_صفوف_شراءح', 'صفوف_شراءح_مجتمعيه', 'شراءح_مجتمعيه_مختلفه', 'مجتمعيه_مختلفه_في', 'مختلفه_في_العراق']]\n",
      "==================================================\n",
      "full gram tokenization : \n",
      " [['يوه', 'عاد', 'الارك', 'تعقيم', 'بصري', 'يوه_عاد', 'عاد_الارك', 'الارك_تعقيم', 'تعقيم_بصري', 'يوه_عاد_الارك', 'عاد_الارك_تعقيم', 'الارك_تعقيم_بصري'], ['جيرانا', 'جو', 'الصين', 'امبارح', 'جيرانا_جو', 'جو_الصين', 'الصين_امبارح', 'جيرانا_جو_الصين', 'جو_الصين_امبارح'], ['انتشار', 'وباء', 'مرض', 'فيروسي', 'بمدينه', 'نيويورك', 'بالولايات', 'المتحده', 'الامريكيه', 'بيوم', 'الجمعه', 'السوداء', 'يقدم', 'فيه', 'تخفضيات', 'هاءله', 'انتشر', 'الفيروس', 'عبر', 'النقود', 'لينتقل', 'خلال', 'انحاء', 'البلاد', 'باقل', 'اسبوع', 'ولم', 'تستطع', 'الامم', 'المتحده', 'علي', 'احتواءه', 'مما', 'ادي', 'انهيارها', 'بخلال', 'خمسه', 'ايام', 'انتشار_وباء', 'وباء_مرض', 'مرض_فيروسي', 'فيروسي_بمدينه', 'بمدينه_نيويورك', 'نيويورك_بالولايات', 'بالولايات_المتحده', 'المتحده_الامريكيه', 'الامريكيه_بيوم', 'بيوم_الجمعه', 'الجمعه_السوداء', 'السوداء_يقدم', 'يقدم_فيه', 'فيه_تخفضيات', 'تخفضيات_هاءله', 'هاءله_انتشر', 'انتشر_الفيروس', 'الفيروس_عبر', 'عبر_النقود', 'النقود_لينتقل', 'لينتقل_خلال', 'خلال_انحاء', 'انحاء_البلاد', 'البلاد_باقل', 'باقل_اسبوع', 'اسبوع_ولم', 'ولم_تستطع', 'تستطع_الامم', 'الامم_المتحده', 'المتحده_علي', 'علي_احتواءه', 'احتواءه_مما', 'مما_ادي', 'ادي_انهيارها', 'انهيارها_بخلال', 'بخلال_خمسه', 'خمسه_ايام', 'انتشار_وباء_مرض', 'وباء_مرض_فيروسي', 'مرض_فيروسي_بمدينه', 'فيروسي_بمدينه_نيويورك', 'بمدينه_نيويورك_بالولايات', 'نيويورك_بالولايات_المتحده', 'بالولايات_المتحده_الامريكيه', 'المتحده_الامريكيه_بيوم', 'الامريكيه_بيوم_الجمعه', 'بيوم_الجمعه_السوداء', 'الجمعه_السوداء_يقدم', 'السوداء_يقدم_فيه', 'يقدم_فيه_تخفضيات', 'فيه_تخفضيات_هاءله', 'تخفضيات_هاءله_انتشر', 'هاءله_انتشر_الفيروس', 'انتشر_الفيروس_عبر', 'الفيروس_عبر_النقود', 'عبر_النقود_لينتقل', 'النقود_لينتقل_خلال', 'لينتقل_خلال_انحاء', 'خلال_انحاء_البلاد', 'انحاء_البلاد_باقل', 'البلاد_باقل_اسبوع', 'باقل_اسبوع_ولم', 'اسبوع_ولم_تستطع', 'ولم_تستطع_الامم', 'تستطع_الامم_المتحده', 'الامم_المتحده_علي', 'المتحده_علي_احتواءه', 'علي_احتواءه_مما', 'احتواءه_مما_ادي', 'مما_ادي_انهيارها', 'ادي_انهيارها_بخلال', 'انهيارها_بخلال_خمسه', 'بخلال_خمسه_ايام']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])\n",
    "\n",
    "fullgram_x_train_text_tokenized = get_all_ngrams(x_train_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "fullgram_x_val_text_tokenized = get_all_ngrams(x_val_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_val_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48ed7e",
   "metadata": {},
   "source": [
    "# Our CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc85e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_word2vec_model = load_word2vec_model(\"../word2vec_models/rezk/skipgram_NS/sk_gr_negative_sampeling_fullgram_vec_size_300-d_min_count_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3316ece3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(8643, 237, 300)\n",
      "(8643, 71100)\n",
      "==================================================\n",
      "[ 0.3357    0.2115    0.2717   -0.2322   -0.41      0.003147 -0.2317\n",
      "  0.2128    0.2019   -0.1569   -0.1876   -0.3286   -0.0557    0.4133\n",
      " -0.4421   -0.2527    0.03934   0.0642    0.3313   -0.03026  -0.0577\n",
      " -0.0404    0.5684    0.201     0.164    -0.01569  -0.1128    0.0933\n",
      " -0.4312    0.2795    0.1592    0.168     0.1774   -0.05978  -0.343\n",
      "  0.003008  0.2512   -0.1476   -0.00782   0.2957   -0.04816  -0.3835\n",
      "  0.7046   -0.04498   0.3044    0.1449   -0.1686   -0.3308    0.03278\n",
      "  0.10016 ]\n",
      "==================================================\n",
      "(177, 237, 300)\n",
      "(177, 71100)\n",
      "==================================================\n",
      "[ 0.1484    0.3384    0.2812   -0.1536   -0.1451   -0.035     0.3137\n",
      " -0.3142    0.189    -0.2605   -0.4102    0.2142    0.382    -0.01069\n",
      " -0.006557  0.04318  -0.0772    0.08093  -0.2255   -0.524    -0.2388\n",
      "  0.643    -0.02719  -0.1041   -0.45      0.05038   0.4126   -0.2084\n",
      "  0.3079   -0.238     0.4382    0.2917   -0.1511   -0.3457   -0.0775\n",
      " -0.1508   -0.238     0.2369    0.3784    0.3108    0.1924   -0.3372\n",
      "  0.499     0.0964   -0.4858   -0.2313   -0.1417   -0.6245   -0.0722\n",
      " -0.2123  ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 237\n",
    "word2vec_path = \"rezk/cbow/\"\n",
    "model_path_to_save = \"../ml_models_saved/\"\n",
    "hid_num_neurons = 50\n",
    "learning_rate = .1\n",
    "epochs = 20\n",
    "estimators = voting_models()\n",
    "\n",
    "performance_lr = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=5)\n",
    "SGD_optimizer     =tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "Adam_optimizer = tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.999)\n",
    "RMSprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=.9)\n",
    "\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_x_val_text_tokenized, max_len_str)\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075e1c4",
   "metadata": {},
   "source": [
    "# Our CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcafe620",
   "metadata": {},
   "source": [
    "# With  SGD and Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45760146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 237, 300)          1200      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 237, 50)           70200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 237, 50)           200       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 237, 50)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 11850)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 18)                213318    \n",
      "=================================================================\n",
      "Total params: 284,918\n",
      "Trainable params: 284,218\n",
      "Non-trainable params: 700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"rezk_word2vec\", model_type=\"sgd_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, SGD_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6285706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "271/271 [==============================] - 52s 174ms/step - loss: 1.6220 - accuracy: 0.8359 - val_loss: 0.4758 - val_accuracy: 0.7853\n",
      "Epoch 2/2\n",
      "271/271 [==============================] - 48s 176ms/step - loss: 0.2099 - accuracy: 0.9196 - val_loss: 0.3426 - val_accuracy: 0.8644\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a2c80",
   "metadata": {},
   "source": [
    "# With  Adam and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"rezk_word2vec\", model_type=\"Adam_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, Adam_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc2268",
   "metadata": {},
   "source": [
    "# With  Rmsprob and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e5d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"rezk_word2vec\", model_type=\"Rmsprob_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ce58b",
   "metadata": {},
   "source": [
    "# Load best model & predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4ce3af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>حساء خفافيش الصين ابداع القرف يعني</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>اجراءات هكذا استعدت مصر لمواجهه فيروس كورونا م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ايش الفايده لابس كمامات ومو لابس قفازات</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>الجدير بالذكر ان في الصين بيستخدموا ال في كل ح...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ولو ترسل صواريخ الصين وقفنا دون برج الفيصليه</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1                 حساء خفافيش الصين ابداع القرف يعني\n",
       "1      1  اجراءات هكذا استعدت مصر لمواجهه فيروس كورونا م...\n",
       "2      1            ايش الفايده لابس كمامات ومو لابس قفازات\n",
       "3      0  الجدير بالذكر ان في الصين بيستخدموا ال في كل ح...\n",
       "4      0       ولو ترسل صواريخ الصين وقفنا دون برج الفيصليه"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_test_set = read_file(\"test/strat_test_set.csv\")\n",
    "strat_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03232335",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text = list(strat_test_set['text'])\n",
    "y_test = strat_test_set['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "787f4f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['حساء خفافيش الصين ابداع القرف يعني', 'اجراءات هكذا استعدت مصر لمواجهه فيروس كورونا مرايتي', 'ايش الفايده لابس كمامات ومو لابس قفازات']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['حساء', 'خفافيش', 'الصين', 'ابداع', 'القرف', 'يعني'], ['اجراءات', 'هكذا', 'استعدت', 'مصر', 'لمواجهه', 'فيروس', 'كورونا', 'مرايتي'], ['ايش', 'الفايده', 'لابس', 'كمامات', 'ومو', 'لابس', 'قفازات']]\n",
      "==================================================\n",
      "full gram tokenization : \n",
      " [['حساء', 'خفافيش', 'الصين', 'ابداع', 'القرف', 'يعني', 'حساء_خفافيش', 'خفافيش_الصين', 'الصين_ابداع', 'ابداع_القرف', 'القرف_يعني', 'حساء_خفافيش_الصين', 'خفافيش_الصين_ابداع', 'الصين_ابداع_القرف', 'ابداع_القرف_يعني'], ['اجراءات', 'هكذا', 'استعدت', 'مصر', 'لمواجهه', 'فيروس', 'كورونا', 'مرايتي', 'اجراءات_هكذا', 'هكذا_استعدت', 'استعدت_مصر', 'مصر_لمواجهه', 'لمواجهه_فيروس', 'فيروس_كورونا', 'كورونا_مرايتي', 'اجراءات_هكذا_استعدت', 'هكذا_استعدت_مصر', 'استعدت_مصر_لمواجهه', 'مصر_لمواجهه_فيروس', 'لمواجهه_فيروس_كورونا', 'فيروس_كورونا_مرايتي'], ['ايش', 'الفايده', 'لابس', 'كمامات', 'ومو', 'لابس', 'قفازات', 'ايش_الفايده', 'الفايده_لابس', 'لابس_كمامات', 'كمامات_ومو', 'ومو_لابس', 'لابس_قفازات', 'ايش_الفايده_لابس', 'الفايده_لابس_كمامات', 'لابس_كمامات_ومو', 'كمامات_ومو_لابس', 'ومو_لابس_قفازات']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "X_test_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(X_test_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", X_test_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "fullgram_X_test_text_tokenized = get_all_ngrams(X_test_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acf98a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(180, 237, 300)\n",
      "(180, 71100)\n",
      "==================================================\n",
      "[-0.371    0.2062   1.604    0.5107   0.0869   0.373    0.25     0.748\n",
      " -0.935    0.4568  -0.11237  1.158   -0.2081  -0.3645   0.6475  -0.6924\n",
      "  0.3015   0.619   -0.0635   0.5483   0.847    0.7573   0.5356   0.5234\n",
      " -0.2281  -0.1232  -0.154   -0.7466   0.366    1.154    0.1589   0.765\n",
      " -0.327   -0.186   -0.0856   0.5527  -0.2445   0.62     1.173    0.1084\n",
      " -0.8857  -0.05118 -0.1131  -1.387    0.0661   0.2197  -0.498    0.4775\n",
      " -0.448    0.0724 ]\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.9111111111111111\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "rezk_model = keras_load_model(\"../test_models/dl_models/run_with_rezk_word2vec_sgd_lstm_with_batch_learning_rate=0.1__model.h5\"  )\n",
    "\n",
    "X_test_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_X_test_text_tokenized, max_len_str)\n",
    "X_test_embed_matrix = X_test_embed_matrix.reshape(X_test_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "\n",
    "keras_f1_score_result(rezk_model, X_test_embed_matrix, y_test)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebeaa86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
