{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../preprocess_assets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada8bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\n",
    "import tensorflow as tf\n",
    "from features_extraction import *\n",
    "from data_shuffling_split import *\n",
    "from ara_vec_preprocess_configs import *\n",
    "from ml_modeling import *\n",
    "from keras_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c616fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set = read_file(\"train/strat_train_set.csv\")\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72a0f020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of instances in the training data after StratifiedShuffleSplit are:  9608\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   197\n",
      "The number of trainin instances:  9608\n",
      "The number of validation instances:  197\n",
      "The number of trainin labels :  9608\n",
      "The number of validation labels :  197\n"
     ]
    }
   ],
   "source": [
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35e43c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " [' 3 : 00 عصرا مع يوسف الحسيني تابعونا …', 'بمناسبه #الامتحانات اللي اتاجلت و #رجعوا في كلامهم تاني -_- #حقوق_انجليزي_لحد_ما_يجيبوا_مدرس_انجليزي', '#باسم_طلع_حرامي باسييم مهزوز علشان اتفيش']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['3', ':', '00', 'عصرا', 'مع', 'يوسف', 'الحسيني', 'تابعونا', '…'], ['بمناسبه', '#', 'الامتحانات', 'اللي', 'اتاجلت', 'و', '#', 'رجعوا', 'في', 'كلامهم', 'تاني', '-_-', '#', 'حقوق_انجليزي_لحد_ما_يجيبوا_مدرس_انجليزي'], ['#', 'باسم_طلع_حرامي', 'باسييم', 'مهزوز', 'علشان', 'اتفيش']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['_ 25 جميع ارقام #الاهلي #مصر في #دوري_ابطال_افريقيا من البدايه وحتي 2017 ', '# الانتخابات العراقيه في مرمي نيران داعش : بغداد - ارتفع منسوب العنف في العراق مع اقتراب الموعد #دبي', 'الطوابير اليوم عند المقار الانتخابيه اقصر من اليوم الاول في المرحله الاولي شاركوا ارجوكم، مشاركتكم نجحت الاولي وهتنجح الثانيه ان شاء الله']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['_', '25', 'جميع', 'ارقام', '#', 'الاهلي', '#', 'مصر', 'في', '#', 'دوري_ابطال_افريقيا', 'من', 'البدايه', 'وحتي', '2017'], ['#', 'الانتخابات', 'العراقيه', 'في', 'مرمي', 'نيران', 'داعش', ':', 'بغداد', '-', 'ارتفع', 'منسوب', 'العنف', 'في', 'العراق', 'مع', 'اقتراب', 'الموعد', '#', 'دبي'], ['الطوابير', 'اليوم', 'عند', 'المقار', 'الانتخابيه', 'اقصر', 'من', 'اليوم', 'الاول', 'في', 'المرحله', 'الاولي', 'شاركوا', 'ارجوكم،', 'مشاركتكم', 'نجحت', 'الاولي', 'وهتنجح', 'الثانيه', 'ان', 'شاء', 'الله']]\n",
      "full gram tokenization : \n",
      " [['3', ':', '00', 'عصرا', 'مع', 'يوسف', 'الحسيني', 'تابعونا', '…', '3_:', ':_00', '00_عصرا', 'عصرا_مع', 'مع_يوسف', 'يوسف_الحسيني', 'الحسيني_تابعونا', 'تابعونا_…', '3_:_00', ':_00_عصرا', '00_عصرا_مع', 'عصرا_مع_يوسف', 'مع_يوسف_الحسيني', 'يوسف_الحسيني_تابعونا', 'الحسيني_تابعونا_…'], ['بمناسبه', '#', 'الامتحانات', 'اللي', 'اتاجلت', 'و', '#', 'رجعوا', 'في', 'كلامهم', 'تاني', '-_-', '#', 'حقوق_انجليزي_لحد_ما_يجيبوا_مدرس_انجليزي', 'بمناسبه_#', '#_الامتحانات', 'الامتحانات_اللي', 'اللي_اتاجلت', 'اتاجلت_و', 'و_#', '#_رجعوا', 'رجعوا_في', 'في_كلامهم', 'كلامهم_تاني', 'تاني_-_-', '-_-_#', '#_حقوق_انجليزي_لحد_ما_يجيبوا_مدرس_انجليزي', 'بمناسبه_#_الامتحانات', '#_الامتحانات_اللي', 'الامتحانات_اللي_اتاجلت', 'اللي_اتاجلت_و', 'اتاجلت_و_#', 'و_#_رجعوا', '#_رجعوا_في', 'رجعوا_في_كلامهم', 'في_كلامهم_تاني', 'كلامهم_تاني_-_-', 'تاني_-_-_#', '-_-_#_حقوق_انجليزي_لحد_ما_يجيبوا_مدرس_انجليزي'], ['#', 'باسم_طلع_حرامي', 'باسييم', 'مهزوز', 'علشان', 'اتفيش', '#_باسم_طلع_حرامي', 'باسم_طلع_حرامي_باسييم', 'باسييم_مهزوز', 'مهزوز_علشان', 'علشان_اتفيش', '#_باسم_طلع_حرامي_باسييم', 'باسم_طلع_حرامي_باسييم_مهزوز', 'باسييم_مهزوز_علشان', 'مهزوز_علشان_اتفيش']]\n",
      "==================================================\n",
      "full gram tokenization : \n",
      " [['_', '25', 'جميع', 'ارقام', '#', 'الاهلي', '#', 'مصر', 'في', '#', 'دوري_ابطال_افريقيا', 'من', 'البدايه', 'وحتي', '2017', '__25', '25_جميع', 'جميع_ارقام', 'ارقام_#', '#_الاهلي', 'الاهلي_#', '#_مصر', 'مصر_في', 'في_#', '#_دوري_ابطال_افريقيا', 'دوري_ابطال_افريقيا_من', 'من_البدايه', 'البدايه_وحتي', 'وحتي_2017', '__25_جميع', '25_جميع_ارقام', 'جميع_ارقام_#', 'ارقام_#_الاهلي', '#_الاهلي_#', 'الاهلي_#_مصر', '#_مصر_في', 'مصر_في_#', 'في_#_دوري_ابطال_افريقيا', '#_دوري_ابطال_افريقيا_من', 'دوري_ابطال_افريقيا_من_البدايه', 'من_البدايه_وحتي', 'البدايه_وحتي_2017'], ['#', 'الانتخابات', 'العراقيه', 'في', 'مرمي', 'نيران', 'داعش', ':', 'بغداد', '-', 'ارتفع', 'منسوب', 'العنف', 'في', 'العراق', 'مع', 'اقتراب', 'الموعد', '#', 'دبي', '#_الانتخابات', 'الانتخابات_العراقيه', 'العراقيه_في', 'في_مرمي', 'مرمي_نيران', 'نيران_داعش', 'داعش_:', ':_بغداد', 'بغداد_-', '-_ارتفع', 'ارتفع_منسوب', 'منسوب_العنف', 'العنف_في', 'في_العراق', 'العراق_مع', 'مع_اقتراب', 'اقتراب_الموعد', 'الموعد_#', '#_دبي', '#_الانتخابات_العراقيه', 'الانتخابات_العراقيه_في', 'العراقيه_في_مرمي', 'في_مرمي_نيران', 'مرمي_نيران_داعش', 'نيران_داعش_:', 'داعش_:_بغداد', ':_بغداد_-', 'بغداد_-_ارتفع', '-_ارتفع_منسوب', 'ارتفع_منسوب_العنف', 'منسوب_العنف_في', 'العنف_في_العراق', 'في_العراق_مع', 'العراق_مع_اقتراب', 'مع_اقتراب_الموعد', 'اقتراب_الموعد_#', 'الموعد_#_دبي'], ['الطوابير', 'اليوم', 'عند', 'المقار', 'الانتخابيه', 'اقصر', 'من', 'اليوم', 'الاول', 'في', 'المرحله', 'الاولي', 'شاركوا', 'ارجوكم،', 'مشاركتكم', 'نجحت', 'الاولي', 'وهتنجح', 'الثانيه', 'ان', 'شاء', 'الله', 'الطوابير_اليوم', 'اليوم_عند', 'عند_المقار', 'المقار_الانتخابيه', 'الانتخابيه_اقصر', 'اقصر_من', 'من_اليوم', 'اليوم_الاول', 'الاول_في', 'في_المرحله', 'المرحله_الاولي', 'الاولي_شاركوا', 'شاركوا_ارجوكم،', 'ارجوكم،_مشاركتكم', 'مشاركتكم_نجحت', 'نجحت_الاولي', 'الاولي_وهتنجح', 'وهتنجح_الثانيه', 'الثانيه_ان', 'ان_شاء', 'شاء_الله', 'الطوابير_اليوم_عند', 'اليوم_عند_المقار', 'عند_المقار_الانتخابيه', 'المقار_الانتخابيه_اقصر', 'الانتخابيه_اقصر_من', 'اقصر_من_اليوم', 'من_اليوم_الاول', 'اليوم_الاول_في', 'الاول_في_المرحله', 'في_المرحله_الاولي', 'المرحله_الاولي_شاركوا', 'الاولي_شاركوا_ارجوكم،', 'شاركوا_ارجوكم،_مشاركتكم', 'ارجوكم،_مشاركتكم_نجحت', 'مشاركتكم_نجحت_الاولي', 'نجحت_الاولي_وهتنجح', 'الاولي_وهتنجح_الثانيه', 'وهتنجح_الثانيه_ان', 'الثانيه_ان_شاء', 'ان_شاء_الله']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])\n",
    "\n",
    "fullgram_x_train_text_tokenized = get_all_ngrams(x_train_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "fullgram_x_val_text_tokenized = get_all_ngrams(x_val_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_val_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5daeb3",
   "metadata": {},
   "source": [
    "# Our CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd885bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_word2vec_model = load_word2vec_model(\"../word2vec_models/rezk/skipgram/skip_gram_fullgram_vec_size_300-d_min_count_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78e486cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(9608, 132, 300)\n",
      "(9608, 39600)\n",
      "==================================================\n",
      "[-0.03217  -0.01317  -0.2024   -0.06274   0.2361    0.0524   -0.0534\n",
      "  0.02296   0.1979    0.03778  -0.07263   0.0847    0.1415    0.04736\n",
      " -0.03717   0.03058   0.2634    0.1676    0.2079   -0.1761    0.11365\n",
      " -0.11273  -0.00891   0.0977    0.0878    0.007427  0.05176  -0.287\n",
      " -0.1898    0.1548    0.1254    0.1622   -0.00457  -0.03232  -0.0723\n",
      " -0.3003    0.02701  -0.2847    0.0875   -0.008804 -0.09656  -0.04337\n",
      "  0.13      0.03595   0.1059    0.3813   -0.1231   -0.02145   0.0797\n",
      " -0.03735 ]\n",
      "==================================================\n",
      "(197, 132, 300)\n",
      "(197, 39600)\n",
      "==================================================\n",
      "[-0.0417   -0.1654   -0.2788   -0.1664    0.3179   -0.02107  -0.1172\n",
      "  0.0637    0.2058    0.161     0.00487  -0.04663   0.02643   0.1666\n",
      "  0.03102  -0.02977   0.4348    0.188    -0.03784   0.006496  0.01959\n",
      " -0.02646   0.0972    0.0683   -0.2998   -0.07324  -0.0563   -0.377\n",
      " -0.06244   0.0668    0.1251    0.2404   -0.189    -0.1329    0.3687\n",
      "  0.2095    0.3115    0.04218   0.003723  0.1471   -0.3901   -0.1722\n",
      "  0.05817  -0.1687   -0.1571    0.1569   -0.01362  -0.1765    0.05765\n",
      "  0.0436  ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 132\n",
    "word2vec_path = \"rezk/cbow/\"\n",
    "model_path_to_save = \"../ml_models_saved/\"\n",
    "hid_num_neurons = 25\n",
    "learning_rate = .00005\n",
    "epochs = 10\n",
    "estimators = voting_models()\n",
    "\n",
    "performance_lr = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=5)\n",
    "RMSprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=.9)\n",
    "\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_x_val_text_tokenized, max_len_str)\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ad407",
   "metadata": {},
   "source": [
    "# Our CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651df5bc",
   "metadata": {},
   "source": [
    "# With  Rmsprob and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e14f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rezk_skipgram_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n",
      "../test_models/ml_models_saved/dl_models/tensor_logs/run_2022_05_10_05_53_54_rezk_skipgram_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 05:53:54.605220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:53:54.615256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:53:54.617094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:53:55.059442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:53:55.061236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:53:55.062955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:53:56.472794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:53:56.474585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:53:56.476293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9632 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:05:00.0, compute capability: 7.5\n",
      "2022-05-10 05:53:56.477023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:53:56.477830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9652 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 132, 300)         1200      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 132, 25)           32600     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 132, 25)          100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 132, 25)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3300)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 18)                59418     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,318\n",
      "Trainable params: 92,668\n",
      "Non-trainable params: 650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"rezk_skipgram_word2vec\", model_type=\"Rmsprob_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f04d583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 05:54:07.003769: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 13s 27ms/step - loss: 1.4904 - accuracy: 0.5541 - val_loss: 1.4810 - val_accuracy: 0.5685 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "301/301 [==============================] - 7s 24ms/step - loss: 1.1122 - accuracy: 0.6170 - val_loss: 1.0856 - val_accuracy: 0.6193 - lr: 5.0000e-05\n",
      "Epoch 3/10\n",
      "301/301 [==============================] - 7s 23ms/step - loss: 1.0373 - accuracy: 0.6394 - val_loss: 1.0453 - val_accuracy: 0.6396 - lr: 5.0000e-05\n",
      "Epoch 4/10\n",
      "301/301 [==============================] - 7s 23ms/step - loss: 0.9858 - accuracy: 0.6509 - val_loss: 1.0289 - val_accuracy: 0.6701 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "301/301 [==============================] - 7s 23ms/step - loss: 0.9452 - accuracy: 0.6584 - val_loss: 0.9981 - val_accuracy: 0.6751 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "301/301 [==============================] - 7s 23ms/step - loss: 0.9142 - accuracy: 0.6682 - val_loss: 0.9971 - val_accuracy: 0.6345 - lr: 5.0000e-05\n",
      "Epoch 7/10\n",
      "301/301 [==============================] - 7s 23ms/step - loss: 0.8875 - accuracy: 0.6729 - val_loss: 0.9742 - val_accuracy: 0.6701 - lr: 5.0000e-05\n",
      "Epoch 8/10\n",
      "301/301 [==============================] - 7s 23ms/step - loss: 0.8654 - accuracy: 0.6760 - val_loss: 0.9819 - val_accuracy: 0.6497 - lr: 5.0000e-05\n",
      "Epoch 9/10\n",
      "301/301 [==============================] - 7s 24ms/step - loss: 0.8432 - accuracy: 0.6858 - val_loss: 0.9645 - val_accuracy: 0.6650 - lr: 5.0000e-05\n",
      "Epoch 10/10\n",
      "301/301 [==============================] - 8s 25ms/step - loss: 0.8310 - accuracy: 0.6859 - val_loss: 0.9742 - val_accuracy: 0.6751 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ac6bc",
   "metadata": {},
   "source": [
    "# Bakr CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a20c10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bakr_word2vec_model = load_word2vec_model(\"../word2vec_models/bakr/skipgram/full_grams_sg_300_twitter.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920bfb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(9608, 132, 300)\n",
      "(9608, 39600)\n",
      "==================================================\n",
      "[ 0.0719   -0.3208   -0.296     0.3457   -0.09485  -0.2087    0.2418\n",
      "  0.4934    0.4187    0.1384   -0.1022    0.1649    0.05237   0.41\n",
      "  0.4453   -0.4553    0.5864   -0.1599    0.1877   -0.1472    0.202\n",
      "  0.1216   -0.12006  -0.1691   -0.2438    0.2272    0.0591   -0.468\n",
      "  0.05576  -0.3276   -0.5435    0.2308   -0.3484    0.587    -0.2744\n",
      "  0.4648    0.164    -0.2766    0.002684  0.3855   -0.1882    0.3435\n",
      "  0.1125    0.2607   -0.3838   -0.0414    0.3545    0.4075    0.0791\n",
      " -0.1009  ]\n",
      "==================================================\n",
      "(197, 132, 300)\n",
      "(197, 39600)\n",
      "==================================================\n",
      "[ 0.194   -0.04285  0.1187   0.6753  -0.5776   0.03906  0.3904  -0.06036\n",
      "  0.0907  -0.1181  -0.1202   0.05658  0.0953  -0.0804  -0.077    0.11505\n",
      " -0.7163   0.1592  -0.282    0.5913  -0.3784  -0.1824  -0.3767  -0.2401\n",
      " -0.2527  -0.3943  -0.0648   0.1337   0.1364   0.04095 -0.05072  0.3215\n",
      " -0.07935  0.0795  -0.11865  0.08307 -0.03586 -0.387    0.3235  -0.39\n",
      "  0.3655   0.04654  0.5005   0.4187  -0.2798   0.01662  0.01596  0.1388\n",
      " -0.3381   0.4678 ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 132\n",
    "word2vec_path = \"bakr/cbow/\"\n",
    "model_path_to_save = \"../ml_models_saved/\"\n",
    "hid_num_neurons = 25\n",
    "learning_rate = .00005\n",
    "epochs = 10\n",
    "estimators = voting_models()\n",
    "\n",
    "performance_lr = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=5)\n",
    "RMSprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=.9)\n",
    "\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(bakr_word2vec_model, fullgram_x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(bakr_word2vec_model, fullgram_x_val_text_tokenized, max_len_str)\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972721f",
   "metadata": {},
   "source": [
    "# With  Rmsprob and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de6dc787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bakr_skipgram_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n",
      "../test_models/ml_models_saved/dl_models/tensor_logs/run_2022_05_10_05_56_46_bakr_skipgram_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_2 (Batc  (None, 132, 300)         1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 132, 25)           32600     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 132, 25)          100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 132, 25)           0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3300)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 18)                59418     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,318\n",
      "Trainable params: 92,668\n",
      "Non-trainable params: 650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"bakr_skipgram_word2vec\", model_type=\"Rmsprob_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c7fa560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "301/301 [==============================] - 11s 28ms/step - loss: 1.4156 - accuracy: 0.5657 - val_loss: 1.9369 - val_accuracy: 0.4061 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 1.1535 - accuracy: 0.6166 - val_loss: 1.1017 - val_accuracy: 0.6294 - lr: 5.0000e-05\n",
      "Epoch 3/10\n",
      "301/301 [==============================] - 8s 25ms/step - loss: 1.0818 - accuracy: 0.6305 - val_loss: 1.0789 - val_accuracy: 0.6650 - lr: 5.0000e-05\n",
      "Epoch 4/10\n",
      "301/301 [==============================] - 8s 25ms/step - loss: 1.0216 - accuracy: 0.6400 - val_loss: 1.0435 - val_accuracy: 0.6447 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "301/301 [==============================] - 7s 25ms/step - loss: 0.9804 - accuracy: 0.6519 - val_loss: 1.0305 - val_accuracy: 0.6802 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "301/301 [==============================] - 7s 25ms/step - loss: 0.9462 - accuracy: 0.6584 - val_loss: 1.0070 - val_accuracy: 0.6548 - lr: 5.0000e-05\n",
      "Epoch 7/10\n",
      "301/301 [==============================] - 8s 26ms/step - loss: 0.9111 - accuracy: 0.6662 - val_loss: 0.9990 - val_accuracy: 0.6548 - lr: 5.0000e-05\n",
      "Epoch 8/10\n",
      "301/301 [==============================] - 7s 24ms/step - loss: 0.8881 - accuracy: 0.6802 - val_loss: 0.9911 - val_accuracy: 0.6548 - lr: 5.0000e-05\n",
      "Epoch 9/10\n",
      "301/301 [==============================] - 7s 24ms/step - loss: 0.8678 - accuracy: 0.6811 - val_loss: 0.9810 - val_accuracy: 0.6396 - lr: 5.0000e-05\n",
      "Epoch 10/10\n",
      "301/301 [==============================] - 7s 23ms/step - loss: 0.8445 - accuracy: 0.6923 - val_loss: 0.9855 - val_accuracy: 0.6193 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c18530",
   "metadata": {},
   "source": [
    "# Muhammed CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "554093f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "muhammed_word2vec_model = load_word2vec_model(\"../word2vec_models/muhammed/skipgram/w2v_SG_300_3_400_10.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4f3dc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(9608, 132, 300)\n",
      "(9608, 39600)\n",
      "==================================================\n",
      "[-0.1059    0.779     0.5645    0.2186    0.064    -0.383     0.03687\n",
      "  0.3027   -0.02603  -0.1044   -0.1112   -0.1466   -0.1012   -0.1087\n",
      " -0.03818  -0.2778   -0.0231   -0.3596    0.0175    0.437    -0.1844\n",
      "  0.3684    0.2272   -0.1696    0.0724    0.1407    0.2087    0.5664\n",
      "  0.3533   -0.189    -0.1112    0.3716   -0.8423    0.2876   -0.2793\n",
      " -0.412    -0.1206    0.3132    0.626    -0.086    -0.1621   -0.1791\n",
      "  0.003435  0.1247   -0.344     0.5044    0.513     0.1846   -0.1996\n",
      "  0.1203  ]\n",
      "==================================================\n",
      "(197, 132, 300)\n",
      "(197, 39600)\n",
      "==================================================\n",
      "[-0.1069     0.1539     0.0705     0.11505   -0.1282     0.1267\n",
      "  0.1873     0.0582    -0.2014     0.275     -0.1477    -0.1154\n",
      " -0.01935   -0.02487    0.4915     0.368      0.4028    -0.0008845\n",
      " -0.09784    0.426      0.01115    0.1553     0.01869   -0.01886\n",
      "  0.08734   -0.3423     0.233      0.266      0.2019     0.3665\n",
      "  0.06775    0.1458     0.04166    0.1588     0.1333     0.1191\n",
      " -0.07104    0.2426     0.2003    -0.12445    0.2161     0.3423\n",
      "  0.2142    -0.3696     0.1232     0.1885     0.0625    -0.1348\n",
      " -0.2279    -0.03543  ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 132\n",
    "word2vec_path = \"muhammed/cbow/\"\n",
    "model_path_to_save = \"../ml_models_saved/\"\n",
    "hid_num_neurons = 25\n",
    "learning_rate = .00005\n",
    "epochs = 10\n",
    "estimators = voting_models()\n",
    "\n",
    "performance_lr = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=5)\n",
    "SGD_optimizer     =tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "Adam_optimizer = tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.999)\n",
    "RMSprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=.9)\n",
    "\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(muhammed_word2vec_model, fullgram_x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(muhammed_word2vec_model, fullgram_x_val_text_tokenized, max_len_str)\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638467b1",
   "metadata": {},
   "source": [
    "# With  Rmsprob and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80cf5b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "muhammed_skipgram_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n",
      "../test_models/ml_models_saved/dl_models/tensor_logs/run_2022_05_10_05_58_16_muhammed_skipgram_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_4 (Batc  (None, 132, 300)         1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 132, 25)           32600     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 132, 25)          100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 132, 25)           0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 3300)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 18)                59418     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,318\n",
      "Trainable params: 92,668\n",
      "Non-trainable params: 650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"muhammed_skipgram_word2vec\", model_type=\"Rmsprob_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f24f920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "301/301 [==============================] - 11s 26ms/step - loss: 1.4786 - accuracy: 0.5568 - val_loss: 4.7477 - val_accuracy: 0.0000e+00 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "301/301 [==============================] - 7s 23ms/step - loss: 1.1690 - accuracy: 0.6123 - val_loss: 1.1037 - val_accuracy: 0.6142 - lr: 5.0000e-05\n",
      "Epoch 3/10\n",
      "301/301 [==============================] - 7s 23ms/step - loss: 1.0933 - accuracy: 0.6326 - val_loss: 1.0886 - val_accuracy: 0.6447 - lr: 5.0000e-05\n",
      "Epoch 4/10\n",
      "301/301 [==============================] - 7s 23ms/step - loss: 1.0321 - accuracy: 0.6431 - val_loss: 1.0488 - val_accuracy: 0.6599 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "301/301 [==============================] - 7s 23ms/step - loss: 0.9981 - accuracy: 0.6459 - val_loss: 1.0445 - val_accuracy: 0.6548 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "301/301 [==============================] - 7s 23ms/step - loss: 0.9707 - accuracy: 0.6551 - val_loss: 1.0186 - val_accuracy: 0.6599 - lr: 5.0000e-05\n",
      "Epoch 7/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 0.9475 - accuracy: 0.6604 - val_loss: 1.0044 - val_accuracy: 0.6497 - lr: 5.0000e-05\n",
      "Epoch 8/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 0.9204 - accuracy: 0.6716 - val_loss: 1.0116 - val_accuracy: 0.6447 - lr: 5.0000e-05\n",
      "Epoch 9/10\n",
      "301/301 [==============================] - 8s 28ms/step - loss: 0.8926 - accuracy: 0.6733 - val_loss: 1.0019 - val_accuracy: 0.6497 - lr: 5.0000e-05\n",
      "Epoch 10/10\n",
      "301/301 [==============================] - 8s 26ms/step - loss: 0.8737 - accuracy: 0.6817 - val_loss: 0.9978 - val_accuracy: 0.6548 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd01899",
   "metadata": {},
   "source": [
    "# Load best model & predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c60f515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نرجوا عدم متابعه وحظر حسابات : المباحث تابعني ...</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>المسلماني اغلي متحدث اعلامي للرئيس واشهر كذاب ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الاحد 3 يوليو : ابو الفتوح في ندوه بالمؤتمر ال...</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#كبسوله_صحيه #صحه #طفل #طفلي #نوم ##نوبه #نصيحه</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#اللي_رافضين_السيسي_رئيس_بيعملوا_فولوا_لبعض #م...</td>\n",
       "      <td>1</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label classes\n",
       "0  نرجوا عدم متابعه وحظر حسابات : المباحث تابعني ...      2     OBJ\n",
       "1  المسلماني اغلي متحدث اعلامي للرئيس واشهر كذاب ...      0     NEG\n",
       "2  الاحد 3 يوليو : ابو الفتوح في ندوه بالمؤتمر ال...      2     OBJ\n",
       "3    #كبسوله_صحيه #صحه #طفل #طفلي #نوم ##نوبه #نصيحه      2     OBJ\n",
       "4  #اللي_رافضين_السيسي_رئيس_بيعملوا_فولوا_لبعض #م...      1     NEU"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_test_set = read_file(\"test/strat_test_set.csv\")\n",
    "strat_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92102c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text = list(strat_test_set['text'])\n",
    "y_test = strat_test_set['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22f4e245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['نرجوا عدم متابعه وحظر حسابات : المباحث تابعني واتابعك زياده المتابعين الاخبار المحليه #الشعب_يقول_كلمته #جماعه_انصار_بيت_طنيطر #الرياض #جده', 'المسلماني اغلي متحدث اعلامي للرئيس واشهر كذاب ومنافق ومضلل ومحرض تحول الي قليل الادب وضع معارضي الجيش في سله القمامه كلام زباله', 'الاحد 3 يوليو : ابو الفتوح في ندوه بالمؤتمر السنوي لهندسه عين شمس 5 مساء']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['نرجوا', 'عدم', 'متابعه', 'وحظر', 'حسابات', ':', 'المباحث', 'تابعني', 'واتابعك', 'زياده', 'المتابعين', 'الاخبار', 'المحليه', '#', 'الشعب_يقول_كلمته', '#', 'جماعه_انصار_بيت_طنيطر', '#', 'الرياض', '#', 'جده'], ['المسلماني', 'اغلي', 'متحدث', 'اعلامي', 'للرئيس', 'واشهر', 'كذاب', 'ومنافق', 'ومضلل', 'ومحرض', 'تحول', 'الي', 'قليل', 'الادب', 'وضع', 'معارضي', 'الجيش', 'في', 'سله', 'القمامه', 'كلام', 'زباله'], ['الاحد', '3', 'يوليو', ':', 'ابو', 'الفتوح', 'في', 'ندوه', 'بالمؤتمر', 'السنوي', 'لهندسه', 'عين', 'شمس', '5', 'مساء']]\n",
      "==================================================\n",
      "full gram tokenization : \n",
      " [['نرجوا', 'عدم', 'متابعه', 'وحظر', 'حسابات', ':', 'المباحث', 'تابعني', 'واتابعك', 'زياده', 'المتابعين', 'الاخبار', 'المحليه', '#', 'الشعب_يقول_كلمته', '#', 'جماعه_انصار_بيت_طنيطر', '#', 'الرياض', '#', 'جده', 'نرجوا_عدم', 'عدم_متابعه', 'متابعه_وحظر', 'وحظر_حسابات', 'حسابات_:', ':_المباحث', 'المباحث_تابعني', 'تابعني_واتابعك', 'واتابعك_زياده', 'زياده_المتابعين', 'المتابعين_الاخبار', 'الاخبار_المحليه', 'المحليه_#', '#_الشعب_يقول_كلمته', 'الشعب_يقول_كلمته_#', '#_جماعه_انصار_بيت_طنيطر', 'جماعه_انصار_بيت_طنيطر_#', '#_الرياض', 'الرياض_#', '#_جده', 'نرجوا_عدم_متابعه', 'عدم_متابعه_وحظر', 'متابعه_وحظر_حسابات', 'وحظر_حسابات_:', 'حسابات_:_المباحث', ':_المباحث_تابعني', 'المباحث_تابعني_واتابعك', 'تابعني_واتابعك_زياده', 'واتابعك_زياده_المتابعين', 'زياده_المتابعين_الاخبار', 'المتابعين_الاخبار_المحليه', 'الاخبار_المحليه_#', 'المحليه_#_الشعب_يقول_كلمته', '#_الشعب_يقول_كلمته_#', 'الشعب_يقول_كلمته_#_جماعه_انصار_بيت_طنيطر', '#_جماعه_انصار_بيت_طنيطر_#', 'جماعه_انصار_بيت_طنيطر_#_الرياض', '#_الرياض_#', 'الرياض_#_جده'], ['المسلماني', 'اغلي', 'متحدث', 'اعلامي', 'للرئيس', 'واشهر', 'كذاب', 'ومنافق', 'ومضلل', 'ومحرض', 'تحول', 'الي', 'قليل', 'الادب', 'وضع', 'معارضي', 'الجيش', 'في', 'سله', 'القمامه', 'كلام', 'زباله', 'المسلماني_اغلي', 'اغلي_متحدث', 'متحدث_اعلامي', 'اعلامي_للرئيس', 'للرئيس_واشهر', 'واشهر_كذاب', 'كذاب_ومنافق', 'ومنافق_ومضلل', 'ومضلل_ومحرض', 'ومحرض_تحول', 'تحول_الي', 'الي_قليل', 'قليل_الادب', 'الادب_وضع', 'وضع_معارضي', 'معارضي_الجيش', 'الجيش_في', 'في_سله', 'سله_القمامه', 'القمامه_كلام', 'كلام_زباله', 'المسلماني_اغلي_متحدث', 'اغلي_متحدث_اعلامي', 'متحدث_اعلامي_للرئيس', 'اعلامي_للرئيس_واشهر', 'للرئيس_واشهر_كذاب', 'واشهر_كذاب_ومنافق', 'كذاب_ومنافق_ومضلل', 'ومنافق_ومضلل_ومحرض', 'ومضلل_ومحرض_تحول', 'ومحرض_تحول_الي', 'تحول_الي_قليل', 'الي_قليل_الادب', 'قليل_الادب_وضع', 'الادب_وضع_معارضي', 'وضع_معارضي_الجيش', 'معارضي_الجيش_في', 'الجيش_في_سله', 'في_سله_القمامه', 'سله_القمامه_كلام', 'القمامه_كلام_زباله'], ['الاحد', '3', 'يوليو', ':', 'ابو', 'الفتوح', 'في', 'ندوه', 'بالمؤتمر', 'السنوي', 'لهندسه', 'عين', 'شمس', '5', 'مساء', 'الاحد_3', '3_يوليو', 'يوليو_:', ':_ابو', 'ابو_الفتوح', 'الفتوح_في', 'في_ندوه', 'ندوه_بالمؤتمر', 'بالمؤتمر_السنوي', 'السنوي_لهندسه', 'لهندسه_عين', 'عين_شمس', 'شمس_5', '5_مساء', 'الاحد_3_يوليو', '3_يوليو_:', 'يوليو_:_ابو', ':_ابو_الفتوح', 'ابو_الفتوح_في', 'الفتوح_في_ندوه', 'في_ندوه_بالمؤتمر', 'ندوه_بالمؤتمر_السنوي', 'بالمؤتمر_السنوي_لهندسه', 'السنوي_لهندسه_عين', 'لهندسه_عين_شمس', 'عين_شمس_5', 'شمس_5_مساء']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "X_test_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(X_test_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", X_test_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "fullgram_X_test_text_tokenized = get_all_ngrams(X_test_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "191ea0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(201, 132, 300)\n",
      "(201, 39600)\n",
      "==================================================\n",
      "[-0.1384   -0.1466    0.0389    0.2112    0.3079    0.1342    0.1416\n",
      "  0.2318   -0.1287   -0.1637   -0.0872    0.1997    0.00798   0.09326\n",
      " -0.3176   -0.1678    0.388     0.5825   -0.10803   0.232    -0.276\n",
      "  0.03293   0.178    -0.2404   -0.01758  -0.158     0.02231  -0.2222\n",
      " -0.3208   -0.2766   -0.2998   -0.08307  -0.1837    0.013504  0.3416\n",
      "  0.3245   -0.533     0.003767  0.3345    0.1655   -0.03128   0.035\n",
      "  0.2615   -0.3096   -0.7563    0.1377    0.143     0.2815   -0.1858\n",
      "  0.01923 ]\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.6716417910447762\n",
      "==================================================\n",
      "==================================================\n",
      "(201, 132, 300)\n",
      "(201, 39600)\n",
      "==================================================\n",
      "[-2.0544e-01  1.7297e-01  2.9639e-01  1.8506e-01 -3.7445e-02 -2.1271e-02\n",
      "  3.2715e-01  1.3477e-01  1.0474e-01 -1.0223e-01  1.3721e-01  6.7139e-04\n",
      " -2.5171e-01  4.9829e-01  5.2490e-02 -2.9443e-01 -1.0754e-01 -5.1193e-03\n",
      "  1.7749e-01 -7.4902e-01  3.2898e-02  2.8052e-01 -5.2307e-02  3.1445e-01\n",
      " -1.0071e-01 -1.4087e-01  7.8809e-01 -1.9531e-02  2.8394e-01  7.4805e-01\n",
      " -1.1285e-01  9.2468e-02  3.3813e-01  1.6602e-02 -4.4507e-01  2.3547e-01\n",
      " -3.3057e-01 -1.1798e-01 -3.2422e-01 -1.7639e-01  2.7246e-01 -2.4261e-02\n",
      " -5.8887e-01  2.7124e-01 -3.1586e-02 -3.0914e-02 -2.8711e-01  3.4839e-01\n",
      "  1.8567e-01  3.2690e-01]\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.6666666666666666\n",
      "==================================================\n",
      "==================================================\n",
      "(201, 132, 300)\n",
      "(201, 39600)\n",
      "==================================================\n",
      "[-0.286     0.4526   -0.0344    0.0621   -0.2974    0.4211   -0.2224\n",
      "  0.0803    0.1646    0.413    -0.1711   -0.2598    0.1533   -0.28\n",
      "  0.1543    0.2384   -0.2515    0.1914    0.1561    0.2673    0.5596\n",
      " -0.4092   -0.1377    0.418     0.2708   -0.1265    0.158    -0.05118\n",
      "  0.12134   0.003025 -0.07324  -0.274    -0.6484   -0.5454   -0.258\n",
      " -0.2834    0.05045  -0.3782    0.4294    0.3928   -0.2416    0.1233\n",
      "  0.2426   -0.0917    0.3125    0.3235    0.012314  0.3557   -0.3928\n",
      " -0.006714]\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.6467661691542289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.647"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rezk_model = keras_load_model(\"../ml_models_saved/dl_models/run_with_rezk_skipgram_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05__model.h5\"  )\n",
    "bakr_model = keras_load_model(\"../ml_models_saved/dl_models/run_with_bakr_skipgram_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05__model.h5\"  )\n",
    "muhammed_model = keras_load_model(\"../ml_models_saved/dl_models/run_with_muhammed_skipgram_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05__model.h5\"  )\n",
    "\n",
    "X_test_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_X_test_text_tokenized, max_len_str)\n",
    "X_test_embed_matrix = X_test_embed_matrix.reshape(X_test_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "\n",
    "keras_f1_score_result(rezk_model, X_test_embed_matrix, y_test)\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_test_embed_matrix = text_to_matrix_using_word2vec(bakr_word2vec_model, fullgram_X_test_text_tokenized, max_len_str)\n",
    "X_test_embed_matrix = X_test_embed_matrix.reshape(X_test_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "keras_f1_score_result(bakr_model, X_test_embed_matrix, y_test)\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_test_embed_matrix = text_to_matrix_using_word2vec(muhammed_word2vec_model, fullgram_X_test_text_tokenized, max_len_str)\n",
    "X_test_embed_matrix = X_test_embed_matrix.reshape(X_test_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "keras_f1_score_result(muhammed_model, X_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfad2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
