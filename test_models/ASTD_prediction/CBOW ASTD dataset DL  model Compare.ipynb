{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec9491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../preprocess_assets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fdba3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\n",
    "import tensorflow as tf\n",
    "from features_extraction import *\n",
    "from data_shuffling_split import *\n",
    "from ara_vec_preprocess_configs import *\n",
    "from ml_modeling import *\n",
    "from keras_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e561ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>راجعت الردود فوجدت المتزمتين دينيا هم الاكثر ا...</td>\n",
       "      <td>0</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#شاهد_سكاي : #عيد_العمال تراجع اعدد العاطلين</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>يقوم د / ابو الفتوح التواصل مع مختلف التيارات ...</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>هل الشرطه والجيش سيطبق عليهم حدود الاجور دي ؟</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 -انسحاب الوفد المصري المشارك في مهرجان مالم...</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label classes\n",
       "0  راجعت الردود فوجدت المتزمتين دينيا هم الاكثر ا...      0     NEG\n",
       "1       #شاهد_سكاي : #عيد_العمال تراجع اعدد العاطلين      2     OBJ\n",
       "2  يقوم د / ابو الفتوح التواصل مع مختلف التيارات ...      2     OBJ\n",
       "3     هل الشرطه والجيش سيطبق عليهم حدود الاجور دي ؟       2     OBJ\n",
       "4   1 -انسحاب الوفد المصري المشارك في مهرجان مالم...      2     OBJ"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set = read_file(\"train/strat_train_set.csv\")\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe701490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of instances in the training data after StratifiedShuffleSplit are:  9608\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   197\n",
      "The number of trainin instances:  9608\n",
      "The number of validation instances:  197\n",
      "The number of trainin labels :  9608\n",
      "The number of validation labels :  197\n"
     ]
    }
   ],
   "source": [
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac23dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['عرض الاسبوع 40 الف متابع #شوارعنا #فن_تتقنه_النساء #ذلك_الشخص #شي_ودك_تجربه #بوح #درر #عجبني #حلو #مكه [ 2059853 ] ', 'نودع اليوم فقيد الصحافه المصريه صاحب الكلمه الحره الاستاذ سلامه احمد سلامه الي مثواه الاخير وهذا اقل ما يمكن ان نقدمه لقلمه المحترم', 'انا رفضت وقف فيلم السبكي ورافضه لالغاء مشهد رفع مبارك العلم في سينا انتوا فاكرنها خطوبه وبتقطعوا الصور ده ايه القرف ده #مرار_طافح']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['عرض', 'الاسبوع', '40', 'الف', 'متابع', '#', 'شوارعنا', '#', 'فن_تتقنه_النساء', '#', 'ذلك_الشخص', '#', 'شي_ودك_تجربه', '#', 'بوح', '#', 'درر', '#', 'عجبني', '#', 'حلو', '#', 'مكه', '[', '2059853', ']'], ['نودع', 'اليوم', 'فقيد', 'الصحافه', 'المصريه', 'صاحب', 'الكلمه', 'الحره', 'الاستاذ', 'سلامه', 'احمد', 'سلامه', 'الي', 'مثواه', 'الاخير', 'وهذا', 'اقل', 'ما', 'يمكن', 'ان', 'نقدمه', 'لقلمه', 'المحترم'], ['انا', 'رفضت', 'وقف', 'فيلم', 'السبكي', 'ورافضه', 'لالغاء', 'مشهد', 'رفع', 'مبارك', 'العلم', 'في', 'سينا', 'انتوا', 'فاكرنها', 'خطوبه', 'وبتقطعوا', 'الصور', 'ده', 'ايه', 'القرف', 'ده', '#', 'مرار_طافح']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['اول ما الاخواني يتزنق يقولك هناك ملائكه تسجل ماتقوله طيب مش هي الملايكه بتسجل كذب قياداتكم ولا هي بتسجل لنا احنا بساتقوا الله يااخوه', 'ارجو من الشبكات اللي بتدعي انها اخباريه انها تشوف الكلام اللي بيقولوه علي لساني اتقال هنا في علي تويتر و لا لا', 'مقال #عمرو_حمزاوي يوميات الارهاب والقمع مسموعا من موقع #اقرا_لي']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['اول', 'ما', 'الاخواني', 'يتزنق', 'يقولك', 'هناك', 'ملائكه', 'تسجل', 'ماتقوله', 'طيب', 'مش', 'هي', 'الملايكه', 'بتسجل', 'كذب', 'قياداتكم', 'ولا', 'هي', 'بتسجل', 'لنا', 'احنا', 'بساتقوا', 'الله', 'يااخوه'], ['ارجو', 'من', 'الشبكات', 'اللي', 'بتدعي', 'انها', 'اخباريه', 'انها', 'تشوف', 'الكلام', 'اللي', 'بيقولوه', 'علي', 'لساني', 'اتقال', 'هنا', 'في', 'علي', 'تويتر', 'و', 'لا', 'لا'], ['مقال', '#', 'عمرو_حمزاوي', 'يوميات', 'الارهاب', 'والقمع', 'مسموعا', 'من', 'موقع', '#', 'اقرا_لي']]\n",
      "full gram tokenization : \n",
      " [['عرض', 'الاسبوع', '40', 'الف', 'متابع', '#', 'شوارعنا', '#', 'فن_تتقنه_النساء', '#', 'ذلك_الشخص', '#', 'شي_ودك_تجربه', '#', 'بوح', '#', 'درر', '#', 'عجبني', '#', 'حلو', '#', 'مكه', '[', '2059853', ']', 'عرض_الاسبوع', 'الاسبوع_40', '40_الف', 'الف_متابع', 'متابع_#', '#_شوارعنا', 'شوارعنا_#', '#_فن_تتقنه_النساء', 'فن_تتقنه_النساء_#', '#_ذلك_الشخص', 'ذلك_الشخص_#', '#_شي_ودك_تجربه', 'شي_ودك_تجربه_#', '#_بوح', 'بوح_#', '#_درر', 'درر_#', '#_عجبني', 'عجبني_#', '#_حلو', 'حلو_#', '#_مكه', 'مكه_[', '[_2059853', '2059853_]', 'عرض_الاسبوع_40', 'الاسبوع_40_الف', '40_الف_متابع', 'الف_متابع_#', 'متابع_#_شوارعنا', '#_شوارعنا_#', 'شوارعنا_#_فن_تتقنه_النساء', '#_فن_تتقنه_النساء_#', 'فن_تتقنه_النساء_#_ذلك_الشخص', '#_ذلك_الشخص_#', 'ذلك_الشخص_#_شي_ودك_تجربه', '#_شي_ودك_تجربه_#', 'شي_ودك_تجربه_#_بوح', '#_بوح_#', 'بوح_#_درر', '#_درر_#', 'درر_#_عجبني', '#_عجبني_#', 'عجبني_#_حلو', '#_حلو_#', 'حلو_#_مكه', '#_مكه_[', 'مكه_[_2059853', '[_2059853_]'], ['نودع', 'اليوم', 'فقيد', 'الصحافه', 'المصريه', 'صاحب', 'الكلمه', 'الحره', 'الاستاذ', 'سلامه', 'احمد', 'سلامه', 'الي', 'مثواه', 'الاخير', 'وهذا', 'اقل', 'ما', 'يمكن', 'ان', 'نقدمه', 'لقلمه', 'المحترم', 'نودع_اليوم', 'اليوم_فقيد', 'فقيد_الصحافه', 'الصحافه_المصريه', 'المصريه_صاحب', 'صاحب_الكلمه', 'الكلمه_الحره', 'الحره_الاستاذ', 'الاستاذ_سلامه', 'سلامه_احمد', 'احمد_سلامه', 'سلامه_الي', 'الي_مثواه', 'مثواه_الاخير', 'الاخير_وهذا', 'وهذا_اقل', 'اقل_ما', 'ما_يمكن', 'يمكن_ان', 'ان_نقدمه', 'نقدمه_لقلمه', 'لقلمه_المحترم', 'نودع_اليوم_فقيد', 'اليوم_فقيد_الصحافه', 'فقيد_الصحافه_المصريه', 'الصحافه_المصريه_صاحب', 'المصريه_صاحب_الكلمه', 'صاحب_الكلمه_الحره', 'الكلمه_الحره_الاستاذ', 'الحره_الاستاذ_سلامه', 'الاستاذ_سلامه_احمد', 'سلامه_احمد_سلامه', 'احمد_سلامه_الي', 'سلامه_الي_مثواه', 'الي_مثواه_الاخير', 'مثواه_الاخير_وهذا', 'الاخير_وهذا_اقل', 'وهذا_اقل_ما', 'اقل_ما_يمكن', 'ما_يمكن_ان', 'يمكن_ان_نقدمه', 'ان_نقدمه_لقلمه', 'نقدمه_لقلمه_المحترم'], ['انا', 'رفضت', 'وقف', 'فيلم', 'السبكي', 'ورافضه', 'لالغاء', 'مشهد', 'رفع', 'مبارك', 'العلم', 'في', 'سينا', 'انتوا', 'فاكرنها', 'خطوبه', 'وبتقطعوا', 'الصور', 'ده', 'ايه', 'القرف', 'ده', '#', 'مرار_طافح', 'انا_رفضت', 'رفضت_وقف', 'وقف_فيلم', 'فيلم_السبكي', 'السبكي_ورافضه', 'ورافضه_لالغاء', 'لالغاء_مشهد', 'مشهد_رفع', 'رفع_مبارك', 'مبارك_العلم', 'العلم_في', 'في_سينا', 'سينا_انتوا', 'انتوا_فاكرنها', 'فاكرنها_خطوبه', 'خطوبه_وبتقطعوا', 'وبتقطعوا_الصور', 'الصور_ده', 'ده_ايه', 'ايه_القرف', 'القرف_ده', 'ده_#', '#_مرار_طافح', 'انا_رفضت_وقف', 'رفضت_وقف_فيلم', 'وقف_فيلم_السبكي', 'فيلم_السبكي_ورافضه', 'السبكي_ورافضه_لالغاء', 'ورافضه_لالغاء_مشهد', 'لالغاء_مشهد_رفع', 'مشهد_رفع_مبارك', 'رفع_مبارك_العلم', 'مبارك_العلم_في', 'العلم_في_سينا', 'في_سينا_انتوا', 'سينا_انتوا_فاكرنها', 'انتوا_فاكرنها_خطوبه', 'فاكرنها_خطوبه_وبتقطعوا', 'خطوبه_وبتقطعوا_الصور', 'وبتقطعوا_الصور_ده', 'الصور_ده_ايه', 'ده_ايه_القرف', 'ايه_القرف_ده', 'القرف_ده_#', 'ده_#_مرار_طافح']]\n",
      "==================================================\n",
      "full gram tokenization : \n",
      " [['اول', 'ما', 'الاخواني', 'يتزنق', 'يقولك', 'هناك', 'ملائكه', 'تسجل', 'ماتقوله', 'طيب', 'مش', 'هي', 'الملايكه', 'بتسجل', 'كذب', 'قياداتكم', 'ولا', 'هي', 'بتسجل', 'لنا', 'احنا', 'بساتقوا', 'الله', 'يااخوه', 'اول_ما', 'ما_الاخواني', 'الاخواني_يتزنق', 'يتزنق_يقولك', 'يقولك_هناك', 'هناك_ملائكه', 'ملائكه_تسجل', 'تسجل_ماتقوله', 'ماتقوله_طيب', 'طيب_مش', 'مش_هي', 'هي_الملايكه', 'الملايكه_بتسجل', 'بتسجل_كذب', 'كذب_قياداتكم', 'قياداتكم_ولا', 'ولا_هي', 'هي_بتسجل', 'بتسجل_لنا', 'لنا_احنا', 'احنا_بساتقوا', 'بساتقوا_الله', 'الله_يااخوه', 'اول_ما_الاخواني', 'ما_الاخواني_يتزنق', 'الاخواني_يتزنق_يقولك', 'يتزنق_يقولك_هناك', 'يقولك_هناك_ملائكه', 'هناك_ملائكه_تسجل', 'ملائكه_تسجل_ماتقوله', 'تسجل_ماتقوله_طيب', 'ماتقوله_طيب_مش', 'طيب_مش_هي', 'مش_هي_الملايكه', 'هي_الملايكه_بتسجل', 'الملايكه_بتسجل_كذب', 'بتسجل_كذب_قياداتكم', 'كذب_قياداتكم_ولا', 'قياداتكم_ولا_هي', 'ولا_هي_بتسجل', 'هي_بتسجل_لنا', 'بتسجل_لنا_احنا', 'لنا_احنا_بساتقوا', 'احنا_بساتقوا_الله', 'بساتقوا_الله_يااخوه'], ['ارجو', 'من', 'الشبكات', 'اللي', 'بتدعي', 'انها', 'اخباريه', 'انها', 'تشوف', 'الكلام', 'اللي', 'بيقولوه', 'علي', 'لساني', 'اتقال', 'هنا', 'في', 'علي', 'تويتر', 'و', 'لا', 'لا', 'ارجو_من', 'من_الشبكات', 'الشبكات_اللي', 'اللي_بتدعي', 'بتدعي_انها', 'انها_اخباريه', 'اخباريه_انها', 'انها_تشوف', 'تشوف_الكلام', 'الكلام_اللي', 'اللي_بيقولوه', 'بيقولوه_علي', 'علي_لساني', 'لساني_اتقال', 'اتقال_هنا', 'هنا_في', 'في_علي', 'علي_تويتر', 'تويتر_و', 'و_لا', 'لا_لا', 'ارجو_من_الشبكات', 'من_الشبكات_اللي', 'الشبكات_اللي_بتدعي', 'اللي_بتدعي_انها', 'بتدعي_انها_اخباريه', 'انها_اخباريه_انها', 'اخباريه_انها_تشوف', 'انها_تشوف_الكلام', 'تشوف_الكلام_اللي', 'الكلام_اللي_بيقولوه', 'اللي_بيقولوه_علي', 'بيقولوه_علي_لساني', 'علي_لساني_اتقال', 'لساني_اتقال_هنا', 'اتقال_هنا_في', 'هنا_في_علي', 'في_علي_تويتر', 'علي_تويتر_و', 'تويتر_و_لا', 'و_لا_لا'], ['مقال', '#', 'عمرو_حمزاوي', 'يوميات', 'الارهاب', 'والقمع', 'مسموعا', 'من', 'موقع', '#', 'اقرا_لي', 'مقال_#', '#_عمرو_حمزاوي', 'عمرو_حمزاوي_يوميات', 'يوميات_الارهاب', 'الارهاب_والقمع', 'والقمع_مسموعا', 'مسموعا_من', 'من_موقع', 'موقع_#', '#_اقرا_لي', 'مقال_#_عمرو_حمزاوي', '#_عمرو_حمزاوي_يوميات', 'عمرو_حمزاوي_يوميات_الارهاب', 'يوميات_الارهاب_والقمع', 'الارهاب_والقمع_مسموعا', 'والقمع_مسموعا_من', 'مسموعا_من_موقع', 'من_موقع_#', 'موقع_#_اقرا_لي']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])\n",
    "\n",
    "fullgram_x_train_text_tokenized = get_all_ngrams(x_train_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "fullgram_x_val_text_tokenized = get_all_ngrams(x_val_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_val_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19feb913",
   "metadata": {},
   "source": [
    "# Our CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f3a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_word2vec_model = load_word2vec_model(\"../word2vec_models/rezk/cbow/continuous_bow_fullgram_vec_size_300-d_min_count_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d257579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(9608, 132, 300)\n",
      "(9608, 39600)\n",
      "==================================================\n",
      "[ 0.1214    0.3682   -0.1448   -0.03687   0.05566  -0.2715    0.7036\n",
      "  0.354    -0.2155    0.1714    0.3108   -0.08636   0.0853   -0.0822\n",
      "  0.733    -0.609    -0.3848    0.1864   -0.1465    0.1383    0.1486\n",
      "  0.2345    0.2715   -0.555    -0.0826    0.0421   -0.1056   -0.06616\n",
      " -0.3127   -0.2988    0.2344    0.0658    0.01614   0.009705 -0.01072\n",
      " -0.03845   0.266    -0.3828    0.0698   -0.4275   -0.1929    0.12\n",
      " -0.1859   -0.03053   0.1207    0.248     0.1338    0.292     0.36\n",
      " -0.133   ]\n",
      "==================================================\n",
      "(197, 132, 300)\n",
      "(197, 39600)\n",
      "==================================================\n",
      "[ 0.195    0.1139   0.11273  0.10834  0.1918  -0.272   -0.0904   0.3499\n",
      "  0.11255 -0.1373   0.1769  -0.01694 -0.331   -0.2861   0.3823  -0.2349\n",
      " -0.2189   0.07245  0.047   -0.03537  0.05942 -0.1351   0.01404 -0.0586\n",
      "  0.237   -0.03625 -0.1224  -0.101    0.2336   0.263   -0.05103 -0.11957\n",
      "  0.3762   0.1641  -0.00931 -0.1451   0.0516  -0.04178 -0.3274  -0.3633\n",
      "  0.0931   0.03833 -0.194   -0.05487 -0.0223   0.3647   0.1729   0.02745\n",
      "  0.2876   0.02945]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 132\n",
    "word2vec_path = \"rezk/cbow/\"\n",
    "model_path_to_save = \"../ml_models_saved/\"\n",
    "hid_num_neurons = 25\n",
    "learning_rate = .00005\n",
    "epochs = 10\n",
    "estimators = voting_models()\n",
    "\n",
    "performance_lr = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=5)\n",
    "RMSprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=.9)\n",
    "\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_x_val_text_tokenized, max_len_str)\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf6ace",
   "metadata": {},
   "source": [
    "# Our CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900da608",
   "metadata": {},
   "source": [
    "# With  Rmsprob and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3b8321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rezk_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n",
      "../test_models/ml_models_saved/dl_models/tensor_logs/run_2022_05_10_05_37_58_rezk_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 05:37:58.764831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:37:58.774460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:37:58.776263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:37:59.193692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:37:59.195461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:37:59.197194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:38:00.538963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:38:00.540739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:38:00.542461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9632 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:05:00.0, compute capability: 7.5\n",
      "2022-05-10 05:38:00.543081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 05:38:00.543887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9652 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 132, 300)         1200      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 132, 25)           32600     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 132, 25)          100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 132, 25)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3300)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 18)                59418     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,318\n",
      "Trainable params: 92,668\n",
      "Non-trainable params: 650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"rezk_cbow_word2vec\", model_type=\"Rmsprob_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e0d023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 05:38:11.034982: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 13s 28ms/step - loss: 1.5208 - accuracy: 0.5481 - val_loss: 1.0865 - val_accuracy: 0.6345 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "301/301 [==============================] - 8s 25ms/step - loss: 1.1568 - accuracy: 0.6125 - val_loss: 1.0419 - val_accuracy: 0.6599 - lr: 5.0000e-05\n",
      "Epoch 3/10\n",
      "301/301 [==============================] - 8s 25ms/step - loss: 1.0648 - accuracy: 0.6340 - val_loss: 1.0167 - val_accuracy: 0.6853 - lr: 5.0000e-05\n",
      "Epoch 4/10\n",
      "301/301 [==============================] - 7s 25ms/step - loss: 1.0126 - accuracy: 0.6392 - val_loss: 0.9872 - val_accuracy: 0.6802 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "301/301 [==============================] - 7s 24ms/step - loss: 0.9568 - accuracy: 0.6546 - val_loss: 0.9718 - val_accuracy: 0.6904 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "301/301 [==============================] - 8s 25ms/step - loss: 0.9280 - accuracy: 0.6699 - val_loss: 0.9454 - val_accuracy: 0.6954 - lr: 5.0000e-05\n",
      "Epoch 7/10\n",
      "301/301 [==============================] - 7s 24ms/step - loss: 0.8943 - accuracy: 0.6734 - val_loss: 0.9346 - val_accuracy: 0.6954 - lr: 5.0000e-05\n",
      "Epoch 8/10\n",
      "301/301 [==============================] - 7s 25ms/step - loss: 0.8623 - accuracy: 0.6839 - val_loss: 0.9291 - val_accuracy: 0.7056 - lr: 5.0000e-05\n",
      "Epoch 9/10\n",
      "301/301 [==============================] - 8s 25ms/step - loss: 0.8383 - accuracy: 0.6932 - val_loss: 0.9225 - val_accuracy: 0.6954 - lr: 5.0000e-05\n",
      "Epoch 10/10\n",
      "301/301 [==============================] - 8s 26ms/step - loss: 0.8129 - accuracy: 0.7038 - val_loss: 0.9141 - val_accuracy: 0.6954 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155ad8d3",
   "metadata": {},
   "source": [
    "# Bakr CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e81b4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "bakr_word2vec_model = load_word2vec_model(\"../word2vec_models//bakr/cbow/full_grams_cbow_300_twitter.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a4b9b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(9608, 132, 300)\n",
      "(9608, 39600)\n",
      "==================================================\n",
      "[ 0.0971  -1.876   -1.263    1.349   -0.8433   0.879    2.324   -1.0205\n",
      "  0.9077  -0.93     1.103    0.01787  0.6445  -1.017   -1.765   -0.539\n",
      " -0.456    0.6274   0.7246   3.027   -1.861    0.1412  -0.2025   1.914\n",
      "  1.376   -0.1142  -0.01277 -1.559    0.872    0.3884   1.963    0.3481\n",
      "  0.558   -2.217   -0.2201   1.38    -0.5503   0.714    1.045   -2.162\n",
      "  0.8477   0.1963  -0.5225   0.439    0.409    1.058    2.438   -1.232\n",
      "  0.08734 -1.635  ]\n",
      "==================================================\n",
      "(197, 132, 300)\n",
      "(197, 39600)\n",
      "==================================================\n",
      "[-0.2115  -0.2556   0.473    1.086    0.11786 -0.5483  -1.174    0.7393\n",
      "  2.07    -1.959    0.2927  -0.335   -0.8047  -0.6226   0.558    0.772\n",
      " -0.1536   0.925    1.451    3.363   -0.6655  -0.6377  -0.2278   0.64\n",
      " -0.491    1.162    0.565   -0.1466  -0.3965  -1.302    1.644    1.453\n",
      " -0.2742   0.3464   0.583    1.733   -0.0814   0.0537  -0.3413  -0.1697\n",
      " -1.409    1.622    1.419    3.764    1.141   -0.5483   3.453   -2.21\n",
      "  0.4814  -1.508  ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 132\n",
    "word2vec_path = \"bakr/cbow/\"\n",
    "model_path_to_save = \"../ml_models_saved/\"\n",
    "hid_num_neurons = 25\n",
    "learning_rate = .00005\n",
    "epochs = 10\n",
    "estimators = voting_models()\n",
    "\n",
    "performance_lr = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=5)\n",
    "RMSprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=.9)\n",
    "\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(bakr_word2vec_model, fullgram_x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(bakr_word2vec_model, fullgram_x_val_text_tokenized, max_len_str)\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df64c6f0",
   "metadata": {},
   "source": [
    "# With  Rmsprob and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aaa68b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bakr_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n",
      "../test_models/ml_models_saved/dl_models/tensor_logs/run_2022_05_10_05_40_55_bakr_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_2 (Batc  (None, 132, 300)         1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 132, 25)           32600     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 132, 25)          100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 132, 25)           0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3300)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 18)                59418     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,318\n",
      "Trainable params: 92,668\n",
      "Non-trainable params: 650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"bakr_cbow_word2vec\", model_type=\"Rmsprob_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dec390f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "301/301 [==============================] - 12s 30ms/step - loss: 1.3791 - accuracy: 0.5713 - val_loss: 1.0389 - val_accuracy: 0.6447 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 1.1142 - accuracy: 0.6229 - val_loss: 1.0975 - val_accuracy: 0.6548 - lr: 5.0000e-05\n",
      "Epoch 3/10\n",
      "301/301 [==============================] - 8s 26ms/step - loss: 1.0456 - accuracy: 0.6364 - val_loss: 1.0433 - val_accuracy: 0.6650 - lr: 5.0000e-05\n",
      "Epoch 4/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 0.9891 - accuracy: 0.6503 - val_loss: 1.0294 - val_accuracy: 0.6701 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "301/301 [==============================] - 8s 26ms/step - loss: 0.9611 - accuracy: 0.6568 - val_loss: 0.9925 - val_accuracy: 0.6599 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "301/301 [==============================] - 8s 25ms/step - loss: 0.9303 - accuracy: 0.6653 - val_loss: 0.9843 - val_accuracy: 0.6396 - lr: 5.0000e-05\n",
      "Epoch 7/10\n",
      "301/301 [==============================] - 8s 26ms/step - loss: 0.9107 - accuracy: 0.6688 - val_loss: 0.9581 - val_accuracy: 0.6650 - lr: 5.0000e-05\n",
      "Epoch 8/10\n",
      "301/301 [==============================] - 8s 26ms/step - loss: 0.8789 - accuracy: 0.6799 - val_loss: 0.9596 - val_accuracy: 0.6802 - lr: 5.0000e-05\n",
      "Epoch 9/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 0.8558 - accuracy: 0.6869 - val_loss: 0.9521 - val_accuracy: 0.6497 - lr: 5.0000e-05\n",
      "Epoch 10/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 0.8415 - accuracy: 0.6931 - val_loss: 0.9562 - val_accuracy: 0.6447 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3742b64",
   "metadata": {},
   "source": [
    "# Muhammed CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e8d4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "muhammed_word2vec_model = load_word2vec_model(\"../word2vec_models/muhammed/cbow/w2v_CBOW_300_3_400_10.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1aa3db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(9608, 132, 300)\n",
      "(9608, 39600)\n",
      "==================================================\n",
      "[ 0.818    0.3699  -0.4402  -1.145   -0.305   -1.197   -0.04993  2.256\n",
      "  2.035    0.6685   0.6113  -0.3662   0.7954  -0.3528   0.4214  -1.98\n",
      "  0.2295   0.9023  -0.9624   0.3145  -0.9424   0.602   -0.3047   0.2708\n",
      " -1.545    2.154   -1.758    0.4424  -1.366    1.809    0.347    2.451\n",
      "  1.658   -0.9473   2.766   -1.251    0.164   -0.1345   0.751   -0.5083\n",
      " -1.247    0.3098  -0.793    0.2264  -1.852   -1.138   -0.05344  0.739\n",
      "  0.11304  0.619  ]\n",
      "==================================================\n",
      "(197, 132, 300)\n",
      "(197, 39600)\n",
      "==================================================\n",
      "[-0.7144 -1.232   0.49   -0.2788  0.3442  0.5703 -1.1045  0.5747  0.646\n",
      "  1.317  -0.3972  0.357   3.158  -0.2544 -0.259  -0.135   1.456   4.97\n",
      " -1.786   2.35    0.232  -1.206   0.846   1.296  -0.7773 -0.0883 -0.5806\n",
      " -0.3147  1.126   1.328   0.33    1.735   1.116  -1.099   0.0929 -0.2737\n",
      "  2.146   1.369  -0.7153 -0.1835  0.332   0.8726  0.2756 -0.4575  2.412\n",
      " -4.758   0.7764  0.3591 -0.806   0.7935]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 132\n",
    "word2vec_path = \"muhammed/cbow/\"\n",
    "model_path_to_save = \"../ml_models_saved/\"\n",
    "hid_num_neurons = 25\n",
    "learning_rate = .00005\n",
    "epochs = 10\n",
    "estimators = voting_models()\n",
    "\n",
    "performance_lr = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=5)\n",
    "RMSprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=.9)\n",
    "\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(muhammed_word2vec_model, fullgram_x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(muhammed_word2vec_model, fullgram_x_val_text_tokenized, max_len_str)\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62305ed",
   "metadata": {},
   "source": [
    "# With  Rmsprob and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82d0c0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "muhammed_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n",
      "../test_models/ml_models_saved/dl_models/tensor_logs/run_2022_05_10_05_42_31_muhammed_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_4 (Batc  (None, 132, 300)         1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 132, 25)           32600     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 132, 25)          100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 132, 25)           0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 3300)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 18)                59418     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,318\n",
      "Trainable params: 92,668\n",
      "Non-trainable params: 650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"muhammed_cbow_word2vec\", model_type=\"Rmsprob_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eea5bc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "301/301 [==============================] - 12s 29ms/step - loss: 1.6548 - accuracy: 0.5129 - val_loss: 0.9654 - val_accuracy: 0.6701 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "301/301 [==============================] - 8s 25ms/step - loss: 1.1832 - accuracy: 0.6099 - val_loss: 1.0232 - val_accuracy: 0.6701 - lr: 5.0000e-05\n",
      "Epoch 3/10\n",
      "301/301 [==============================] - 8s 28ms/step - loss: 1.0803 - accuracy: 0.6307 - val_loss: 0.9990 - val_accuracy: 0.6599 - lr: 5.0000e-05\n",
      "Epoch 4/10\n",
      "301/301 [==============================] - 8s 28ms/step - loss: 1.0379 - accuracy: 0.6399 - val_loss: 0.9705 - val_accuracy: 0.6650 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "301/301 [==============================] - 8s 28ms/step - loss: 0.9896 - accuracy: 0.6514 - val_loss: 0.9430 - val_accuracy: 0.6751 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 0.9539 - accuracy: 0.6609 - val_loss: 0.9292 - val_accuracy: 0.6650 - lr: 5.0000e-05\n",
      "Epoch 7/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 0.9206 - accuracy: 0.6640 - val_loss: 0.9148 - val_accuracy: 0.6853 - lr: 5.0000e-05\n",
      "Epoch 8/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 0.9009 - accuracy: 0.6724 - val_loss: 0.9024 - val_accuracy: 0.6599 - lr: 5.0000e-05\n",
      "Epoch 9/10\n",
      "301/301 [==============================] - 8s 28ms/step - loss: 0.8734 - accuracy: 0.6793 - val_loss: 0.9020 - val_accuracy: 0.6548 - lr: 5.0000e-05\n",
      "Epoch 10/10\n",
      "301/301 [==============================] - 8s 26ms/step - loss: 0.8565 - accuracy: 0.6902 - val_loss: 0.9003 - val_accuracy: 0.6599 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8eb7e",
   "metadata": {},
   "source": [
    "# Load best model & predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "742bc678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نرجوا عدم متابعه وحظر حسابات : المباحث تابعني ...</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>المسلماني اغلي متحدث اعلامي للرئيس واشهر كذاب ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الاحد 3 يوليو : ابو الفتوح في ندوه بالمؤتمر ال...</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#كبسوله_صحيه #صحه #طفل #طفلي #نوم ##نوبه #نصيحه</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#اللي_رافضين_السيسي_رئيس_بيعملوا_فولوا_لبعض #م...</td>\n",
       "      <td>1</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label classes\n",
       "0  نرجوا عدم متابعه وحظر حسابات : المباحث تابعني ...      2     OBJ\n",
       "1  المسلماني اغلي متحدث اعلامي للرئيس واشهر كذاب ...      0     NEG\n",
       "2  الاحد 3 يوليو : ابو الفتوح في ندوه بالمؤتمر ال...      2     OBJ\n",
       "3    #كبسوله_صحيه #صحه #طفل #طفلي #نوم ##نوبه #نصيحه      2     OBJ\n",
       "4  #اللي_رافضين_السيسي_رئيس_بيعملوا_فولوا_لبعض #م...      1     NEU"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_test_set = read_file(\"test/strat_test_set.csv\")\n",
    "strat_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ef93122",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text = list(strat_test_set['text'])\n",
    "y_test = strat_test_set['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41d8c6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['نرجوا عدم متابعه وحظر حسابات : المباحث تابعني واتابعك زياده المتابعين الاخبار المحليه #الشعب_يقول_كلمته #جماعه_انصار_بيت_طنيطر #الرياض #جده', 'المسلماني اغلي متحدث اعلامي للرئيس واشهر كذاب ومنافق ومضلل ومحرض تحول الي قليل الادب وضع معارضي الجيش في سله القمامه كلام زباله', 'الاحد 3 يوليو : ابو الفتوح في ندوه بالمؤتمر السنوي لهندسه عين شمس 5 مساء']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['نرجوا', 'عدم', 'متابعه', 'وحظر', 'حسابات', ':', 'المباحث', 'تابعني', 'واتابعك', 'زياده', 'المتابعين', 'الاخبار', 'المحليه', '#', 'الشعب_يقول_كلمته', '#', 'جماعه_انصار_بيت_طنيطر', '#', 'الرياض', '#', 'جده'], ['المسلماني', 'اغلي', 'متحدث', 'اعلامي', 'للرئيس', 'واشهر', 'كذاب', 'ومنافق', 'ومضلل', 'ومحرض', 'تحول', 'الي', 'قليل', 'الادب', 'وضع', 'معارضي', 'الجيش', 'في', 'سله', 'القمامه', 'كلام', 'زباله'], ['الاحد', '3', 'يوليو', ':', 'ابو', 'الفتوح', 'في', 'ندوه', 'بالمؤتمر', 'السنوي', 'لهندسه', 'عين', 'شمس', '5', 'مساء']]\n",
      "==================================================\n",
      "full gram tokenization : \n",
      " [['نرجوا', 'عدم', 'متابعه', 'وحظر', 'حسابات', ':', 'المباحث', 'تابعني', 'واتابعك', 'زياده', 'المتابعين', 'الاخبار', 'المحليه', '#', 'الشعب_يقول_كلمته', '#', 'جماعه_انصار_بيت_طنيطر', '#', 'الرياض', '#', 'جده', 'نرجوا_عدم', 'عدم_متابعه', 'متابعه_وحظر', 'وحظر_حسابات', 'حسابات_:', ':_المباحث', 'المباحث_تابعني', 'تابعني_واتابعك', 'واتابعك_زياده', 'زياده_المتابعين', 'المتابعين_الاخبار', 'الاخبار_المحليه', 'المحليه_#', '#_الشعب_يقول_كلمته', 'الشعب_يقول_كلمته_#', '#_جماعه_انصار_بيت_طنيطر', 'جماعه_انصار_بيت_طنيطر_#', '#_الرياض', 'الرياض_#', '#_جده', 'نرجوا_عدم_متابعه', 'عدم_متابعه_وحظر', 'متابعه_وحظر_حسابات', 'وحظر_حسابات_:', 'حسابات_:_المباحث', ':_المباحث_تابعني', 'المباحث_تابعني_واتابعك', 'تابعني_واتابعك_زياده', 'واتابعك_زياده_المتابعين', 'زياده_المتابعين_الاخبار', 'المتابعين_الاخبار_المحليه', 'الاخبار_المحليه_#', 'المحليه_#_الشعب_يقول_كلمته', '#_الشعب_يقول_كلمته_#', 'الشعب_يقول_كلمته_#_جماعه_انصار_بيت_طنيطر', '#_جماعه_انصار_بيت_طنيطر_#', 'جماعه_انصار_بيت_طنيطر_#_الرياض', '#_الرياض_#', 'الرياض_#_جده'], ['المسلماني', 'اغلي', 'متحدث', 'اعلامي', 'للرئيس', 'واشهر', 'كذاب', 'ومنافق', 'ومضلل', 'ومحرض', 'تحول', 'الي', 'قليل', 'الادب', 'وضع', 'معارضي', 'الجيش', 'في', 'سله', 'القمامه', 'كلام', 'زباله', 'المسلماني_اغلي', 'اغلي_متحدث', 'متحدث_اعلامي', 'اعلامي_للرئيس', 'للرئيس_واشهر', 'واشهر_كذاب', 'كذاب_ومنافق', 'ومنافق_ومضلل', 'ومضلل_ومحرض', 'ومحرض_تحول', 'تحول_الي', 'الي_قليل', 'قليل_الادب', 'الادب_وضع', 'وضع_معارضي', 'معارضي_الجيش', 'الجيش_في', 'في_سله', 'سله_القمامه', 'القمامه_كلام', 'كلام_زباله', 'المسلماني_اغلي_متحدث', 'اغلي_متحدث_اعلامي', 'متحدث_اعلامي_للرئيس', 'اعلامي_للرئيس_واشهر', 'للرئيس_واشهر_كذاب', 'واشهر_كذاب_ومنافق', 'كذاب_ومنافق_ومضلل', 'ومنافق_ومضلل_ومحرض', 'ومضلل_ومحرض_تحول', 'ومحرض_تحول_الي', 'تحول_الي_قليل', 'الي_قليل_الادب', 'قليل_الادب_وضع', 'الادب_وضع_معارضي', 'وضع_معارضي_الجيش', 'معارضي_الجيش_في', 'الجيش_في_سله', 'في_سله_القمامه', 'سله_القمامه_كلام', 'القمامه_كلام_زباله'], ['الاحد', '3', 'يوليو', ':', 'ابو', 'الفتوح', 'في', 'ندوه', 'بالمؤتمر', 'السنوي', 'لهندسه', 'عين', 'شمس', '5', 'مساء', 'الاحد_3', '3_يوليو', 'يوليو_:', ':_ابو', 'ابو_الفتوح', 'الفتوح_في', 'في_ندوه', 'ندوه_بالمؤتمر', 'بالمؤتمر_السنوي', 'السنوي_لهندسه', 'لهندسه_عين', 'عين_شمس', 'شمس_5', '5_مساء', 'الاحد_3_يوليو', '3_يوليو_:', 'يوليو_:_ابو', ':_ابو_الفتوح', 'ابو_الفتوح_في', 'الفتوح_في_ندوه', 'في_ندوه_بالمؤتمر', 'ندوه_بالمؤتمر_السنوي', 'بالمؤتمر_السنوي_لهندسه', 'السنوي_لهندسه_عين', 'لهندسه_عين_شمس', 'عين_شمس_5', 'شمس_5_مساء']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "X_test_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(X_test_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", X_test_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "fullgram_X_test_text_tokenized = get_all_ngrams(X_test_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6302b876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(201, 132, 300)\n",
      "(201, 39600)\n",
      "==================================================\n",
      "[ 0.2988  -0.2693   0.10986 -0.641   -0.2272  -0.833    0.5947   0.2289\n",
      "  1.029    1.69     0.1128  -0.513    0.5083  -0.5146   0.455    1.061\n",
      "  0.623    0.8276  -0.07904  1.13     0.2983   0.584   -0.336    0.817\n",
      " -0.747    0.2122  -0.772   -1.026   -0.213    0.78     0.2832   0.05463\n",
      "  0.2041   0.4233  -1.4795   0.4827  -0.4382   0.2524  -0.6396   0.5166\n",
      " -0.0176  -0.1283  -0.4321  -0.4316   0.2345   0.10675  0.08984  0.03406\n",
      " -0.349    0.0258 ]\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.6567164179104478\n",
      "==================================================\n",
      "==================================================\n",
      "(201, 132, 300)\n",
      "(201, 39600)\n",
      "==================================================\n",
      "[ 0.1403   2.791   -0.1461  -1.685   -0.5776   0.8594   0.1614  -0.643\n",
      "  1.949    1.234    0.5005  -1.628    0.01268  0.6436  -1.111   -0.3813\n",
      "  1.564   -0.1381   0.2703   1.786   -0.4143   0.3918  -0.94     0.798\n",
      "  1.602    1.107   -1.866    1.3955   2.795   -0.5894   2.129    2.266\n",
      "  0.918   -0.12256  0.03775 -0.8667  -1.592   -1.315   -0.4163   1.871\n",
      " -0.1918  -1.6455  -1.846   -0.6865   1.72     1.369   -0.808    0.01648\n",
      "  1.045    0.5747 ]\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.6318407960199005\n",
      "==================================================\n",
      "==================================================\n",
      "(201, 132, 300)\n",
      "(201, 39600)\n",
      "==================================================\n",
      "[-0.2301   -0.4343    0.8735   -0.1881    0.05475   0.4888   -1.04\n",
      " -1.126     0.721    -0.3496   -0.02415  -0.849     0.4631    0.2191\n",
      " -1.184    -1.629     1.764    -1.162    -0.097     1.413    -0.7773\n",
      " -1.366     1.651    -2.98     -1.312    -0.02803  -0.523     0.8813\n",
      " -0.0701   -0.449     0.0607   -0.652     0.6035   -0.6265    1.326\n",
      "  1.025    -0.9766   -0.03558   0.659     0.926    -0.3704   -1.571\n",
      " -0.4841    0.3823   -0.009605 -0.3118   -0.974    -0.827    -1.117\n",
      " -1.829   ]\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.6119402985074627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.612"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rezk_model = keras_load_model(\"../ml_models_saved/dl_models/run_with_rezk_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05__model.h5\"  )\n",
    "bakr_model = keras_load_model(\"../ml_models_saved/dl_models/run_with_bakr_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05__model.h5\"  )\n",
    "muhammed_model = keras_load_model(\"../ml_models_saved/dl_models/run_with_muhammed_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05__model.h5\"  )\n",
    "\n",
    "X_test_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_X_test_text_tokenized, max_len_str)\n",
    "X_test_embed_matrix = X_test_embed_matrix.reshape(X_test_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "\n",
    "keras_f1_score_result(rezk_model, X_test_embed_matrix, y_test)\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_test_embed_matrix = text_to_matrix_using_word2vec(bakr_word2vec_model, fullgram_X_test_text_tokenized, max_len_str)\n",
    "X_test_embed_matrix = X_test_embed_matrix.reshape(X_test_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "keras_f1_score_result(bakr_model, X_test_embed_matrix, y_test)\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_test_embed_matrix = text_to_matrix_using_word2vec(muhammed_word2vec_model, fullgram_X_test_text_tokenized, max_len_str)\n",
    "X_test_embed_matrix = X_test_embed_matrix.reshape(X_test_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "keras_f1_score_result(muhammed_model, X_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81c91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
