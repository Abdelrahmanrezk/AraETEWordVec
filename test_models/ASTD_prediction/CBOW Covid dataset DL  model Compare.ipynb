{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26287380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../preprocess_assets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd2800f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\n",
    "import tensorflow as tf\n",
    "from features_extraction import *\n",
    "from data_shuffling_split import *\n",
    "from ara_vec_preprocess_configs import *\n",
    "from ml_modeling import *\n",
    "from keras_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81b3161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>مع انتشار الامراض ومنها الكورونا اليكم النصيحه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1 دراسه بحثيه صادره معهد الدراسات الاستراتيجي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>بدات في مدينه وهان في الصين انتقلت لبعض المقاط...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>coronarvirus CoronaOutbreak فيروس_كورونا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>الاداره_العامه_للطيران_المدني اجراءات وقاءيه ح...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  مع انتشار الامراض ومنها الكورونا اليكم النصيحه...\n",
       "1      0   1 دراسه بحثيه صادره معهد الدراسات الاستراتيجي...\n",
       "2      1  بدات في مدينه وهان في الصين انتقلت لبعض المقاط...\n",
       "3      1           coronarvirus CoronaOutbreak فيروس_كورونا\n",
       "4      1  الاداره_العامه_للطيران_المدني اجراءات وقاءيه ح..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set = read_file(\"train/strat_train_set.csv\")\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc8b1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of instances in the training data after StratifiedShuffleSplit are:  8643\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   177\n",
      "The number of trainin instances:  8643\n",
      "The number of validation instances:  177\n",
      "The number of trainin labels :  8643\n",
      "The number of validation labels :  177\n"
     ]
    }
   ],
   "source": [
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beb33e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['الصين تشيد مستشفي لعلاج مرضي ', 'ومع ان تصنيف الفيروس ورقعه انتشاره تعدت المرحله السادسه فتره الا ان منظمه الصحه العالميه لم تعلن حتي تفشي وباء عالمي قد يكون خوفا انتشار حاله هلع غير مبرر حول العالم وما قد ينتج عواقب اقتصاديه واجتماعيه', 'اتفاقيه الصين 300 الف برميل نفط يوميا لصين ولمده عشر اعوام مقابل اعمار العراق خلال سنتين سيكون العراق اكبر مركز تجاري عالمي يربط الشرق اوروبا عبر شبكه خطوط سكك حديد مما يءدي افلاس موانء دبي و خساءر فادحه لمضيق السويس وكسر الهيمنه الامريكيه اتفاقيه_الصين_مشاريع_واعمار']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['الصين', 'تشيد', 'مستشفي', 'لعلاج', 'مرضي'], ['ومع', 'ان', 'تصنيف', 'الفيروس', 'ورقعه', 'انتشاره', 'تعدت', 'المرحله', 'السادسه', 'فتره', 'الا', 'ان', 'منظمه', 'الصحه', 'العالميه', 'لم', 'تعلن', 'حتي', 'تفشي', 'وباء', 'عالمي', 'قد', 'يكون', 'خوفا', 'انتشار', 'حاله', 'هلع', 'غير', 'مبرر', 'حول', 'العالم', 'وما', 'قد', 'ينتج', 'عواقب', 'اقتصاديه', 'واجتماعيه'], ['اتفاقيه', 'الصين', '300', 'الف', 'برميل', 'نفط', 'يوميا', 'لصين', 'ولمده', 'عشر', 'اعوام', 'مقابل', 'اعمار', 'العراق', 'خلال', 'سنتين', 'سيكون', 'العراق', 'اكبر', 'مركز', 'تجاري', 'عالمي', 'يربط', 'الشرق', 'اوروبا', 'عبر', 'شبكه', 'خطوط', 'سكك', 'حديد', 'مما', 'يءدي', 'افلاس', 'موانء', 'دبي', 'و', 'خساءر', 'فادحه', 'لمضيق', 'السويس', 'وكسر', 'الهيمنه', 'الامريكيه', 'اتفاقيه_الصين_مشاريع_واعمار']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['كورونا سيلعب مباراته رقم 200 مع بورتو ضد فسينيتي بورتو', 'لا تدرون قد يكون عقاب الله لما يفعلونه ضد المسلمين فلا شي يحصل في الدنيا بدون سبب', 'حمي_الضنك وباء يفتك بالحياه يلزم الاحتشاد لمواجهته معا_لمواجهه_حمي_الضنك']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['كورونا', 'سيلعب', 'مباراته', 'رقم', '200', 'مع', 'بورتو', 'ضد', 'فسينيتي', 'بورتو'], ['لا', 'تدرون', 'قد', 'يكون', 'عقاب', 'الله', 'لما', 'يفعلونه', 'ضد', 'المسلمين', 'فلا', 'شي', 'يحصل', 'في', 'الدنيا', 'بدون', 'سبب'], ['حمي_الضنك', 'وباء', 'يفتك', 'بالحياه', 'يلزم', 'الاحتشاد', 'لمواجهته', 'معا_لمواجهه_حمي_الضنك']]\n",
      "full gram tokenization : \n",
      " [['الصين', 'تشيد', 'مستشفي', 'لعلاج', 'مرضي', 'الصين_تشيد', 'تشيد_مستشفي', 'مستشفي_لعلاج', 'لعلاج_مرضي', 'الصين_تشيد_مستشفي', 'تشيد_مستشفي_لعلاج', 'مستشفي_لعلاج_مرضي'], ['ومع', 'ان', 'تصنيف', 'الفيروس', 'ورقعه', 'انتشاره', 'تعدت', 'المرحله', 'السادسه', 'فتره', 'الا', 'ان', 'منظمه', 'الصحه', 'العالميه', 'لم', 'تعلن', 'حتي', 'تفشي', 'وباء', 'عالمي', 'قد', 'يكون', 'خوفا', 'انتشار', 'حاله', 'هلع', 'غير', 'مبرر', 'حول', 'العالم', 'وما', 'قد', 'ينتج', 'عواقب', 'اقتصاديه', 'واجتماعيه', 'ومع_ان', 'ان_تصنيف', 'تصنيف_الفيروس', 'الفيروس_ورقعه', 'ورقعه_انتشاره', 'انتشاره_تعدت', 'تعدت_المرحله', 'المرحله_السادسه', 'السادسه_فتره', 'فتره_الا', 'الا_ان', 'ان_منظمه', 'منظمه_الصحه', 'الصحه_العالميه', 'العالميه_لم', 'لم_تعلن', 'تعلن_حتي', 'حتي_تفشي', 'تفشي_وباء', 'وباء_عالمي', 'عالمي_قد', 'قد_يكون', 'يكون_خوفا', 'خوفا_انتشار', 'انتشار_حاله', 'حاله_هلع', 'هلع_غير', 'غير_مبرر', 'مبرر_حول', 'حول_العالم', 'العالم_وما', 'وما_قد', 'قد_ينتج', 'ينتج_عواقب', 'عواقب_اقتصاديه', 'اقتصاديه_واجتماعيه', 'ومع_ان_تصنيف', 'ان_تصنيف_الفيروس', 'تصنيف_الفيروس_ورقعه', 'الفيروس_ورقعه_انتشاره', 'ورقعه_انتشاره_تعدت', 'انتشاره_تعدت_المرحله', 'تعدت_المرحله_السادسه', 'المرحله_السادسه_فتره', 'السادسه_فتره_الا', 'فتره_الا_ان', 'الا_ان_منظمه', 'ان_منظمه_الصحه', 'منظمه_الصحه_العالميه', 'الصحه_العالميه_لم', 'العالميه_لم_تعلن', 'لم_تعلن_حتي', 'تعلن_حتي_تفشي', 'حتي_تفشي_وباء', 'تفشي_وباء_عالمي', 'وباء_عالمي_قد', 'عالمي_قد_يكون', 'قد_يكون_خوفا', 'يكون_خوفا_انتشار', 'خوفا_انتشار_حاله', 'انتشار_حاله_هلع', 'حاله_هلع_غير', 'هلع_غير_مبرر', 'غير_مبرر_حول', 'مبرر_حول_العالم', 'حول_العالم_وما', 'العالم_وما_قد', 'وما_قد_ينتج', 'قد_ينتج_عواقب', 'ينتج_عواقب_اقتصاديه', 'عواقب_اقتصاديه_واجتماعيه'], ['اتفاقيه', 'الصين', '300', 'الف', 'برميل', 'نفط', 'يوميا', 'لصين', 'ولمده', 'عشر', 'اعوام', 'مقابل', 'اعمار', 'العراق', 'خلال', 'سنتين', 'سيكون', 'العراق', 'اكبر', 'مركز', 'تجاري', 'عالمي', 'يربط', 'الشرق', 'اوروبا', 'عبر', 'شبكه', 'خطوط', 'سكك', 'حديد', 'مما', 'يءدي', 'افلاس', 'موانء', 'دبي', 'و', 'خساءر', 'فادحه', 'لمضيق', 'السويس', 'وكسر', 'الهيمنه', 'الامريكيه', 'اتفاقيه_الصين_مشاريع_واعمار', 'اتفاقيه_الصين', 'الصين_300', '300_الف', 'الف_برميل', 'برميل_نفط', 'نفط_يوميا', 'يوميا_لصين', 'لصين_ولمده', 'ولمده_عشر', 'عشر_اعوام', 'اعوام_مقابل', 'مقابل_اعمار', 'اعمار_العراق', 'العراق_خلال', 'خلال_سنتين', 'سنتين_سيكون', 'سيكون_العراق', 'العراق_اكبر', 'اكبر_مركز', 'مركز_تجاري', 'تجاري_عالمي', 'عالمي_يربط', 'يربط_الشرق', 'الشرق_اوروبا', 'اوروبا_عبر', 'عبر_شبكه', 'شبكه_خطوط', 'خطوط_سكك', 'سكك_حديد', 'حديد_مما', 'مما_يءدي', 'يءدي_افلاس', 'افلاس_موانء', 'موانء_دبي', 'دبي_و', 'و_خساءر', 'خساءر_فادحه', 'فادحه_لمضيق', 'لمضيق_السويس', 'السويس_وكسر', 'وكسر_الهيمنه', 'الهيمنه_الامريكيه', 'الامريكيه_اتفاقيه_الصين_مشاريع_واعمار', 'اتفاقيه_الصين_300', 'الصين_300_الف', '300_الف_برميل', 'الف_برميل_نفط', 'برميل_نفط_يوميا', 'نفط_يوميا_لصين', 'يوميا_لصين_ولمده', 'لصين_ولمده_عشر', 'ولمده_عشر_اعوام', 'عشر_اعوام_مقابل', 'اعوام_مقابل_اعمار', 'مقابل_اعمار_العراق', 'اعمار_العراق_خلال', 'العراق_خلال_سنتين', 'خلال_سنتين_سيكون', 'سنتين_سيكون_العراق', 'سيكون_العراق_اكبر', 'العراق_اكبر_مركز', 'اكبر_مركز_تجاري', 'مركز_تجاري_عالمي', 'تجاري_عالمي_يربط', 'عالمي_يربط_الشرق', 'يربط_الشرق_اوروبا', 'الشرق_اوروبا_عبر', 'اوروبا_عبر_شبكه', 'عبر_شبكه_خطوط', 'شبكه_خطوط_سكك', 'خطوط_سكك_حديد', 'سكك_حديد_مما', 'حديد_مما_يءدي', 'مما_يءدي_افلاس', 'يءدي_افلاس_موانء', 'افلاس_موانء_دبي', 'موانء_دبي_و', 'دبي_و_خساءر', 'و_خساءر_فادحه', 'خساءر_فادحه_لمضيق', 'فادحه_لمضيق_السويس', 'لمضيق_السويس_وكسر', 'السويس_وكسر_الهيمنه', 'وكسر_الهيمنه_الامريكيه', 'الهيمنه_الامريكيه_اتفاقيه_الصين_مشاريع_واعمار']]\n",
      "==================================================\n",
      "full gram tokenization : \n",
      " [['كورونا', 'سيلعب', 'مباراته', 'رقم', '200', 'مع', 'بورتو', 'ضد', 'فسينيتي', 'بورتو', 'كورونا_سيلعب', 'سيلعب_مباراته', 'مباراته_رقم', 'رقم_200', '200_مع', 'مع_بورتو', 'بورتو_ضد', 'ضد_فسينيتي', 'فسينيتي_بورتو', 'كورونا_سيلعب_مباراته', 'سيلعب_مباراته_رقم', 'مباراته_رقم_200', 'رقم_200_مع', '200_مع_بورتو', 'مع_بورتو_ضد', 'بورتو_ضد_فسينيتي', 'ضد_فسينيتي_بورتو'], ['لا', 'تدرون', 'قد', 'يكون', 'عقاب', 'الله', 'لما', 'يفعلونه', 'ضد', 'المسلمين', 'فلا', 'شي', 'يحصل', 'في', 'الدنيا', 'بدون', 'سبب', 'لا_تدرون', 'تدرون_قد', 'قد_يكون', 'يكون_عقاب', 'عقاب_الله', 'الله_لما', 'لما_يفعلونه', 'يفعلونه_ضد', 'ضد_المسلمين', 'المسلمين_فلا', 'فلا_شي', 'شي_يحصل', 'يحصل_في', 'في_الدنيا', 'الدنيا_بدون', 'بدون_سبب', 'لا_تدرون_قد', 'تدرون_قد_يكون', 'قد_يكون_عقاب', 'يكون_عقاب_الله', 'عقاب_الله_لما', 'الله_لما_يفعلونه', 'لما_يفعلونه_ضد', 'يفعلونه_ضد_المسلمين', 'ضد_المسلمين_فلا', 'المسلمين_فلا_شي', 'فلا_شي_يحصل', 'شي_يحصل_في', 'يحصل_في_الدنيا', 'في_الدنيا_بدون', 'الدنيا_بدون_سبب'], ['حمي_الضنك', 'وباء', 'يفتك', 'بالحياه', 'يلزم', 'الاحتشاد', 'لمواجهته', 'معا_لمواجهه_حمي_الضنك', 'حمي_الضنك_وباء', 'وباء_يفتك', 'يفتك_بالحياه', 'بالحياه_يلزم', 'يلزم_الاحتشاد', 'الاحتشاد_لمواجهته', 'لمواجهته_معا_لمواجهه_حمي_الضنك', 'حمي_الضنك_وباء_يفتك', 'وباء_يفتك_بالحياه', 'يفتك_بالحياه_يلزم', 'بالحياه_يلزم_الاحتشاد', 'يلزم_الاحتشاد_لمواجهته', 'الاحتشاد_لمواجهته_معا_لمواجهه_حمي_الضنك']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])\n",
    "\n",
    "fullgram_x_train_text_tokenized = get_all_ngrams(x_train_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "fullgram_x_val_text_tokenized = get_all_ngrams(x_val_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_val_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a6dadf",
   "metadata": {},
   "source": [
    "# Our CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ce13592",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_word2vec_model = load_word2vec_model(\"../word2vec_models/rezk/cbow/continuous_bow_fullgram_vec_size_300-d_min_count_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f3c783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(8643, 237, 300)\n",
      "(8643, 71100)\n",
      "==================================================\n",
      "[-0.4805   0.1469   0.515   -0.0917   0.1716  -0.5327  -0.00957 -0.2554\n",
      "  0.1427  -0.1697   0.1628  -0.1864  -0.423    0.1849  -0.01895  0.2593\n",
      "  0.0554  -0.202   -0.9077  -0.1611  -0.2866  -0.1271  -0.119    0.0774\n",
      "  0.00858 -0.03262  0.01156 -0.05792  0.09357 -0.303   -0.1738  -0.2012\n",
      " -0.252   -0.10443  0.1705   0.2007  -0.3098   0.0258  -0.2323  -0.282\n",
      " -0.514   -0.3896   0.1611  -0.1481   0.0856  -0.01016  1.23     0.2974\n",
      "  0.3635  -0.679  ]\n",
      "==================================================\n",
      "(177, 237, 300)\n",
      "(177, 71100)\n",
      "==================================================\n",
      "[ 0.3225  -0.2566   0.346   -0.258    0.2615   0.2467  -0.4841   0.10724\n",
      "  0.02872 -0.7285  -0.1974   0.3418  -0.516    0.1583   0.7427  -0.02202\n",
      " -0.1685   0.1998  -0.1421  -0.1327   0.3577  -0.0834  -0.4927  -0.3665\n",
      " -0.562    0.05362 -0.06793 -0.1676  -0.278   -0.333    0.384    0.8403\n",
      "  0.0937   0.419    0.2471  -0.519   -0.1809   0.2449  -0.432    0.1971\n",
      "  0.148   -0.2384   0.4375  -0.03372  0.2788   0.04242  0.683    0.7944\n",
      " -0.0239   0.3176 ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 237\n",
    "word2vec_path = \"rezk/cbow/\"\n",
    "model_path_to_save = \"../ml_models_saved/\"\n",
    "hid_num_neurons = 25\n",
    "learning_rate = .00005\n",
    "epochs = 10\n",
    "estimators = voting_models()\n",
    "\n",
    "performance_lr = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=5)\n",
    "RMSprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=.9)\n",
    "\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_x_val_text_tokenized, max_len_str)\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4277f5",
   "metadata": {},
   "source": [
    "# Our CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d1450a",
   "metadata": {},
   "source": [
    "# With  Rmsprob and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db6690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "./../ml_models_saved/dl_models/tensor_logs/run_2022_05_09_23_38_49_rezk_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 23:38:49.299756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:38:49.309677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:38:49.311504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:38:49.753498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:38:49.755288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:38:49.757031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:38:51.178517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:38:51.180315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:38:51.182057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9632 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:05:00.0, compute capability: 7.5\n",
      "2022-05-09 23:38:51.182790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:38:51.183608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9652 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 237, 300)         1200      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 237, 25)           32600     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 237, 25)          100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 237, 25)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5925)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 18)                106668    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140,568\n",
      "Trainable params: 139,918\n",
      "Non-trainable params: 650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"rezk_cbow_word2vec\", model_type=\"Rmsprob_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e19b0dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 23:39:06.356560: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/271 [==============================] - 16s 40ms/step - loss: 1.0416 - accuracy: 0.5688 - val_loss: 2.3733 - val_accuracy: 0.3446 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.5393 - accuracy: 0.7493 - val_loss: 0.4666 - val_accuracy: 0.7571 - lr: 5.0000e-05\n",
      "Epoch 3/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.4138 - accuracy: 0.8134 - val_loss: 0.3850 - val_accuracy: 0.8305 - lr: 5.0000e-05\n",
      "Epoch 4/10\n",
      "271/271 [==============================] - 10s 36ms/step - loss: 0.3545 - accuracy: 0.8460 - val_loss: 0.3605 - val_accuracy: 0.8588 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.3051 - accuracy: 0.8688 - val_loss: 0.3357 - val_accuracy: 0.8701 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "271/271 [==============================] - 10s 36ms/step - loss: 0.2706 - accuracy: 0.8878 - val_loss: 0.3282 - val_accuracy: 0.8644 - lr: 5.0000e-05\n",
      "Epoch 7/10\n",
      "271/271 [==============================] - 11s 39ms/step - loss: 0.2458 - accuracy: 0.8997 - val_loss: 0.3180 - val_accuracy: 0.8814 - lr: 5.0000e-05\n",
      "Epoch 8/10\n",
      "271/271 [==============================] - 10s 36ms/step - loss: 0.2315 - accuracy: 0.9054 - val_loss: 0.3149 - val_accuracy: 0.8927 - lr: 5.0000e-05\n",
      "Epoch 9/10\n",
      "271/271 [==============================] - 10s 36ms/step - loss: 0.2141 - accuracy: 0.9151 - val_loss: 0.3053 - val_accuracy: 0.8927 - lr: 5.0000e-05\n",
      "Epoch 10/10\n",
      "271/271 [==============================] - 10s 36ms/step - loss: 0.1940 - accuracy: 0.9233 - val_loss: 0.3031 - val_accuracy: 0.8927 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6397ac7",
   "metadata": {},
   "source": [
    "# Bakr CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30710feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bakr_word2vec_model = load_word2vec_model(\"../word2vec_models//bakr/cbow/full_grams_cbow_300_twitter.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6025fc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(8643, 237, 300)\n",
      "(8643, 71100)\n",
      "==================================================\n",
      "[-1.135   -2.104    1.518    1.966   -1.1875  -0.8887   0.02058 -0.6045\n",
      "  0.812    0.503   -0.561    1.524   -0.6035   0.00866 -0.825   -0.9634\n",
      " -0.643    0.3687   0.14     2.32    -0.665   -0.2158   0.3616   1.305\n",
      "  0.144    0.522    0.5557  -2.898    0.1288   1.279   -0.3296   1.474\n",
      "  0.725   -0.2395  -1.222   -0.2517   0.418   -1.958   -0.1783  -0.779\n",
      "  0.08136 -1.038   -0.05753 -0.5967   0.1014   1.523   -0.4175  -0.562\n",
      " -0.9556  -1.049  ]\n",
      "==================================================\n",
      "(177, 237, 300)\n",
      "(177, 71100)\n",
      "==================================================\n",
      "[-1.510e+00 -5.952e-01 -9.412e-02  7.877e-04  2.033e+00  1.078e+00\n",
      " -7.002e-01  6.519e-02  1.880e+00  1.279e-01  1.307e+00 -2.866e-01\n",
      "  4.124e-01  8.623e-01  9.976e-01  1.078e+00  4.167e-01 -2.338e-01\n",
      "  5.049e-01  4.478e-01 -3.826e-01  2.027e+00 -1.108e+00 -4.668e-01\n",
      "  2.443e+00  1.760e+00  6.851e-01 -1.512e+00  1.218e+00  2.974e-01\n",
      " -2.649e-01 -8.027e-01  5.322e-01 -6.064e-01  3.132e-01 -1.330e+00\n",
      "  5.669e-01 -1.406e+00 -4.509e-01  7.573e-01  4.875e-01 -2.795e+00\n",
      " -1.024e+00 -6.104e-01 -1.008e+00 -7.852e-01  3.582e-01  4.016e-01\n",
      "  8.945e-01 -1.272e+00]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 237\n",
    "word2vec_path = \"bakr/cbow/\"\n",
    "model_path_to_save = \"../ml_models_saved/\"\n",
    "hid_num_neurons = 25\n",
    "learning_rate = .00005\n",
    "epochs = 10\n",
    "estimators = voting_models()\n",
    "\n",
    "performance_lr = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=5)\n",
    "RMSprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=.9)\n",
    "\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(bakr_word2vec_model, fullgram_x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(bakr_word2vec_model, fullgram_x_val_text_tokenized, max_len_str)\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5111c2b",
   "metadata": {},
   "source": [
    "# With  Rmsprob and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5167cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "./../ml_models_saved/dl_models/tensor_logs/run_2022_05_09_23_42_14_bakr_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_2 (Batc  (None, 237, 300)         1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 237, 25)           32600     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 237, 25)          100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 237, 25)           0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 5925)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 18)                106668    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140,568\n",
      "Trainable params: 139,918\n",
      "Non-trainable params: 650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"bakr_cbow_word2vec\", model_type=\"Rmsprob_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf1d0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "271/271 [==============================] - 14s 39ms/step - loss: 0.9720 - accuracy: 0.6112 - val_loss: 0.6660 - val_accuracy: 0.7401 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "271/271 [==============================] - 10s 38ms/step - loss: 0.5625 - accuracy: 0.7455 - val_loss: 0.4405 - val_accuracy: 0.7910 - lr: 5.0000e-05\n",
      "Epoch 3/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.4529 - accuracy: 0.8033 - val_loss: 0.3958 - val_accuracy: 0.8249 - lr: 5.0000e-05\n",
      "Epoch 4/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.3816 - accuracy: 0.8392 - val_loss: 0.3729 - val_accuracy: 0.8475 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "271/271 [==============================] - 10s 36ms/step - loss: 0.3384 - accuracy: 0.8624 - val_loss: 0.3355 - val_accuracy: 0.8418 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.3059 - accuracy: 0.8746 - val_loss: 0.3238 - val_accuracy: 0.8531 - lr: 5.0000e-05\n",
      "Epoch 7/10\n",
      "271/271 [==============================] - 10s 36ms/step - loss: 0.2876 - accuracy: 0.8836 - val_loss: 0.3084 - val_accuracy: 0.8588 - lr: 5.0000e-05\n",
      "Epoch 8/10\n",
      "271/271 [==============================] - 10s 35ms/step - loss: 0.2615 - accuracy: 0.8985 - val_loss: 0.2954 - val_accuracy: 0.8588 - lr: 5.0000e-05\n",
      "Epoch 9/10\n",
      "271/271 [==============================] - 10s 36ms/step - loss: 0.2459 - accuracy: 0.9040 - val_loss: 0.2908 - val_accuracy: 0.8757 - lr: 5.0000e-05\n",
      "Epoch 10/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.2264 - accuracy: 0.9111 - val_loss: 0.2929 - val_accuracy: 0.8757 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95f188f",
   "metadata": {},
   "source": [
    "# Muhammed CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f828fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "muhammed_word2vec_model = load_word2vec_model(\"../word2vec_models/muhammed/cbow/w2v_CBOW_300_3_400_10.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da95997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(8643, 237, 300)\n",
      "(8643, 71100)\n",
      "==================================================\n",
      "[ 0.2268  -0.9756  -0.4565   0.9346   0.4282  -0.9824  -0.4766  -1.237\n",
      "  1.305   -0.777   -0.2522   0.6216   0.4358   1.75    -0.996   -0.9746\n",
      " -0.118    1.038    0.2406   1.468   -0.6333  -0.355    1.129   -0.2583\n",
      " -0.1019  -0.02823 -0.2379   2.113    0.874    0.4878   0.07336  0.6147\n",
      "  1.942    0.2979   1.115   -0.6704  -0.7065  -0.912    0.3728   0.4736\n",
      "  0.748   -0.632    1.598    0.4553   0.656   -1.322    1.005   -0.376\n",
      "  0.1926  -0.5146 ]\n",
      "==================================================\n",
      "(177, 237, 300)\n",
      "(177, 71100)\n",
      "==================================================\n",
      "[ 0.832    0.2073   0.9546   2.123   -1.687    0.0622  -0.8013  -0.0675\n",
      " -0.3008   0.902    0.7485   0.264    0.947    1.629   -1.772   -0.0646\n",
      " -0.1963   1.31     0.325    0.6016   0.576   -0.6763  -0.612    0.01938\n",
      " -1.041   -1.348   -0.921    0.2272  -0.4575   0.636    1.286    0.3552\n",
      "  2.49     0.2988   0.257    0.8784  -0.1746   0.4263  -0.5815   0.9062\n",
      " -0.2245   0.82    -1.285    0.3457   1.321    0.767   -1.027   -0.1918\n",
      " -1.423    0.8867 ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 237\n",
    "word2vec_path = \"muhammed/cbow/\"\n",
    "model_path_to_save = \"../ml_models_saved/\"\n",
    "hid_num_neurons = 25\n",
    "learning_rate = .00005\n",
    "epochs = 10\n",
    "estimators = voting_models()\n",
    "\n",
    "performance_lr = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=5)\n",
    "RMSprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=.9)\n",
    "\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(muhammed_word2vec_model, fullgram_x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(muhammed_word2vec_model, fullgram_x_val_text_tokenized, max_len_str)\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afdd257",
   "metadata": {},
   "source": [
    "# With  Rmsprob and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a843c42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "./../ml_models_saved/dl_models/tensor_logs/run_2022_05_09_23_44_14_muhammed_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_4 (Batc  (None, 237, 300)         1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 237, 25)           32600     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 237, 25)          100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 237, 25)           0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 5925)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 18)                106668    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140,568\n",
      "Trainable params: 139,918\n",
      "Non-trainable params: 650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"muhammed_cbow_word2vec\", model_type=\"Rmsprob_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eac22b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "271/271 [==============================] - 14s 40ms/step - loss: 1.0483 - accuracy: 0.5794 - val_loss: 0.9075 - val_accuracy: 0.7006 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "271/271 [==============================] - 9s 35ms/step - loss: 0.5939 - accuracy: 0.7184 - val_loss: 0.5432 - val_accuracy: 0.7345 - lr: 5.0000e-05\n",
      "Epoch 3/10\n",
      "271/271 [==============================] - 10s 35ms/step - loss: 0.4922 - accuracy: 0.7809 - val_loss: 0.5051 - val_accuracy: 0.7740 - lr: 5.0000e-05\n",
      "Epoch 4/10\n",
      "271/271 [==============================] - 9s 35ms/step - loss: 0.4254 - accuracy: 0.8087 - val_loss: 0.4714 - val_accuracy: 0.7910 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "271/271 [==============================] - 10s 36ms/step - loss: 0.3825 - accuracy: 0.8337 - val_loss: 0.4536 - val_accuracy: 0.8079 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "271/271 [==============================] - 10s 35ms/step - loss: 0.3515 - accuracy: 0.8504 - val_loss: 0.4388 - val_accuracy: 0.8136 - lr: 5.0000e-05\n",
      "Epoch 7/10\n",
      "271/271 [==============================] - 10s 36ms/step - loss: 0.3244 - accuracy: 0.8647 - val_loss: 0.4220 - val_accuracy: 0.8362 - lr: 5.0000e-05\n",
      "Epoch 8/10\n",
      "271/271 [==============================] - 10s 38ms/step - loss: 0.2999 - accuracy: 0.8750 - val_loss: 0.4218 - val_accuracy: 0.8192 - lr: 5.0000e-05\n",
      "Epoch 9/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.2847 - accuracy: 0.8812 - val_loss: 0.4141 - val_accuracy: 0.8362 - lr: 5.0000e-05\n",
      "Epoch 10/10\n",
      "271/271 [==============================] - 10s 38ms/step - loss: 0.2713 - accuracy: 0.8897 - val_loss: 0.4177 - val_accuracy: 0.8305 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b6e74",
   "metadata": {},
   "source": [
    "# Load best model & predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e720478d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>حساء خفافيش الصين ابداع القرف يعني</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>اجراءات هكذا استعدت مصر لمواجهه فيروس كورونا م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ايش الفايده لابس كمامات ومو لابس قفازات</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>الجدير بالذكر ان في الصين بيستخدموا ال في كل ح...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ولو ترسل صواريخ الصين وقفنا دون برج الفيصليه</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1                 حساء خفافيش الصين ابداع القرف يعني\n",
       "1      1  اجراءات هكذا استعدت مصر لمواجهه فيروس كورونا م...\n",
       "2      1            ايش الفايده لابس كمامات ومو لابس قفازات\n",
       "3      0  الجدير بالذكر ان في الصين بيستخدموا ال في كل ح...\n",
       "4      0       ولو ترسل صواريخ الصين وقفنا دون برج الفيصليه"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_test_set = read_file(\"test/strat_test_set.csv\")\n",
    "strat_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22f53506",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text = list(strat_test_set['text'])\n",
    "y_test = strat_test_set['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8d48e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['حساء خفافيش الصين ابداع القرف يعني', 'اجراءات هكذا استعدت مصر لمواجهه فيروس كورونا مرايتي', 'ايش الفايده لابس كمامات ومو لابس قفازات']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['حساء', 'خفافيش', 'الصين', 'ابداع', 'القرف', 'يعني'], ['اجراءات', 'هكذا', 'استعدت', 'مصر', 'لمواجهه', 'فيروس', 'كورونا', 'مرايتي'], ['ايش', 'الفايده', 'لابس', 'كمامات', 'ومو', 'لابس', 'قفازات']]\n",
      "==================================================\n",
      "full gram tokenization : \n",
      " [['حساء', 'خفافيش', 'الصين', 'ابداع', 'القرف', 'يعني', 'حساء_خفافيش', 'خفافيش_الصين', 'الصين_ابداع', 'ابداع_القرف', 'القرف_يعني', 'حساء_خفافيش_الصين', 'خفافيش_الصين_ابداع', 'الصين_ابداع_القرف', 'ابداع_القرف_يعني'], ['اجراءات', 'هكذا', 'استعدت', 'مصر', 'لمواجهه', 'فيروس', 'كورونا', 'مرايتي', 'اجراءات_هكذا', 'هكذا_استعدت', 'استعدت_مصر', 'مصر_لمواجهه', 'لمواجهه_فيروس', 'فيروس_كورونا', 'كورونا_مرايتي', 'اجراءات_هكذا_استعدت', 'هكذا_استعدت_مصر', 'استعدت_مصر_لمواجهه', 'مصر_لمواجهه_فيروس', 'لمواجهه_فيروس_كورونا', 'فيروس_كورونا_مرايتي'], ['ايش', 'الفايده', 'لابس', 'كمامات', 'ومو', 'لابس', 'قفازات', 'ايش_الفايده', 'الفايده_لابس', 'لابس_كمامات', 'كمامات_ومو', 'ومو_لابس', 'لابس_قفازات', 'ايش_الفايده_لابس', 'الفايده_لابس_كمامات', 'لابس_كمامات_ومو', 'كمامات_ومو_لابس', 'ومو_لابس_قفازات']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "X_test_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(X_test_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", X_test_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "fullgram_X_test_text_tokenized = get_all_ngrams(X_test_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0041393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(180, 237, 300)\n",
      "(180, 71100)\n",
      "==================================================\n",
      "[ 0.2527   0.522    0.1779   0.4744  -0.2773   0.06058 -0.129   -0.2115\n",
      "  0.2888   0.6753   0.1991   0.4854   0.887    0.1575   0.04596  0.02391\n",
      "  0.393    0.4778  -0.2332  -0.2058  -0.5474  -0.7773  -0.11176 -0.1393\n",
      " -0.3208  -0.1919  -0.277   -1.303    0.1914  -0.503    0.1576  -0.4834\n",
      " -0.1271  -0.799    1.056    0.5728  -0.7295  -0.4622  -0.3062  -0.05975\n",
      " -0.651    0.2406   0.12335  0.547    0.3616   0.00967  0.6387   0.6045\n",
      "  0.07007 -0.457  ]\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.9277777777777778\n",
      "==================================================\n",
      "==================================================\n",
      "(180, 237, 300)\n",
      "(180, 71100)\n",
      "==================================================\n",
      "[ 0.1781   0.0907  -0.855    0.3633  -0.117   -0.253   -0.3142   0.0964\n",
      "  0.9585  -0.1278  -0.10754  0.962    1.264   -1.41    -0.01978 -0.5273\n",
      " -0.8696  -0.364    1.239   -1.581   -0.75    -0.6987  -0.3423   0.4714\n",
      "  0.2732  -0.2334   0.575   -1.28    -0.2057   0.1677  -1.732    0.969\n",
      "  0.188   -0.746   -0.961   -0.794    0.2544  -0.10864  0.3572  -0.454\n",
      " -0.9976  -0.3215  -0.371   -0.6963  -1.173    1.402    0.5557   0.2207\n",
      " -1.509   -1.046  ]\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.8722222222222223\n",
      "==================================================\n",
      "==================================================\n",
      "(180, 237, 300)\n",
      "(180, 71100)\n",
      "==================================================\n",
      "[ 0.2268  -0.9756  -0.4565   0.9346   0.4282  -0.9824  -0.4766  -1.237\n",
      "  1.305   -0.777   -0.2522   0.6216   0.4358   1.75    -0.996   -0.9746\n",
      " -0.118    1.038    0.2406   1.468   -0.6333  -0.355    1.129   -0.2583\n",
      " -0.1019  -0.02823 -0.2379   2.113    0.874    0.4878   0.07336  0.6147\n",
      "  1.942    0.2979   1.115   -0.6704  -0.7065  -0.912    0.3728   0.4736\n",
      "  0.748   -0.632    1.598    0.4553   0.656   -1.322    1.005   -0.376\n",
      "  0.1926  -0.5146 ]\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rezk_model = keras_load_model(\"../ml_models_saved/dl_models/run_with_rezk_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05__model.h5\"  )\n",
    "bakr_model = keras_load_model(\"../ml_models_saved/dl_models/run_with_bakr_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05__model.h5\"  )\n",
    "muhammed_model = keras_load_model(\"../ml_models_saved/dl_models/run_with_muhammed_cbow_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05__model.h5\"  )\n",
    "\n",
    "X_test_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_X_test_text_tokenized, max_len_str)\n",
    "X_test_embed_matrix = X_test_embed_matrix.reshape(X_test_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "\n",
    "keras_f1_score_result(rezk_model, X_test_embed_matrix, y_test)\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_test_embed_matrix = text_to_matrix_using_word2vec(bakr_word2vec_model, fullgram_X_test_text_tokenized, max_len_str)\n",
    "X_test_embed_matrix = X_test_embed_matrix.reshape(X_test_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "keras_f1_score_result(bakr_model, X_test_embed_matrix, y_test)\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_test_embed_matrix = text_to_matrix_using_word2vec(muhammed_word2vec_model, fullgram_X_test_text_tokenized, max_len_str)\n",
    "X_test_embed_matrix = X_test_embed_matrix.reshape(X_test_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "keras_f1_score_result(muhammed_model, X_test_embed_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da785e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
