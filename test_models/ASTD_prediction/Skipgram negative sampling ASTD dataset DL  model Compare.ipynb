{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acd13de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../preprocess_assets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4370417d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\n",
    "import tensorflow as tf\n",
    "from features_extraction import *\n",
    "from data_shuffling_split import *\n",
    "from ara_vec_preprocess_configs import *\n",
    "from ml_modeling import *\n",
    "from keras_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca76090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>راجعت الردود فوجدت المتزمتين دينيا هم الاكثر ا...</td>\n",
       "      <td>0</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#شاهد_سكاي : #عيد_العمال تراجع اعدد العاطلين</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>يقوم د / ابو الفتوح التواصل مع مختلف التيارات ...</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>هل الشرطه والجيش سيطبق عليهم حدود الاجور دي ؟</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 -انسحاب الوفد المصري المشارك في مهرجان مالم...</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label classes\n",
       "0  راجعت الردود فوجدت المتزمتين دينيا هم الاكثر ا...      0     NEG\n",
       "1       #شاهد_سكاي : #عيد_العمال تراجع اعدد العاطلين      2     OBJ\n",
       "2  يقوم د / ابو الفتوح التواصل مع مختلف التيارات ...      2     OBJ\n",
       "3     هل الشرطه والجيش سيطبق عليهم حدود الاجور دي ؟       2     OBJ\n",
       "4   1 -انسحاب الوفد المصري المشارك في مهرجان مالم...      2     OBJ"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set = read_file(\"train/strat_train_set.csv\")\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d88b2a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of instances in the training data after StratifiedShuffleSplit are:  9608\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   197\n",
      "The number of trainin instances:  9608\n",
      "The number of validation instances:  197\n",
      "The number of trainin labels :  9608\n",
      "The number of validation labels :  197\n"
     ]
    }
   ],
   "source": [
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d039cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['#فتاه_تغتصب_شباب_كانوا_في_طريقهم_لمنزلهم يمكن بويه ‹☺›', 'تابعوني علي قناه العربيه التاسعه مساء باذن الله', 'هل سنري اليوم الذي نعود فيه الي حياه طبيعيه دون مظاهرات او اعتصامات او اضراباتو لا اتهامات و محاكمات و انتقام ؟ نرجع الي عملنا و حياتنا']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['#', 'فتاه_تغتصب_شباب_كانوا_في_طريقهم_لمنزلهم', 'يمكن', 'بويه', '‹☺›'], ['تابعوني', 'علي', 'قناه', 'العربيه', 'التاسعه', 'مساء', 'باذن', 'الله'], ['هل', 'سنري', 'اليوم', 'الذي', 'نعود', 'فيه', 'الي', 'حياه', 'طبيعيه', 'دون', 'مظاهرات', 'او', 'اعتصامات', 'او', 'اضراباتو', 'لا', 'اتهامات', 'و', 'محاكمات', 'و', 'انتقام', '؟', 'نرجع', 'الي', 'عملنا', 'و', 'حياتنا']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['قصه شاب مسيحي يشارك معنا في صناع الحياه تحيه كبيره الي سامي والي 100 مسيحي يشاركون معنا في معسكر ابو قير،', '#سري ويرد الصباح علي كلام سعود الفيصل ( انا لن اقبل تهديدك لي وللكويتين بهذه اللهجه ) ', 'ليه يوصلوا الواحد في سنه انه يكره عيشته واليوم اللي اتولد فيه ؟ #ثانويه_عامه #علمونا_صح']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['قصه', 'شاب', 'مسيحي', 'يشارك', 'معنا', 'في', 'صناع', 'الحياه', 'تحيه', 'كبيره', 'الي', 'سامي', 'والي', '100', 'مسيحي', 'يشاركون', 'معنا', 'في', 'معسكر', 'ابو', 'قير،'], ['#', 'سري', 'ويرد', 'الصباح', 'علي', 'كلام', 'سعود', 'الفيصل', '(', 'انا', 'لن', 'اقبل', 'تهديدك', 'لي', 'وللكويتين', 'بهذه', 'اللهجه', ')'], ['ليه', 'يوصلوا', 'الواحد', 'في', 'سنه', 'انه', 'يكره', 'عيشته', 'واليوم', 'اللي', 'اتولد', 'فيه', '؟', '#', 'ثانويه_عامه', '#', 'علمونا_صح']]\n",
      "full gram tokenization : \n",
      " [['#', 'فتاه_تغتصب_شباب_كانوا_في_طريقهم_لمنزلهم', 'يمكن', 'بويه', '‹☺›', '#_فتاه_تغتصب_شباب_كانوا_في_طريقهم_لمنزلهم', 'فتاه_تغتصب_شباب_كانوا_في_طريقهم_لمنزلهم_يمكن', 'يمكن_بويه', 'بويه_‹☺›', '#_فتاه_تغتصب_شباب_كانوا_في_طريقهم_لمنزلهم_يمكن', 'فتاه_تغتصب_شباب_كانوا_في_طريقهم_لمنزلهم_يمكن_بويه', 'يمكن_بويه_‹☺›'], ['تابعوني', 'علي', 'قناه', 'العربيه', 'التاسعه', 'مساء', 'باذن', 'الله', 'تابعوني_علي', 'علي_قناه', 'قناه_العربيه', 'العربيه_التاسعه', 'التاسعه_مساء', 'مساء_باذن', 'باذن_الله', 'تابعوني_علي_قناه', 'علي_قناه_العربيه', 'قناه_العربيه_التاسعه', 'العربيه_التاسعه_مساء', 'التاسعه_مساء_باذن', 'مساء_باذن_الله'], ['هل', 'سنري', 'اليوم', 'الذي', 'نعود', 'فيه', 'الي', 'حياه', 'طبيعيه', 'دون', 'مظاهرات', 'او', 'اعتصامات', 'او', 'اضراباتو', 'لا', 'اتهامات', 'و', 'محاكمات', 'و', 'انتقام', '؟', 'نرجع', 'الي', 'عملنا', 'و', 'حياتنا', 'هل_سنري', 'سنري_اليوم', 'اليوم_الذي', 'الذي_نعود', 'نعود_فيه', 'فيه_الي', 'الي_حياه', 'حياه_طبيعيه', 'طبيعيه_دون', 'دون_مظاهرات', 'مظاهرات_او', 'او_اعتصامات', 'اعتصامات_او', 'او_اضراباتو', 'اضراباتو_لا', 'لا_اتهامات', 'اتهامات_و', 'و_محاكمات', 'محاكمات_و', 'و_انتقام', 'انتقام_؟', '؟_نرجع', 'نرجع_الي', 'الي_عملنا', 'عملنا_و', 'و_حياتنا', 'هل_سنري_اليوم', 'سنري_اليوم_الذي', 'اليوم_الذي_نعود', 'الذي_نعود_فيه', 'نعود_فيه_الي', 'فيه_الي_حياه', 'الي_حياه_طبيعيه', 'حياه_طبيعيه_دون', 'طبيعيه_دون_مظاهرات', 'دون_مظاهرات_او', 'مظاهرات_او_اعتصامات', 'او_اعتصامات_او', 'اعتصامات_او_اضراباتو', 'او_اضراباتو_لا', 'اضراباتو_لا_اتهامات', 'لا_اتهامات_و', 'اتهامات_و_محاكمات', 'و_محاكمات_و', 'محاكمات_و_انتقام', 'و_انتقام_؟', 'انتقام_؟_نرجع', '؟_نرجع_الي', 'نرجع_الي_عملنا', 'الي_عملنا_و', 'عملنا_و_حياتنا']]\n",
      "==================================================\n",
      "full gram tokenization : \n",
      " [['قصه', 'شاب', 'مسيحي', 'يشارك', 'معنا', 'في', 'صناع', 'الحياه', 'تحيه', 'كبيره', 'الي', 'سامي', 'والي', '100', 'مسيحي', 'يشاركون', 'معنا', 'في', 'معسكر', 'ابو', 'قير،', 'قصه_شاب', 'شاب_مسيحي', 'مسيحي_يشارك', 'يشارك_معنا', 'معنا_في', 'في_صناع', 'صناع_الحياه', 'الحياه_تحيه', 'تحيه_كبيره', 'كبيره_الي', 'الي_سامي', 'سامي_والي', 'والي_100', '100_مسيحي', 'مسيحي_يشاركون', 'يشاركون_معنا', 'معنا_في', 'في_معسكر', 'معسكر_ابو', 'ابو_قير،', 'قصه_شاب_مسيحي', 'شاب_مسيحي_يشارك', 'مسيحي_يشارك_معنا', 'يشارك_معنا_في', 'معنا_في_صناع', 'في_صناع_الحياه', 'صناع_الحياه_تحيه', 'الحياه_تحيه_كبيره', 'تحيه_كبيره_الي', 'كبيره_الي_سامي', 'الي_سامي_والي', 'سامي_والي_100', 'والي_100_مسيحي', '100_مسيحي_يشاركون', 'مسيحي_يشاركون_معنا', 'يشاركون_معنا_في', 'معنا_في_معسكر', 'في_معسكر_ابو', 'معسكر_ابو_قير،'], ['#', 'سري', 'ويرد', 'الصباح', 'علي', 'كلام', 'سعود', 'الفيصل', '(', 'انا', 'لن', 'اقبل', 'تهديدك', 'لي', 'وللكويتين', 'بهذه', 'اللهجه', ')', '#_سري', 'سري_ويرد', 'ويرد_الصباح', 'الصباح_علي', 'علي_كلام', 'كلام_سعود', 'سعود_الفيصل', 'الفيصل_(', '(_انا', 'انا_لن', 'لن_اقبل', 'اقبل_تهديدك', 'تهديدك_لي', 'لي_وللكويتين', 'وللكويتين_بهذه', 'بهذه_اللهجه', 'اللهجه_)', '#_سري_ويرد', 'سري_ويرد_الصباح', 'ويرد_الصباح_علي', 'الصباح_علي_كلام', 'علي_كلام_سعود', 'كلام_سعود_الفيصل', 'سعود_الفيصل_(', 'الفيصل_(_انا', '(_انا_لن', 'انا_لن_اقبل', 'لن_اقبل_تهديدك', 'اقبل_تهديدك_لي', 'تهديدك_لي_وللكويتين', 'لي_وللكويتين_بهذه', 'وللكويتين_بهذه_اللهجه', 'بهذه_اللهجه_)'], ['ليه', 'يوصلوا', 'الواحد', 'في', 'سنه', 'انه', 'يكره', 'عيشته', 'واليوم', 'اللي', 'اتولد', 'فيه', '؟', '#', 'ثانويه_عامه', '#', 'علمونا_صح', 'ليه_يوصلوا', 'يوصلوا_الواحد', 'الواحد_في', 'في_سنه', 'سنه_انه', 'انه_يكره', 'يكره_عيشته', 'عيشته_واليوم', 'واليوم_اللي', 'اللي_اتولد', 'اتولد_فيه', 'فيه_؟', '؟_#', '#_ثانويه_عامه', 'ثانويه_عامه_#', '#_علمونا_صح', 'ليه_يوصلوا_الواحد', 'يوصلوا_الواحد_في', 'الواحد_في_سنه', 'في_سنه_انه', 'سنه_انه_يكره', 'انه_يكره_عيشته', 'يكره_عيشته_واليوم', 'عيشته_واليوم_اللي', 'واليوم_اللي_اتولد', 'اللي_اتولد_فيه', 'اتولد_فيه_؟', 'فيه_؟_#', '؟_#_ثانويه_عامه', '#_ثانويه_عامه_#', 'ثانويه_عامه_#_علمونا_صح']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])\n",
    "\n",
    "fullgram_x_train_text_tokenized = get_all_ngrams(x_train_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "fullgram_x_val_text_tokenized = get_all_ngrams(x_val_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_val_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e147e1a",
   "metadata": {},
   "source": [
    "# Our CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cfe4bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_word2vec_model = load_word2vec_model(\"../word2vec_models/rezk/skipgram_NS/sk_gr_negative_sampeling_fullgram_vec_size_300-d_min_count_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc132be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(9608, 132, 300)\n",
      "(9608, 39600)\n",
      "==================================================\n",
      "[ 0.005478 -0.04172   0.005714 -0.03497  -0.1553    0.1969    0.05286\n",
      "  0.2722    0.3384   -0.2032   -0.1765   -0.05478  -0.1627    0.1953\n",
      " -0.001688 -0.08417   0.1536   -0.3528   -0.1882   -0.1313    0.2078\n",
      " -0.0564   -0.2031    0.02293   0.1746   -0.1052    0.007717  0.05777\n",
      " -0.06696  -0.1255    0.01427   0.0869    0.1155    0.1318   -0.0492\n",
      " -0.01573  -0.1229    0.07776  -0.0928   -0.1912    0.05603  -0.01492\n",
      "  0.01863  -0.2324    0.002361  0.2047    0.11084   0.1925   -0.0379\n",
      " -0.2247  ]\n",
      "==================================================\n",
      "(197, 132, 300)\n",
      "(197, 39600)\n",
      "==================================================\n",
      "[ 0.2832   -0.2688   -0.010796  0.2869    0.2957   -0.1874   -0.25\n",
      "  0.04913   0.2607   -0.32     -0.07385  -0.0223    0.2112   -0.2418\n",
      " -0.0984    0.186    -0.01915   0.0668    0.1037   -0.3662   -0.05264\n",
      " -0.2766   -0.1348    0.03833  -0.3352    0.2297   -0.05524  -0.1621\n",
      " -0.1427    0.1648    0.2405   -0.1508   -0.0077    0.1161   -0.156\n",
      "  0.1943    0.1304    0.1466    0.285    -0.315    -0.1134    0.06235\n",
      "  0.264    -0.1782    0.169     0.3687    0.08325   0.3142    0.02283\n",
      "  0.06195 ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 132\n",
    "word2vec_path = \"rezk/cbow/\"\n",
    "model_path_to_save = \"../ml_models_saved/\"\n",
    "hid_num_neurons = 25\n",
    "learning_rate = .00005\n",
    "epochs = 10\n",
    "estimators = voting_models()\n",
    "\n",
    "performance_lr = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=5)\n",
    "RMSprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=.9)\n",
    "\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_x_val_text_tokenized, max_len_str)\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581249cd",
   "metadata": {},
   "source": [
    "# Our CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679c424",
   "metadata": {},
   "source": [
    "# With  Rmsprob and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "480be119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rezk_skipgram_NS_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n",
      "../test_models/ml_models_saved/dl_models/tensor_logs/run_2022_05_10_06_04_54_rezk_skipgram_NS_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 06:04:54.934719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 06:04:54.945514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 06:04:54.947337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 06:04:55.407826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 06:04:55.409639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 06:04:55.411362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 06:04:56.838776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 06:04:56.840558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 06:04:56.842297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9632 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:05:00.0, compute capability: 7.5\n",
      "2022-05-10 06:04:56.843062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 06:04:56.843872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9652 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 132, 300)         1200      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 132, 25)           32600     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 132, 25)          100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 132, 25)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3300)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 18)                59418     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,318\n",
      "Trainable params: 92,668\n",
      "Non-trainable params: 650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"rezk_skipgram_NS_word2vec\", model_type=\"Rmsprob_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b018cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 06:05:07.818662: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 15s 31ms/step - loss: 1.3518 - accuracy: 0.5803 - val_loss: 1.4356 - val_accuracy: 0.4873 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 1.1128 - accuracy: 0.6233 - val_loss: 1.0729 - val_accuracy: 0.6548 - lr: 5.0000e-05\n",
      "Epoch 3/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 1.0418 - accuracy: 0.6419 - val_loss: 1.0488 - val_accuracy: 0.6599 - lr: 5.0000e-05\n",
      "Epoch 4/10\n",
      "301/301 [==============================] - 8s 25ms/step - loss: 0.9918 - accuracy: 0.6538 - val_loss: 1.0266 - val_accuracy: 0.6396 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "301/301 [==============================] - 8s 26ms/step - loss: 0.9560 - accuracy: 0.6565 - val_loss: 1.0170 - val_accuracy: 0.6599 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "301/301 [==============================] - 7s 25ms/step - loss: 0.9225 - accuracy: 0.6678 - val_loss: 1.0098 - val_accuracy: 0.6599 - lr: 5.0000e-05\n",
      "Epoch 7/10\n",
      "301/301 [==============================] - 8s 25ms/step - loss: 0.9009 - accuracy: 0.6655 - val_loss: 1.0065 - val_accuracy: 0.6497 - lr: 5.0000e-05\n",
      "Epoch 8/10\n",
      "301/301 [==============================] - 8s 26ms/step - loss: 0.8814 - accuracy: 0.6770 - val_loss: 0.9965 - val_accuracy: 0.6599 - lr: 5.0000e-05\n",
      "Epoch 9/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 0.8561 - accuracy: 0.6840 - val_loss: 0.9942 - val_accuracy: 0.6650 - lr: 5.0000e-05\n",
      "Epoch 10/10\n",
      "301/301 [==============================] - 8s 27ms/step - loss: 0.8344 - accuracy: 0.6914 - val_loss: 0.9988 - val_accuracy: 0.6650 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493664c5",
   "metadata": {},
   "source": [
    "# Load best model & predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecb84597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نرجوا عدم متابعه وحظر حسابات : المباحث تابعني ...</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>المسلماني اغلي متحدث اعلامي للرئيس واشهر كذاب ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الاحد 3 يوليو : ابو الفتوح في ندوه بالمؤتمر ال...</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#كبسوله_صحيه #صحه #طفل #طفلي #نوم ##نوبه #نصيحه</td>\n",
       "      <td>2</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#اللي_رافضين_السيسي_رئيس_بيعملوا_فولوا_لبعض #م...</td>\n",
       "      <td>1</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label classes\n",
       "0  نرجوا عدم متابعه وحظر حسابات : المباحث تابعني ...      2     OBJ\n",
       "1  المسلماني اغلي متحدث اعلامي للرئيس واشهر كذاب ...      0     NEG\n",
       "2  الاحد 3 يوليو : ابو الفتوح في ندوه بالمؤتمر ال...      2     OBJ\n",
       "3    #كبسوله_صحيه #صحه #طفل #طفلي #نوم ##نوبه #نصيحه      2     OBJ\n",
       "4  #اللي_رافضين_السيسي_رئيس_بيعملوا_فولوا_لبعض #م...      1     NEU"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_test_set = read_file(\"test/strat_test_set.csv\")\n",
    "strat_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66aa6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text = list(strat_test_set['text'])\n",
    "y_test = strat_test_set['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a39726a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['نرجوا عدم متابعه وحظر حسابات : المباحث تابعني واتابعك زياده المتابعين الاخبار المحليه #الشعب_يقول_كلمته #جماعه_انصار_بيت_طنيطر #الرياض #جده', 'المسلماني اغلي متحدث اعلامي للرئيس واشهر كذاب ومنافق ومضلل ومحرض تحول الي قليل الادب وضع معارضي الجيش في سله القمامه كلام زباله', 'الاحد 3 يوليو : ابو الفتوح في ندوه بالمؤتمر السنوي لهندسه عين شمس 5 مساء']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['نرجوا', 'عدم', 'متابعه', 'وحظر', 'حسابات', ':', 'المباحث', 'تابعني', 'واتابعك', 'زياده', 'المتابعين', 'الاخبار', 'المحليه', '#', 'الشعب_يقول_كلمته', '#', 'جماعه_انصار_بيت_طنيطر', '#', 'الرياض', '#', 'جده'], ['المسلماني', 'اغلي', 'متحدث', 'اعلامي', 'للرئيس', 'واشهر', 'كذاب', 'ومنافق', 'ومضلل', 'ومحرض', 'تحول', 'الي', 'قليل', 'الادب', 'وضع', 'معارضي', 'الجيش', 'في', 'سله', 'القمامه', 'كلام', 'زباله'], ['الاحد', '3', 'يوليو', ':', 'ابو', 'الفتوح', 'في', 'ندوه', 'بالمؤتمر', 'السنوي', 'لهندسه', 'عين', 'شمس', '5', 'مساء']]\n",
      "==================================================\n",
      "full gram tokenization : \n",
      " [['نرجوا', 'عدم', 'متابعه', 'وحظر', 'حسابات', ':', 'المباحث', 'تابعني', 'واتابعك', 'زياده', 'المتابعين', 'الاخبار', 'المحليه', '#', 'الشعب_يقول_كلمته', '#', 'جماعه_انصار_بيت_طنيطر', '#', 'الرياض', '#', 'جده', 'نرجوا_عدم', 'عدم_متابعه', 'متابعه_وحظر', 'وحظر_حسابات', 'حسابات_:', ':_المباحث', 'المباحث_تابعني', 'تابعني_واتابعك', 'واتابعك_زياده', 'زياده_المتابعين', 'المتابعين_الاخبار', 'الاخبار_المحليه', 'المحليه_#', '#_الشعب_يقول_كلمته', 'الشعب_يقول_كلمته_#', '#_جماعه_انصار_بيت_طنيطر', 'جماعه_انصار_بيت_طنيطر_#', '#_الرياض', 'الرياض_#', '#_جده', 'نرجوا_عدم_متابعه', 'عدم_متابعه_وحظر', 'متابعه_وحظر_حسابات', 'وحظر_حسابات_:', 'حسابات_:_المباحث', ':_المباحث_تابعني', 'المباحث_تابعني_واتابعك', 'تابعني_واتابعك_زياده', 'واتابعك_زياده_المتابعين', 'زياده_المتابعين_الاخبار', 'المتابعين_الاخبار_المحليه', 'الاخبار_المحليه_#', 'المحليه_#_الشعب_يقول_كلمته', '#_الشعب_يقول_كلمته_#', 'الشعب_يقول_كلمته_#_جماعه_انصار_بيت_طنيطر', '#_جماعه_انصار_بيت_طنيطر_#', 'جماعه_انصار_بيت_طنيطر_#_الرياض', '#_الرياض_#', 'الرياض_#_جده'], ['المسلماني', 'اغلي', 'متحدث', 'اعلامي', 'للرئيس', 'واشهر', 'كذاب', 'ومنافق', 'ومضلل', 'ومحرض', 'تحول', 'الي', 'قليل', 'الادب', 'وضع', 'معارضي', 'الجيش', 'في', 'سله', 'القمامه', 'كلام', 'زباله', 'المسلماني_اغلي', 'اغلي_متحدث', 'متحدث_اعلامي', 'اعلامي_للرئيس', 'للرئيس_واشهر', 'واشهر_كذاب', 'كذاب_ومنافق', 'ومنافق_ومضلل', 'ومضلل_ومحرض', 'ومحرض_تحول', 'تحول_الي', 'الي_قليل', 'قليل_الادب', 'الادب_وضع', 'وضع_معارضي', 'معارضي_الجيش', 'الجيش_في', 'في_سله', 'سله_القمامه', 'القمامه_كلام', 'كلام_زباله', 'المسلماني_اغلي_متحدث', 'اغلي_متحدث_اعلامي', 'متحدث_اعلامي_للرئيس', 'اعلامي_للرئيس_واشهر', 'للرئيس_واشهر_كذاب', 'واشهر_كذاب_ومنافق', 'كذاب_ومنافق_ومضلل', 'ومنافق_ومضلل_ومحرض', 'ومضلل_ومحرض_تحول', 'ومحرض_تحول_الي', 'تحول_الي_قليل', 'الي_قليل_الادب', 'قليل_الادب_وضع', 'الادب_وضع_معارضي', 'وضع_معارضي_الجيش', 'معارضي_الجيش_في', 'الجيش_في_سله', 'في_سله_القمامه', 'سله_القمامه_كلام', 'القمامه_كلام_زباله'], ['الاحد', '3', 'يوليو', ':', 'ابو', 'الفتوح', 'في', 'ندوه', 'بالمؤتمر', 'السنوي', 'لهندسه', 'عين', 'شمس', '5', 'مساء', 'الاحد_3', '3_يوليو', 'يوليو_:', ':_ابو', 'ابو_الفتوح', 'الفتوح_في', 'في_ندوه', 'ندوه_بالمؤتمر', 'بالمؤتمر_السنوي', 'السنوي_لهندسه', 'لهندسه_عين', 'عين_شمس', 'شمس_5', '5_مساء', 'الاحد_3_يوليو', '3_يوليو_:', 'يوليو_:_ابو', ':_ابو_الفتوح', 'ابو_الفتوح_في', 'الفتوح_في_ندوه', 'في_ندوه_بالمؤتمر', 'ندوه_بالمؤتمر_السنوي', 'بالمؤتمر_السنوي_لهندسه', 'السنوي_لهندسه_عين', 'لهندسه_عين_شمس', 'عين_شمس_5', 'شمس_5_مساء']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "X_test_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(X_test_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", X_test_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "fullgram_X_test_text_tokenized = get_all_ngrams(X_test_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "911f1085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(201, 132, 300)\n",
      "(201, 39600)\n",
      "==================================================\n",
      "[-0.05737  -0.2964    0.4402   -0.03635   0.2576   -0.579     0.0986\n",
      "  0.9897    0.005226 -0.2986    0.1382   -0.047    -0.355    -0.5176\n",
      "  0.1824    0.00398  -0.115    -0.398     0.4712   -0.6914    0.0857\n",
      "  0.3457    0.01616  -0.4534   -0.04105  -0.1515   -0.525     0.3433\n",
      " -0.07227  -0.6094    0.663    -0.7783    0.1589    0.2815   -0.2023\n",
      "  0.04883  -0.2415    0.08405   0.2712    0.0965    0.236    -0.2524\n",
      "  0.059    -0.3374   -0.3354    0.4438   -0.3762    0.348     0.00602\n",
      " -0.4531  ]\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.6915422885572139\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "rezk_model = keras_load_model(\"../ml_models_saved/dl_models/run_with_rezk_skipgram_NS_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05__model.h5\"  )\n",
    "\n",
    "X_test_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_X_test_text_tokenized, max_len_str)\n",
    "X_test_embed_matrix = X_test_embed_matrix.reshape(X_test_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "\n",
    "keras_f1_score_result(rezk_model, X_test_embed_matrix, y_test)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e9bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
