{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "becc1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../preprocess_assets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd696ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\n",
    "import tensorflow as tf\n",
    "from features_extraction import *\n",
    "from data_shuffling_split import *\n",
    "from ara_vec_preprocess_configs import *\n",
    "from ml_modeling import *\n",
    "from keras_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26944902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>مع انتشار الامراض ومنها الكورونا اليكم النصيحه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1 دراسه بحثيه صادره معهد الدراسات الاستراتيجي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>بدات في مدينه وهان في الصين انتقلت لبعض المقاط...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>coronarvirus CoronaOutbreak فيروس_كورونا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>الاداره_العامه_للطيران_المدني اجراءات وقاءيه ح...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  مع انتشار الامراض ومنها الكورونا اليكم النصيحه...\n",
       "1      0   1 دراسه بحثيه صادره معهد الدراسات الاستراتيجي...\n",
       "2      1  بدات في مدينه وهان في الصين انتقلت لبعض المقاط...\n",
       "3      1           coronarvirus CoronaOutbreak فيروس_كورونا\n",
       "4      1  الاداره_العامه_للطيران_المدني اجراءات وقاءيه ح..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set = read_file(\"train/strat_train_set.csv\")\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f520f906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of instances in the training data after StratifiedShuffleSplit are:  8643\n",
      "The number of instances in the testing data after StratifiedShuffleSplit are:   177\n",
      "The number of trainin instances:  8643\n",
      "The number of validation instances:  177\n",
      "The number of trainin labels :  8643\n",
      "The number of validation labels :  177\n"
     ]
    }
   ],
   "source": [
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a3b997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['حراميه البرلمان الفاسدين يريدون يدمرون العراق لسبعين سنه قادمه باتفاقيه الصين ام الصناعه الفاشله تشتري صناعه صينيه ينزل سعرها للنص لانها فاشله لعنه الله علي ايران وعلي الحكومه العراقيه الفاسده والمعممين الفاسدين والميليشيات الخونه والاحزاب الاسلاميه المدسوسه عميله ايران', 'الصين سوف تسيطر علي العالم سياسيا وعسكريا قريبا', 'لا للتبادل التجاري مع الصين ما دامت تكذب وتخادع وتسرق ']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['حراميه', 'البرلمان', 'الفاسدين', 'يريدون', 'يدمرون', 'العراق', 'لسبعين', 'سنه', 'قادمه', 'باتفاقيه', 'الصين', 'ام', 'الصناعه', 'الفاشله', 'تشتري', 'صناعه', 'صينيه', 'ينزل', 'سعرها', 'للنص', 'لانها', 'فاشله', 'لعنه', 'الله', 'علي', 'ايران', 'وعلي', 'الحكومه', 'العراقيه', 'الفاسده', 'والمعممين', 'الفاسدين', 'والميليشيات', 'الخونه', 'والاحزاب', 'الاسلاميه', 'المدسوسه', 'عميله', 'ايران'], ['الصين', 'سوف', 'تسيطر', 'علي', 'العالم', 'سياسيا', 'وعسكريا', 'قريبا'], ['لا', 'للتبادل', 'التجاري', 'مع', 'الصين', 'ما', 'دامت', 'تكذب', 'وتخادع', 'وتسرق']]\n",
      "==================================================\n",
      "Before Tokenization : \n",
      " ['الخبر جاء باللحضه خايفه عليه وادور عن موقعه بالصين واذا بعيد عن وهان يربي منه لمتي يهمل نفسه', 'بما ان سعد البزاز ممتعض اتفاقيه الصين اذن خير للعراق', 'الصين ارتفاع وفيات فيروس كورونا 106 اشخاص كونا']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['الخبر', 'جاء', 'باللحضه', 'خايفه', 'عليه', 'وادور', 'عن', 'موقعه', 'بالصين', 'واذا', 'بعيد', 'عن', 'وهان', 'يربي', 'منه', 'لمتي', 'يهمل', 'نفسه'], ['بما', 'ان', 'سعد', 'البزاز', 'ممتعض', 'اتفاقيه', 'الصين', 'اذن', 'خير', 'للعراق'], ['الصين', 'ارتفاع', 'وفيات', 'فيروس', 'كورونا', '106', 'اشخاص', 'كونا']]\n",
      "full gram tokenization : \n",
      " [['حراميه', 'البرلمان', 'الفاسدين', 'يريدون', 'يدمرون', 'العراق', 'لسبعين', 'سنه', 'قادمه', 'باتفاقيه', 'الصين', 'ام', 'الصناعه', 'الفاشله', 'تشتري', 'صناعه', 'صينيه', 'ينزل', 'سعرها', 'للنص', 'لانها', 'فاشله', 'لعنه', 'الله', 'علي', 'ايران', 'وعلي', 'الحكومه', 'العراقيه', 'الفاسده', 'والمعممين', 'الفاسدين', 'والميليشيات', 'الخونه', 'والاحزاب', 'الاسلاميه', 'المدسوسه', 'عميله', 'ايران', 'حراميه_البرلمان', 'البرلمان_الفاسدين', 'الفاسدين_يريدون', 'يريدون_يدمرون', 'يدمرون_العراق', 'العراق_لسبعين', 'لسبعين_سنه', 'سنه_قادمه', 'قادمه_باتفاقيه', 'باتفاقيه_الصين', 'الصين_ام', 'ام_الصناعه', 'الصناعه_الفاشله', 'الفاشله_تشتري', 'تشتري_صناعه', 'صناعه_صينيه', 'صينيه_ينزل', 'ينزل_سعرها', 'سعرها_للنص', 'للنص_لانها', 'لانها_فاشله', 'فاشله_لعنه', 'لعنه_الله', 'الله_علي', 'علي_ايران', 'ايران_وعلي', 'وعلي_الحكومه', 'الحكومه_العراقيه', 'العراقيه_الفاسده', 'الفاسده_والمعممين', 'والمعممين_الفاسدين', 'الفاسدين_والميليشيات', 'والميليشيات_الخونه', 'الخونه_والاحزاب', 'والاحزاب_الاسلاميه', 'الاسلاميه_المدسوسه', 'المدسوسه_عميله', 'عميله_ايران', 'حراميه_البرلمان_الفاسدين', 'البرلمان_الفاسدين_يريدون', 'الفاسدين_يريدون_يدمرون', 'يريدون_يدمرون_العراق', 'يدمرون_العراق_لسبعين', 'العراق_لسبعين_سنه', 'لسبعين_سنه_قادمه', 'سنه_قادمه_باتفاقيه', 'قادمه_باتفاقيه_الصين', 'باتفاقيه_الصين_ام', 'الصين_ام_الصناعه', 'ام_الصناعه_الفاشله', 'الصناعه_الفاشله_تشتري', 'الفاشله_تشتري_صناعه', 'تشتري_صناعه_صينيه', 'صناعه_صينيه_ينزل', 'صينيه_ينزل_سعرها', 'ينزل_سعرها_للنص', 'سعرها_للنص_لانها', 'للنص_لانها_فاشله', 'لانها_فاشله_لعنه', 'فاشله_لعنه_الله', 'لعنه_الله_علي', 'الله_علي_ايران', 'علي_ايران_وعلي', 'ايران_وعلي_الحكومه', 'وعلي_الحكومه_العراقيه', 'الحكومه_العراقيه_الفاسده', 'العراقيه_الفاسده_والمعممين', 'الفاسده_والمعممين_الفاسدين', 'والمعممين_الفاسدين_والميليشيات', 'الفاسدين_والميليشيات_الخونه', 'والميليشيات_الخونه_والاحزاب', 'الخونه_والاحزاب_الاسلاميه', 'والاحزاب_الاسلاميه_المدسوسه', 'الاسلاميه_المدسوسه_عميله', 'المدسوسه_عميله_ايران'], ['الصين', 'سوف', 'تسيطر', 'علي', 'العالم', 'سياسيا', 'وعسكريا', 'قريبا', 'الصين_سوف', 'سوف_تسيطر', 'تسيطر_علي', 'علي_العالم', 'العالم_سياسيا', 'سياسيا_وعسكريا', 'وعسكريا_قريبا', 'الصين_سوف_تسيطر', 'سوف_تسيطر_علي', 'تسيطر_علي_العالم', 'علي_العالم_سياسيا', 'العالم_سياسيا_وعسكريا', 'سياسيا_وعسكريا_قريبا'], ['لا', 'للتبادل', 'التجاري', 'مع', 'الصين', 'ما', 'دامت', 'تكذب', 'وتخادع', 'وتسرق', 'لا_للتبادل', 'للتبادل_التجاري', 'التجاري_مع', 'مع_الصين', 'الصين_ما', 'ما_دامت', 'دامت_تكذب', 'تكذب_وتخادع', 'وتخادع_وتسرق', 'لا_للتبادل_التجاري', 'للتبادل_التجاري_مع', 'التجاري_مع_الصين', 'مع_الصين_ما', 'الصين_ما_دامت', 'ما_دامت_تكذب', 'دامت_تكذب_وتخادع', 'تكذب_وتخادع_وتسرق']]\n",
      "==================================================\n",
      "full gram tokenization : \n",
      " [['الخبر', 'جاء', 'باللحضه', 'خايفه', 'عليه', 'وادور', 'عن', 'موقعه', 'بالصين', 'واذا', 'بعيد', 'عن', 'وهان', 'يربي', 'منه', 'لمتي', 'يهمل', 'نفسه', 'الخبر_جاء', 'جاء_باللحضه', 'باللحضه_خايفه', 'خايفه_عليه', 'عليه_وادور', 'وادور_عن', 'عن_موقعه', 'موقعه_بالصين', 'بالصين_واذا', 'واذا_بعيد', 'بعيد_عن', 'عن_وهان', 'وهان_يربي', 'يربي_منه', 'منه_لمتي', 'لمتي_يهمل', 'يهمل_نفسه', 'الخبر_جاء_باللحضه', 'جاء_باللحضه_خايفه', 'باللحضه_خايفه_عليه', 'خايفه_عليه_وادور', 'عليه_وادور_عن', 'وادور_عن_موقعه', 'عن_موقعه_بالصين', 'موقعه_بالصين_واذا', 'بالصين_واذا_بعيد', 'واذا_بعيد_عن', 'بعيد_عن_وهان', 'عن_وهان_يربي', 'وهان_يربي_منه', 'يربي_منه_لمتي', 'منه_لمتي_يهمل', 'لمتي_يهمل_نفسه'], ['بما', 'ان', 'سعد', 'البزاز', 'ممتعض', 'اتفاقيه', 'الصين', 'اذن', 'خير', 'للعراق', 'بما_ان', 'ان_سعد', 'سعد_البزاز', 'البزاز_ممتعض', 'ممتعض_اتفاقيه', 'اتفاقيه_الصين', 'الصين_اذن', 'اذن_خير', 'خير_للعراق', 'بما_ان_سعد', 'ان_سعد_البزاز', 'سعد_البزاز_ممتعض', 'البزاز_ممتعض_اتفاقيه', 'ممتعض_اتفاقيه_الصين', 'اتفاقيه_الصين_اذن', 'الصين_اذن_خير', 'اذن_خير_للعراق'], ['الصين', 'ارتفاع', 'وفيات', 'فيروس', 'كورونا', '106', 'اشخاص', 'كونا', 'الصين_ارتفاع', 'ارتفاع_وفيات', 'وفيات_فيروس', 'فيروس_كورونا', 'كورونا_106', '106_اشخاص', 'اشخاص_كونا', 'الصين_ارتفاع_وفيات', 'ارتفاع_وفيات_فيروس', 'وفيات_فيروس_كورونا', 'فيروس_كورونا_106', 'كورونا_106_اشخاص', '106_اشخاص_كونا']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])\n",
    "\n",
    "fullgram_x_train_text_tokenized = get_all_ngrams(x_train_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "fullgram_x_val_text_tokenized = get_all_ngrams(x_val_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_val_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aba759",
   "metadata": {},
   "source": [
    "# Our CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9673ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_word2vec_model = load_word2vec_model(\"../word2vec_models/rezk/skipgram_NS/sk_gr_negative_sampeling_fullgram_vec_size_300-d_min_count_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e0df14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(8643, 237, 300)\n",
      "(8643, 71100)\n",
      "==================================================\n",
      "[ 0.2402    -0.1918     0.1027    -0.0613     0.1823    -0.0956\n",
      "  0.1512     0.249      0.1882    -0.0598    -0.00992   -0.2202\n",
      "  0.0854     0.08923   -0.173      0.246      0.1532     0.07855\n",
      " -0.1445    -0.504      0.03442   -0.3127    -0.002003  -0.0835\n",
      " -0.1401    -0.3276    -0.6074    -0.3103    -0.2012    -0.005264\n",
      "  0.06476   -0.1892    -0.07336    0.0315    -0.3127    -0.2084\n",
      " -0.0134    -0.3484     0.1562     0.2559    -0.02588   -0.0537\n",
      " -0.0007763 -0.0876     0.13      -0.1254    -0.2491     0.524\n",
      "  0.1714     0.00745  ]\n",
      "==================================================\n",
      "(177, 237, 300)\n",
      "(177, 71100)\n",
      "==================================================\n",
      "[ 0.0918   -0.011734  0.1099   -0.03403  -0.2878   -0.1137    0.1555\n",
      "  0.5137   -0.04056   0.00814  -0.006405  0.127    -0.1863   -0.2913\n",
      " -0.1727   -0.00464  -0.2751   -0.2072   -0.1681   -0.5215    0.11334\n",
      "  0.003372 -0.366     0.02788   0.3557   -0.1781   -0.1787   -0.2018\n",
      " -0.1284   -0.0605    0.478    -0.1995   -0.1384   -0.1465   -0.01225\n",
      "  0.118     0.1432    0.2211    0.3       0.274    -0.1823    0.1716\n",
      " -0.00285  -0.483     0.10095   0.0397    0.02806   0.1826   -0.01897\n",
      " -0.1285  ]\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 237\n",
    "word2vec_path = \"rezk/cbow/\"\n",
    "model_path_to_save = \"../ml_models_saved/\"\n",
    "hid_num_neurons = 25\n",
    "learning_rate = .00005\n",
    "epochs = 10\n",
    "estimators = voting_models()\n",
    "\n",
    "performance_lr = keras.callbacks.ReduceLROnPlateau(factor=.5, patience=5)\n",
    "RMSprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=.9)\n",
    "\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_x_val_text_tokenized, max_len_str)\n",
    "# Reshape because of deep learning model\n",
    "X_train_embed_matrix = X_train_embed_matrix.reshape(X_train_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "X_val_embed_matrix = X_val_embed_matrix.reshape(X_val_embed_matrix.shape[0], max_len_str, number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d393ac9",
   "metadata": {},
   "source": [
    "# Our CBOW Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f9a31",
   "metadata": {},
   "source": [
    "# With  Rmsprob and  Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d7309aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "./../ml_models_saved/dl_models/tensor_logs/run_2022_05_09_23_58_39_rezk_skipgram_NS_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 23:58:39.997776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:58:40.009086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:58:40.010909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:58:40.448301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:58:40.450121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:58:40.451846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:58:41.872254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:58:41.874078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:58:41.875785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9632 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:05:00.0, compute capability: 7.5\n",
      "2022-05-09 23:58:41.876484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-09 23:58:41.877307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9652 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 237, 300)         1200      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 237, 25)           32600     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 237, 25)          100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 237, 25)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5925)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 18)                106668    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140,568\n",
      "Trainable params: 139,918\n",
      "Non-trainable params: 650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "callbacks_ = keras_callbacks(word2vec_type=\"rezk_skipgram_NS_word2vec\", model_type=\"Rmsprob_lstm_with_batch\", learning_rate=learning_rate)\n",
    "callbacks_.append(performance_lr)\n",
    "model = lstm_with_batch_model_create(hid_num_neurons, max_len_str, number_of_features, dropout=.2)\n",
    "model = seqential_model_compile(model, RMSprop_optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "765ddbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 23:58:57.138094: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/271 [==============================] - 16s 40ms/step - loss: 1.0044 - accuracy: 0.5588 - val_loss: 5.3490 - val_accuracy: 0.0000e+00 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "271/271 [==============================] - 10s 39ms/step - loss: 0.5751 - accuracy: 0.7179 - val_loss: 0.4173 - val_accuracy: 0.8249 - lr: 5.0000e-05\n",
      "Epoch 3/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.4230 - accuracy: 0.8086 - val_loss: 0.3143 - val_accuracy: 0.8531 - lr: 5.0000e-05\n",
      "Epoch 4/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.3603 - accuracy: 0.8444 - val_loss: 0.2678 - val_accuracy: 0.8870 - lr: 5.0000e-05\n",
      "Epoch 5/10\n",
      "271/271 [==============================] - 10s 38ms/step - loss: 0.3111 - accuracy: 0.8697 - val_loss: 0.2479 - val_accuracy: 0.8814 - lr: 5.0000e-05\n",
      "Epoch 6/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.2876 - accuracy: 0.8823 - val_loss: 0.2392 - val_accuracy: 0.8927 - lr: 5.0000e-05\n",
      "Epoch 7/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.2619 - accuracy: 0.8942 - val_loss: 0.2306 - val_accuracy: 0.8927 - lr: 5.0000e-05\n",
      "Epoch 8/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.2385 - accuracy: 0.9032 - val_loss: 0.2277 - val_accuracy: 0.8870 - lr: 5.0000e-05\n",
      "Epoch 9/10\n",
      "271/271 [==============================] - 10s 36ms/step - loss: 0.2261 - accuracy: 0.9096 - val_loss: 0.2437 - val_accuracy: 0.9040 - lr: 5.0000e-05\n",
      "Epoch 10/10\n",
      "271/271 [==============================] - 10s 37ms/step - loss: 0.2100 - accuracy: 0.9153 - val_loss: 0.2383 - val_accuracy: 0.8983 - lr: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_embed_matrix, y_train, batch_size=32, epochs=epochs, validation_data=(X_val_embed_matrix, y_val),\n",
    "                   callbacks=callbacks_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecb7343",
   "metadata": {},
   "source": [
    "# Load best model & predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60627c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>حساء خفافيش الصين ابداع القرف يعني</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>اجراءات هكذا استعدت مصر لمواجهه فيروس كورونا م...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ايش الفايده لابس كمامات ومو لابس قفازات</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>الجدير بالذكر ان في الصين بيستخدموا ال في كل ح...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ولو ترسل صواريخ الصين وقفنا دون برج الفيصليه</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1                 حساء خفافيش الصين ابداع القرف يعني\n",
       "1      1  اجراءات هكذا استعدت مصر لمواجهه فيروس كورونا م...\n",
       "2      1            ايش الفايده لابس كمامات ومو لابس قفازات\n",
       "3      0  الجدير بالذكر ان في الصين بيستخدموا ال في كل ح...\n",
       "4      0       ولو ترسل صواريخ الصين وقفنا دون برج الفيصليه"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_test_set = read_file(\"test/strat_test_set.csv\")\n",
    "strat_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd5fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text = list(strat_test_set['text'])\n",
    "y_test = strat_test_set['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47184f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenization : \n",
      " ['حساء خفافيش الصين ابداع القرف يعني', 'اجراءات هكذا استعدت مصر لمواجهه فيروس كورونا مرايتي', 'ايش الفايده لابس كمامات ومو لابس قفازات']\n",
      "==================================================\n",
      "After Tokenization : \n",
      " [['حساء', 'خفافيش', 'الصين', 'ابداع', 'القرف', 'يعني'], ['اجراءات', 'هكذا', 'استعدت', 'مصر', 'لمواجهه', 'فيروس', 'كورونا', 'مرايتي'], ['ايش', 'الفايده', 'لابس', 'كمامات', 'ومو', 'لابس', 'قفازات']]\n",
      "==================================================\n",
      "full gram tokenization : \n",
      " [['حساء', 'خفافيش', 'الصين', 'ابداع', 'القرف', 'يعني', 'حساء_خفافيش', 'خفافيش_الصين', 'الصين_ابداع', 'ابداع_القرف', 'القرف_يعني', 'حساء_خفافيش_الصين', 'خفافيش_الصين_ابداع', 'الصين_ابداع_القرف', 'ابداع_القرف_يعني'], ['اجراءات', 'هكذا', 'استعدت', 'مصر', 'لمواجهه', 'فيروس', 'كورونا', 'مرايتي', 'اجراءات_هكذا', 'هكذا_استعدت', 'استعدت_مصر', 'مصر_لمواجهه', 'لمواجهه_فيروس', 'فيروس_كورونا', 'كورونا_مرايتي', 'اجراءات_هكذا_استعدت', 'هكذا_استعدت_مصر', 'استعدت_مصر_لمواجهه', 'مصر_لمواجهه_فيروس', 'لمواجهه_فيروس_كورونا', 'فيروس_كورونا_مرايتي'], ['ايش', 'الفايده', 'لابس', 'كمامات', 'ومو', 'لابس', 'قفازات', 'ايش_الفايده', 'الفايده_لابس', 'لابس_كمامات', 'كمامات_ومو', 'ومو_لابس', 'لابس_قفازات', 'ايش_الفايده_لابس', 'الفايده_لابس_كمامات', 'لابس_كمامات_ومو', 'كمامات_ومو_لابس', 'ومو_لابس_قفازات']]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "X_test_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(X_test_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", X_test_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "fullgram_X_test_text_tokenized = get_all_ngrams(X_test_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cd571dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "(180, 237, 300)\n",
      "(180, 71100)\n",
      "==================================================\n",
      "[-3.567e-04 -8.929e-02  2.818e-02  2.356e-01  1.252e-01  3.792e-03\n",
      " -2.191e-02  5.156e-01  1.274e-01 -2.681e-01 -3.408e-01 -1.803e-01\n",
      " -1.157e-01  1.272e-01 -3.137e-01 -2.996e-01  1.681e-01 -7.983e-02\n",
      " -1.969e-01 -5.366e-01 -3.218e-03 -3.503e-01  2.489e-01 -1.070e-01\n",
      "  4.443e-01  1.738e-02 -2.444e-01 -5.503e-01 -3.960e-01  3.381e-01\n",
      "  1.674e-01 -5.487e-02  3.347e-01  1.087e-01  2.939e-01  1.553e-01\n",
      "  3.411e-01  2.720e-01  3.496e-03 -2.512e-01  2.664e-01  2.593e-01\n",
      "  8.331e-02 -1.775e-01  2.263e-01  1.796e-01  1.604e-01 -5.878e-02\n",
      "  2.223e-01  3.082e-02]\n",
      "===================== Validate Result =====================\n",
      "F1 score is:  0.9222222222222223\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "rezk_model = keras_load_model(\"../ml_models_saved/dl_models/run_with_rezk_skipgram_NS_word2vec_Rmsprob_lstm_with_batch_learning_rate=5e-05__model.h5\"  )\n",
    "\n",
    "X_test_embed_matrix = text_to_matrix_using_word2vec(our_word2vec_model, fullgram_X_test_text_tokenized, max_len_str)\n",
    "X_test_embed_matrix = X_test_embed_matrix.reshape(X_test_embed_matrix.shape[0], max_len_str, number_of_features)\n",
    "\n",
    "keras_f1_score_result(rezk_model, X_test_embed_matrix, y_test)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c25a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
