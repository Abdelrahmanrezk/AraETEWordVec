{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c38b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../preprocess_assets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main libraries \n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Our files\n",
    "from data_shuffling_split import *\n",
    "from features_extraction import *\n",
    "from ara_vec_preprocess_configs import *\n",
    "from ml_modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b125bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set = read_file(\"train/strat_train_set.csv\")\n",
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adb7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text, x_val_text, y_train, y_val = prepare_data(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d443a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_train_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_train_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_val_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(x_val_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", x_val_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", x_val_text_tokenized[:3])\n",
    "\n",
    "fullgram_x_train_text_tokenized = get_all_ngrams(x_train_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_train_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "fullgram_x_val_text_tokenized = get_all_ngrams(x_val_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_x_val_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b574d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get how many words inside each text after tokenization\n",
    "num_of_words_in_each_text = [len(text) for text in fullgram_x_train_text_tokenized]\n",
    "max_len = max(num_of_words_in_each_text)\n",
    "print(\"The max length is: \", max_len)\n",
    "num_of_words_in_each_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times these value repeated and sort them\n",
    "new_dicts = get_keys_that_val_gr_than_num(num_of_words_in_each_text, 1000)\n",
    "keys = list(new_dicts.keys())\n",
    "values = list(new_dicts.values())\n",
    "plt.style.use('dark_background')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 6)\n",
    "plt.bar(range(len(new_dicts)), values, tick_label=keys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d666b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = load_word2vec_model(\"../word2vec_models/rezk_unigram_CBOW_model/train_word2vec_cbow__window_3_min_count_300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = 300\n",
    "max_len_str = 128\n",
    "word2vec_path = \"rezk/\"\n",
    "model_path_to_save = \"../ml_models_saved/\"\n",
    "estimators = voting_models()\n",
    "\n",
    "X_train_embed_matrix = text_to_matrix_using_word2vec(word2vec_model, fullgram_x_train_text_tokenized, max_len_str)\n",
    "X_val_embed_matrix = text_to_matrix_using_word2vec(word2vec_model, fullgram_x_val_text_tokenized, max_len_str)\n",
    "\n",
    "print(X_train_embed_matrix.shape)\n",
    "print(\"=\"*50)\n",
    "print(X_val_embed_matrix.shape)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7fc860",
   "metadata": {},
   "source": [
    "# Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='l2', C=1, multi_class='multinomial', solver='lbfgs', verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d088a7",
   "metadata": {},
   "source": [
    "# Train SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240589b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(C=0.5,  max_iter=50, verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef9bae",
   "metadata": {},
   "source": [
    "# Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c04c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VotingClassifier(estimators, voting=\"hard\")\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6ab06",
   "metadata": {},
   "source": [
    "# Extr Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=100, max_depth=5, max_samples=.1, verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d2aa4",
   "metadata": {},
   "source": [
    "# AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f861c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(C=0.5,  verbose=1)\n",
    "model = AdaBoostClassifier(model,  algorithm=\"SAMME\", n_estimators=5)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693af6be",
   "metadata": {},
   "source": [
    "#  Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc7ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=10, subsample=.1, learning_rate=.5,   max_depth=5, verbose=1)\n",
    "model = ml_classifer_pipeline(model, X_train_embed_matrix, y_train, X_val_embed_matrix, y_val,word2vec_path, model_path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d2a63",
   "metadata": {},
   "source": [
    "# Load best model & predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set = read_file(\"test/strat_test_set.csv\")\n",
    "strat_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c4379",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text = list(strat_test_set['text'])\n",
    "y_test = strat_test_set['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de658bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text_tokenized = tokenize_using_nltk_TreebankWordTokenizer(X_test_text)\n",
    "\n",
    "print(\"Before Tokenization : \\n\", X_test_text[:3])\n",
    "print(\"=\"*50)\n",
    "print(\"After Tokenization : \\n\", X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)\n",
    "\n",
    "fullgram_X_test_text_tokenized = get_all_ngrams(X_test_text_tokenized)\n",
    "print(\"full gram tokenization : \\n\", fullgram_X_test_text_tokenized[:3])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_embed_matrix = text_to_matrix_using_word2vec(word2vec_model, fullgram_X_test_text_tokenized, max_len_str)\n",
    "\n",
    "print(X_test_embed_matrix.shape)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28963565",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle_load_model(\"ml_models_saved/rezk/\" + )\n",
    "f1_score_result(model, X_test_embed_matrix, y_test):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
