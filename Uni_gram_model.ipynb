{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91decaea",
   "metadata": {},
   "source": [
    "##  هام جدا عشان تعمل تران جملة جملة\n",
    "\n",
    "هنشتغل على السيرفر الى بنص دولار إن شاء الله \n",
    "https://rare-technologies.com/word2vec-tutorial/\n",
    "\n",
    "https://stackoverflow.com/questions/60852962/training-time-of-gensim-word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca8435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "import gensim\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "\n",
    "from utilities import * # import utilities.py module\n",
    "from DifferentTokenization import * # import utilities.py module\n",
    "import pandas as pd\n",
    "from configs.Configs import get_ngrams\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "from train_word2vec_from_scratch import *\n",
    "from ara_vec_preprocess_configs import *\n",
    "# ============================   \n",
    "# ====== N-Grams Models ======\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae9cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gdown\n",
    "\n",
    "# url = \"https://drive.google.com/u/2/uc?id=1ckYRCU7tRbvWyeT78gQssTgRcy9fpb4T&export=download\"\n",
    "# output = \"server_1.zip\"\n",
    "# gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbafae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram_text_list = tweets_to_build_word2vec_on(DATA_PREPROCESSED_DIR)\n",
    "# print(unigram_text_list[:3])\n",
    "# random.shuffle(unigram_text_list)\n",
    "# print(unigram_text_list[:3])\n",
    "\n",
    "# print(\"The overall number of tweets are: \" + str(len(unigram_text_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "822c4337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus():\n",
    "    \"\"\"Iterate over sentences from the corpus.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.dirname = ARABIC_FILTERED_DATA_DIR\n",
    "        self.tree_tokenizer = TreebankWordTokenizer()\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            print(fname)\n",
    "            readed_data                 = read_file(os.path.join(self.dirname, fname))\n",
    "            tweets                      = list(readed_data['tweets'])\n",
    "\n",
    "            \n",
    "            for tweet in tweets:\n",
    "                try:\n",
    "                    words = self.tree_tokenizer.tokenize(tweet)\n",
    "                    yield words\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "sentences = Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5090f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = test_nltk_RegexpTokenizer_time(unigram_text_list, True)\n",
    "# unigram_text_list_tokenized = tokenizer.tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f79ed31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(unigram_text_list_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb10aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_all_tokens = convert_list_of_lists_to_one_list(unigram_text_list_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ac7df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import collections\n",
    "# counter=collections.Counter(list_of_all_tokens)\n",
    "# c = dict(sorted(counter.items(), key=lambda item: item[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4de049e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# els = list(c.keys()) # explicitly convert to a list, in case it's Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb555edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# els[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca3c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic_filtered_tweets_of_year_2008.csv\n",
      "arabic_filtered_tweets_of_year_2009.csv\n",
      "arabic_filtered_tweets_of_year_2010.csv\n",
      "arabic_filtered_tweets_of_year_2016.csv\n",
      "arabic_filtered_tweets_of_year_2017.csv\n",
      "arabic_filtered_tweets_of_year_2019.csv\n",
      "arabic_filtered_tweets_of_year_2020.csv\n",
      "arabic_filtered_tweets_of_year_2021.csv\n",
      "Epoch #0 start\n",
      "2022-02-17 17:32:20.421972\n",
      "arabic_filtered_tweets_of_year_2008.csv\n",
      "arabic_filtered_tweets_of_year_2009.csv\n",
      "arabic_filtered_tweets_of_year_2010.csv\n",
      "arabic_filtered_tweets_of_year_2016.csv\n",
      "arabic_filtered_tweets_of_year_2017.csv\n",
      "arabic_filtered_tweets_of_year_2019.csv\n",
      "arabic_filtered_tweets_of_year_2020.csv\n",
      "arabic_filtered_tweets_of_year_2021.csv\n"
     ]
    }
   ],
   "source": [
    "cbow_fullgram_100d_25_count = train_word2vec_cbow(\"unigram\", sentences, \n",
    "                                                 vector_size=300, min_count=25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c9218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_gram_fullgram_100d_25_count = train_word2vec_skip_gram(\"unigram\", sentences, \n",
    "                                                 vector_size=100, min_count=25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3456e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_g_neg_sam_fullgram_100d_25_count = train_word2vec_skip_gram_negative_sampeling(\"unigram\", \n",
    "                                             sentences, vector_size=100, min_count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f594a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
