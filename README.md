# AraETEWordVec


We provide our paper with code used for that research from first stage of collecting more than 350 million tweet, to provide free word embedding in Arabic language and run on different dataset to see the model ability compared to other available models.


We will start by some examples to show our work, then we will start to let you know how you can go from collecting data to the end of research.




## How to use to get Similar words & others

```

import gensim
from sklearn.decomposition import PCA
from bidi.algorithm import get_display
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import arabic_reshaper
import pandas as pd
from word2vec_results import *


# Once you download the model from drive link you can pass the direction of
# any model you need

# Example
rezk_model = gensim.models.Word2Vec.load("model_dir/continuous_bow_fullgram_vec_size_300-d_min_count_100")

similar_tokens = rezk_model.wv.most_similar('ğŸ¤£')
print(similar_tokens)

'''
[('ğŸ˜†', 0.8333120942115784),
 ('ğŸ˜', 0.8321138024330139),
 ('ğŸ˜œ', 0.8205730319023132),
 ('ğŸ˜', 0.819418728351593),
 ('ğŸ˜…', 0.8010570406913757),
 ('ğŸ˜’', 0.7967824339866638),
 ('Ù‡Ù‡', 0.7967004776000977),
 ('ğŸ˜‚', 0.7781848907470703),
 ('ğŸ˜¬', 0.7624844312667847),
 ('ğŸ˜€', 0.7618412375450134)]
'''

similar_tokens = rezk_model.wv.most_similar('ğŸ‡°ğŸ‡¼')
print(similar_tokens)

'''
[('ğŸ‡´ğŸ‡²', 0.6610525846481323),
 ('Ø§Ù„Ø¹ÙŠØ¯_Ø§Ù„ÙˆØ·Ù†ÙŠ_Ø§Ù„ÙƒÙˆÙŠØªÙŠ', 0.6014205813407898),
 ('Ø§Ù„ÙŠÙˆÙ…_Ø§Ù„ÙˆØ·Ù†ÙŠ_Ø§Ù„ÙƒÙˆÙŠØªÙŠ', 0.5943148732185364),
 ('ğŸ‡¦ğŸ‡ª', 0.5859056711196899),
 ('Ø§Ù„ÙƒÙˆÙŠØª', 0.5772003531455994),
 ('ğŸ‡¸ğŸ‡¦', 0.5617915987968445),
 ('ğŸ‡¶ğŸ‡¦', 0.5346092581748962),
 ('Ø¹Ù…Ø§Ù†', 0.5341886281967163),
 ('ğŸ‡¯ğŸ‡´', 0.5333636999130249),
 ('Ø§Ù„Ø­Ø¨ÙŠØ¨Ù‡', 0.5255107283592224)]
'''

similar_tokens = rezk_model.wv.most_similar('ÙƒÙˆÙÙŠØ¯')
print(similar_tokens)

'''
[('ÙƒÙˆÙÙŠØ¯-', 0.8830006122589111),
 ('ÙƒÙˆÙÙŠØ¯_', 0.8637586832046509),
 ('Covid', 0.7545498609542847),
 ('Ø¨ÙƒÙˆÙÙŠØ¯', 0.7023125886917114),
 ('Ø§Ù„ÙƒÙˆÙÙŠØ¯', 0.6937721967697144),
 ('ÙƒÙˆØ±ÙˆÙ†Ø§', 0.6714788675308228),
 ('ÙƒÙˆÚ¤ÙŠØ¯', 0.6528131365776062),
 ('COVID', 0.6056150794029236),
 ('Ø¨Ù€ÙƒÙˆÙÙŠØ¯-', 0.600813090801239),
 ('ÙÙŠØ±ÙˆØ³', 0.5611270666122437)]
'''

similar_tokens = rezk_model.wv.most_similar('Ù…Ù†ØµÙ‡_Ù…Ø¯Ø±Ø³ØªÙŠ')
print(similar_tokens)

'''
[('ØªØ¹Ù„ÙŠÙ…_Ø¬Ø¯Ù‡', 0.6406617164611816),
 ('Ù„Ø¹ÙˆØ¯Ù‡_Ø­Ø¶ÙˆØ±ÙŠÙ‡_Ø§Ù…Ù†Ù‡', 0.6082739233970642),
 ('ØªØ¹Ù„ÙŠÙ…_Ø§Ù„Ù…Ø®ÙˆØ§Ù‡', 0.6013671159744263),
 ('ØªØ¹Ù„ÙŠÙ…_Ø§Ù„Ø·Ø§Ø¦Ù', 0.5990505814552307),
 ('Ø§Ù„Ù…Ù„Ù_Ø§Ù„Ø§Ø¹Ù„Ø§Ù…ÙŠ_Ø¨ÙˆØ²Ø§Ø±Ù‡_Ø§Ù„ØªØ¹Ù„ÙŠÙ…', 0.5927740931510925),
 ('ØªØ¹Ù„ÙŠÙ…_Ø¹Ø³ÙŠØ±', 0.5926639437675476),
 ('ØªØ¹Ù„ÙŠÙ…_Ù…ÙƒÙ‡', 0.5854555368423462),
 ('ØªØ¹Ù„ÙŠÙ…_Ø§Ù„Ø±ÙŠØ§Ø¶', 0.581294059753418),
 ('ØªØ¹Ù„ÙŠÙ…_Ø§Ù„Ø±Ø³', 0.5742909908294678),
 ('ØªØ¹Ù„ÙŠÙ…_Ø§Ù„Ù‚ØµÙŠÙ…', 0.5729984641075134)]

'''

NER_WORDS = ["Ù†Øª", "Ø§Ù†Ø³ØªØ¬Ø±Ø§Ù…", "ÙÙŠØ³Ø¨ÙˆÙƒ", "IT", "HR", "AI", "Ø±Ø§ÙˆØªØ±", "Ù…ÙˆØ¯Ù…", "Ø±Ø³ÙŠÙØ±", "marketing", "Ø§Ù„Ø¯Ø¨Ø§Ø¨Ø§Øª", "Ø§Ù„Ù…Ø¯Ø±Ø¹Ø§Øª", "Ø§Ù„Ø·Ø§Ø¦Ø±Ø§Øª", 
             "Ø´Ø§ÙˆÙ…ÙŠ", "Ø³ÙˆÙ†ÙŠ", "Ù„ÙŠÙ†ÙˆÙÙˆ", "Ø³Ø§Ù…Ø³ÙˆÙ†Ø¬", "Ù‡ÙˆØ§ÙˆÙŠ", "Ø§ÙŠÙÙˆÙ†", "Ø§Ù„ØµÙˆØ§Ø±ÙŠØ®", "Ø§Ù„Ø±Ø§Ø¯Ø§Ø±", "Ø§Ù„Ø¨Ø§ØµØ§Øª", "Ø§Ù„Ù‚Ø·Ø§Ø±", "Ø§Ù„Ø³ÙŠØ§Ø±Ù‡", "Ø§Ù„Ø¯Ø±Ø§Ø¬Ù‡",
             "Ø³ÙˆÙ†ÙŠ", "ØªÙˆÙŠØªØ±", "Ø³Ù†Ø§Ø¨Ø´Ø§Øª", "Ø§Ù„ÙˆØ§ØªØ³Ø§Ø¨", "design", "Ø§ÙƒØªÙˆØ¨Ø±", "ÙŠÙˆÙ†ÙŠÙˆ", "Ø§ØºØ³Ø·Ø³", "Ù†ÙˆÙÙ…Ø¨Ø±", "Ù…Ø§ÙŠÙˆ", "ØªÙ…ÙˆØ²", "Ø´Ø¨Ø§Ø·", "Ø§ÙŠÙ„ÙˆÙ„",
             "Ù…Ø­Ù…Ø¯", "Ø§Ø¨Ø±Ø§Ù‡ÙŠÙ…", "Ø¹ÙŠØ³ÙŠ", "Ø§Ø³Ù…Ø§Ø¹ÙŠÙ„", "Ù‡Ù†Ø¯", "Ø³Ø§Ø±Ù‡", "Ù…Ø±Ø§Ù…", "Ø±ÙŠÙ…Ø§", "Ø®Ù„ÙˆØ¯",
            "Ø§ÙŠØ±Ø§Ù†", "ØªØ±ÙƒÙŠØ§", "Ø§Ù„Ø¨Ø­Ø±ÙŠÙ†", "Ø§Ù„ÙƒÙˆÙŠØª", "Ù‚Ø·Ø±", "Ø§Ù„Ø³ÙˆØ¯Ø§Ù†", "Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±", "ØªÙˆÙ†Ø³", "Ù…ØµØ±",
            "Ù…Ø®ØªØ¨Ø±", "Ù…Ø±ÙƒØ²", "Ø¨Ù„Ø¯ÙŠÙ‡", "Ù†Ù‚Ø§Ø¨Ù‡", "Ø¬Ù…Ø¹ÙŠÙ‡", "Ø´Ø±ÙƒÙ‡", "Ù…Ø¤Ø³Ø³Ù‡", "Ù…Ø¹Ù‡Ø¯", "Ø§ÙƒØ§Ø¯ÙŠÙ…ÙŠÙ‡"]


# Using functions in side word2vec_results.py to display graphs in image below

# Reduce the dimension of NER_WORDS
tsne_df_scale = tsne_graph(rezk_model, NER_WORDS, 1400, .03)

_ = init_graph_style(figsize=(16, 10))

_ = word_display(tsne_df_scale, NER_WORDS, "NER_WORDS.png")

```

### 
<img src="images/NER_WORDS_2.png">


## How to use to train your ML or DL Model



### Collecting Tweets

To collect this large dataset we have used twint project that help you collect very large number of tweets instead of waiting months to collect your dataset using twitter API, and as to be legal with twittter developer we have not provide this dataset except for resarch work.

```python

pip3 install -r requirements.txt

# or you have to clone the project in case of error(I have doing that)
# https://github.com/twintproject/twint
```

This work of collecting this large number of tweets is splited into two files:
- create directories from 2008 to 2021
- inside each of these directories create Month_01 to Month_12 case senstive
- twitter_configs.py inside preprocess_assets direction, which contain documented code and the main file.
- Twitter Crawling 2018 to 2021.ipynb inside notebooks direction, notebook to run the code that depends on twitter_configs file.


### Collecting Tweets