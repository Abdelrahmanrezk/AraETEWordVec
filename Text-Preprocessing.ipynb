{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "from ara_vec_preprocess_configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c7705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scrapped tweets now: \" + str((NUM_SCRAP_TWEETS)) + \"\\n\")\n",
    "print(\"AR tweets now: \" + str((NUM_AR_COL_TWEETS)) + \"\\n\")\n",
    "print(\"After remove duplicates tweets: \" + str((NUM_RM_DUBLICATE_TWEETS)) + \"\\n\")\n",
    "print(\"The number of tweets at the end is: \" + str((NUM_DETECT_AR_TWEETS)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14091f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi process for first filtration to get only tweets based on Arabic column\n",
    "if __name__ == '__main__':\n",
    "    processes = []\n",
    "    for year in range(2008, 2022):\n",
    "        p = Process(target=filter_2_get_only_arabic_tweets_column, \n",
    "             args=(str(year), DATA_DIR, DATA_PREPROCESSED_DIR, ))\n",
    "        p.start()\n",
    "        processes.append(p)     \n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c8bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scrapped tweets now: \" + str(sum(NUM_SCRAP_TWEETS)) + \"\\n\")\n",
    "print(\"AR tweets now: \" + str(sum(NUM_AR_COL_TWEETS)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434a9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi process to clean and remove dublicated tweets\n",
    "if __name__ == '__main__':\n",
    "    processes = []\n",
    "    for year in range(2008, 2022):\n",
    "        p = Process(target=filter_3_clean_get_arabic_tweets_by_detect_language, \n",
    "             args=(str(year), \"arabic_tweets_based_on_tweets_column_of_year_\", DATA_DIR, ARABIC_FILTERED_DATA_DIR, \n",
    "                                                               DATA_PREPROCESSED_DIR))\n",
    "        p.start()\n",
    "        processes.append(p)     \n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scrapped tweets now: \" + str(len(NUM_SCRAP_TWEETS)) + \"\\n\")\n",
    "print(\"AR tweets now: \" + str(len(NUM_AR_COL_TWEETS)) + \"\\n\")\n",
    "\n",
    "print(\"After remove duplicates tweets: \" + str(len(NUM_RM_DUBLICATE_TWEETS)) + \"\\n\")\n",
    "print(\"The number of tweets at the end is: \" + str(len(NUM_DETECT_AR_TWEETS)) + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"Scrapped tweets now: \" + str(sum(NUM_SCRAP_TWEETS)) + \"\\n\")\n",
    "print(\"AR tweets now: \" + str(sum(NUM_AR_COL_TWEETS)) + \"\\n\")\n",
    "\n",
    "print(\"After remove duplicates tweets: \" + str(sum(NUM_RM_DUBLICATE_TWEETS)) + \"\\n\")\n",
    "print(\"The number of tweets at the end is: \" + str(sum(NUM_DETECT_AR_TWEETS)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae2873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
